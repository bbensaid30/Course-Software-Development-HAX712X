[
  {
    "objectID": "Projects/2025-2026/README.html",
    "href": "Projects/2025-2026/README.html",
    "title": "2025-2026",
    "section": "",
    "text": "For this course, the grading consists of a long-term project.\nYou will need to work all along the semester to master it. Please start to work on it as soon as possible. You need to use your GitHub account (see Git lecture) as the expected outputs for this course are GitHub repositories.",
    "crumbs": [
      "2025-2026"
    ]
  },
  {
    "objectID": "Projects/2025-2026/README.html#setting-and-objectives",
    "href": "Projects/2025-2026/README.html#setting-and-objectives",
    "title": "2025-2026",
    "section": "2.1 Setting and objectives",
    "text": "2.1 Setting and objectives\nThe group composition is available on Moodle.\n\n\n\n\n\n\nNo free-rider tolerated\n\n\n\nThe project repository must show a balanced contribution between group members and intra-group grade variation could be made to reflect issues on the intra-group workload balance. Group issues must be reported early to the teaching team.\n\n\nThe project consists of an investigation of\nIt will be mostly a visualization project, hopefully an interactive one, with the creation of (interractive) maps/histograms/kde or a website (your choice), helping to navigate your work.",
    "crumbs": [
      "2025-2026"
    ]
  },
  {
    "objectID": "Projects/2025-2026/README.html#timing",
    "href": "Projects/2025-2026/README.html#timing",
    "title": "2025-2026",
    "section": "2.2 Timing",
    "text": "2.2 Timing\n\nMid-term project snapshot: This part consists of starting the group project, explaining the main question of interest chosen to investigate. You should show preliminary work and organization of the workload, git first steps for the group project, etc. See details below Section 2.2.1. This part corresponds to 25% of the final grade. The due date is October 25 (23:59).\nThe GitHub repository with the presentations slides should be completed before Tuesday 10 December (23:59). This part corresponds to 40% of the final grade. Nothing pushed after the deadline will be taken into account.\nThe oral presentation (15mn + 5mn questions) December 13 (8:00) (room: SC36.01). This part corresponds to 35% of the final grade.\n\n\n2.2.1 Mid-term project snapshot\nPlease provide the URL of the group repository and the group composition (do not forget your student number) in the following survey TODO_TODO_TODO_TODO_TODO_TODO_TODO_TODO_TODO_TODO_TODO_TODO_..\nThe main point for this step is to create a README.qmd (maximum length: approximately 4 pages) in a roadmap directory at the root of your project. We will provide you feedback in the form of GitHub issues on your project repository. Your file should give the outline of the project with the following ingredients:\n\nYou have to choose a name for your project. Hereafter, it is denoted by my_module_name.\ndescription of a minimum viable project, showing the architecture (website, files, classes, etc.). It does not have to be functional at this stage but must have the main files needed, details of the coding pipeline, choice of the packages/technologies used, etc.\nCreate simple pictures (possibly camera pictures of handmade drawings) showing the results you want to create (time series, maps, etc.).\nProvide several git branches (at least 2) so the project can move forward independently.\nRetro planning with a Gantt diagram as follows (see for instance Quarto and Mermaid):\n\n\n\n\n\n\n    gantt\n        title Preparing Polyglot Notebooks Talk for Stir Trek 2023\n        axisFormat %m-%d\n        section Proposal &lt;br&gt; and &lt;br&gt; Evaluation\n            Submit Abstract         :done,                2023-01-15, 2023-02-18\n            Session Evaluation      :done, EVAL,          2023-02-18, 2023-03-05\n            Talk Accepted           :milestone, done,     after EVAL,\n        section Talk &lt;br&gt; Preparation\n            Research & Outlining    :done, OUTLINE,       2023-03-12, 9d\n            Create Mermaid Examples :done, MER_EXAMPLE,   after OUTLINE, 5d\n            Write Mermaid Articles  :active, MER_ART,     after MER_EXAMPLE, 7d\n            Write Jupyter Articles  :                     after MER_ART, 3d\n        section Delivery\n            Final Notebook          :crit, NOTEBOOK,      2023-04-19, 7d\n            Rehearsal               :crit,                after NOTEBOOK, 2023-05-04\n            Stir Trek 2023          :milestone, crit,     2023-05-05, 1d\n\n\n\n\n\n\n\n2.2.1.1 Elements expected for the mid-term project\n\n\n\n\n\n\n\n\nGeneral\nDetails\nPoints (out of 20)\n\n\n\n\nMid-term\nGit / branches\n4\n\n\n\nTask affectation / Gantt chart\n4\n\n\n\nDataset choices / Download / Description\n4\n\n\n\nPackages/software description for the project\n4\n\n\n\nFigure of interest/narration\n4\n\n\nTotal\n\n20\n\n\n\n\n\n\n2.2.2 Final project\n\n2.2.2.1 General guidelines\nThe ultimate goal is to provide a:\n\na Python module that can be imported with pip and contains your work/app\na website with quarto or with sphinx that presents your project. This website should be deployed through GitHubPages. At least one of the pages should contain an interactive element (maps, widgets, etc.).\n\nA description of the procedure will be needed (imagine you are addressing a user not aware of your package). An example of a project made in 2020, is available at https://github.com/tanglef/chaoseverywhere.\n\n\n2.2.2.2 Project structure\n\nAll the code will be placed in a subdirectory called /my_module_name (choosing your module name accordingly).\nA slide deck in Beamer/LaTex, quarto or LibreOffice will be put in a /slide directory of the repository. The latter will be a short presentation of the work that will be orally presented during 15mn in front of a jury, at the end of the project.\n\n\n\n2.2.2.3 Git aspects\n\nA .gitignore that prevents garbage files from being included in your project.\nEquilibrated commits in two branches should be done (e.g., in the development branch and the master one), and merged for the milestone day.\nYour repository should contain a README.md:\n\ncontaining the title and a short description of the project\nSource of data\nfor those who choose the module/app, a description of how to run/install it\nfor those who choose a website, the link to the website and a code snippet to build it\nauthors list and a license description. See this website. A default choice could be MIT Licence.\n\n\n\n\n2.2.2.4 Object programming aspects\n\nYou should code at least one Python class.\nYour python project should contain submodules. Each submodule will be devoted to a specific sub-task of your project.\n\n\n\n2.2.2.5 Dataset(s)\n\nFor reproducibility, the data should be easily available to anyone that want to check your work. Bonus: The data used should be available in a way that the end user does not need to perform a manual download of any kind (use the pooch package or variants for instance).\n\n\n\n2.2.2.6 Time/memory evaluation\n\nA full study of the time and memory footprint of the code produced will be provided for the whole pipeline used. Elements showing speed-up / memory savings you could find along the way would be also appreciated.\n\n\n\n2.2.2.7 Documentation\n\nDocstrings should be populated for every python class and function.\nBonus points will be given if you create an API documentation using sphinx for instance.\n\n\n\n2.2.2.8 Test and CI\n\nProvide unitary tests to check that the function you proposed satisfies the requirement you target.\nImplement a Continuous Integration solution with GitHub that runs your unitary test at each commit.\n\nHere’s a recap for the final project (code, app/website):\n\n\n\n\n\n\n\n\nGeneral\nDetails\nPoints (/20)\n\n\n\n\nCode\nProblem Resolution\n8\n\n\n\nGeneral code quality / readability\n2\n\n\n\nReadme/Comments/Pep8\n1\n\n\n\nUnit Tests/CI/Deploy: wheel\n1\n\n\n\nClass (create at least 1 class)\n1\n\n\n\nReproducibility/Dataset loading\n1\n\n\n\nGraphical aspects: Widgets, clickable map, etc.\n2\n\n\n\nTime/Memory efficiency\n1\n\n\n\nDocumentation\n3\n\n\nTotal·\n\n20\n\n\n\nHere’s also a recap for the presentation part (slides, oral presentation):\n\n\n\n\n\n\n\n\nGeneral\nDetails\nPoints (/20)\n\n\n\n\nOral\nSlides quality and structure\n10\n\n\n\nClarity / lively presentation / Rhythm / Show\n10\n\n\nTotal·\n\n20",
    "crumbs": [
      "2025-2026"
    ]
  },
  {
    "objectID": "Projects/2023-2024/README.html",
    "href": "Projects/2023-2024/README.html",
    "title": "2023-2024",
    "section": "",
    "text": "For this course, the grading consists in creating a Weather Forecast App for Montpellier.\n\n\nThe main objective of this project is to create:\n\na GitHub repository containing all the code and documentation of your project\na GitHub web page displaying images representing the weather forecast for the 4 next days (in Montpellier) that automatically updates. The website URL should be accessible in the README.md at the root of your Git repository.\na short description of the methodology used, below the forecast itself.\n\n\n\n\n\nFor this project, you need to create a GitHub repository with your code, and we suggest using GitHub action for the deployment phase (weather forecast update and website generation).\nThe data to be used for your project is to be obtained from open-meteo.com.\nYou have to create a simple webpage using a GitHub page and a GitHub action with Quarto for instance; see https://quarto.org/docs/publishing/github-pages.html for details, and also the associated yml here. The webpage created will display the weather forecast in Montpellier for the next 4 days. For instance, it could display something like this:\n\n\n\n\nWeather Forecast (source: meteoblue.com)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou should display the forecast together with the highest/lowest temperature for the day, wind average, and amount of precipitation. The closer you can reproduce something in the spirit of the previous illustration, the higher the grade.\n\n\n\nAn additional constraint is that the app should be autonomous once created, and should be automatically refreshed every day (like a real weather forecast website!). An example to help you in this task is available here (and the associated source code), updating time series on a monthly basis. For the automatic update and scheduling, see the schedule event description.\nThe only image format accepted is SVG. See examples from freesvg.org, creativecommons, clker, openclipart, etc.\nThe code must be structured, commented and properly formatted with the black linter and follow pep8 convention. Guidance on this can be obtained here https://www.freecodecamp.org/news/auto-format-your-python-code-with-black/ or here for VSCode.",
    "crumbs": [
      "2023-2024"
    ]
  },
  {
    "objectID": "Projects/2023-2024/README.html#setting-and-objective",
    "href": "Projects/2023-2024/README.html#setting-and-objective",
    "title": "2023-2024",
    "section": "",
    "text": "The main objective of this project is to create:\n\na GitHub repository containing all the code and documentation of your project\na GitHub web page displaying images representing the weather forecast for the 4 next days (in Montpellier) that automatically updates. The website URL should be accessible in the README.md at the root of your Git repository.\na short description of the methodology used, below the forecast itself.",
    "crumbs": [
      "2023-2024"
    ]
  },
  {
    "objectID": "Projects/2023-2024/README.html#guidelines",
    "href": "Projects/2023-2024/README.html#guidelines",
    "title": "2023-2024",
    "section": "",
    "text": "For this project, you need to create a GitHub repository with your code, and we suggest using GitHub action for the deployment phase (weather forecast update and website generation).\nThe data to be used for your project is to be obtained from open-meteo.com.\nYou have to create a simple webpage using a GitHub page and a GitHub action with Quarto for instance; see https://quarto.org/docs/publishing/github-pages.html for details, and also the associated yml here. The webpage created will display the weather forecast in Montpellier for the next 4 days. For instance, it could display something like this:\n\n\n\n\nWeather Forecast (source: meteoblue.com)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou should display the forecast together with the highest/lowest temperature for the day, wind average, and amount of precipitation. The closer you can reproduce something in the spirit of the previous illustration, the higher the grade.\n\n\n\nAn additional constraint is that the app should be autonomous once created, and should be automatically refreshed every day (like a real weather forecast website!). An example to help you in this task is available here (and the associated source code), updating time series on a monthly basis. For the automatic update and scheduling, see the schedule event description.\nThe only image format accepted is SVG. See examples from freesvg.org, creativecommons, clker, openclipart, etc.\nThe code must be structured, commented and properly formatted with the black linter and follow pep8 convention. Guidance on this can be obtained here https://www.freecodecamp.org/news/auto-format-your-python-code-with-black/ or here for VSCode.",
    "crumbs": [
      "2023-2024"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_bikes.html",
    "href": "Courses/Scientific_Data_Libraries/pandas_bikes.html",
    "title": "Bike accident dataset",
    "section": "",
    "text": "Disclaimer: this course is adapted from the work Pandas tutorial by Joris Van den Bossche. R users might also want to read Pandas: Comparison with R / R libraries for a smooth start in Pandas.\nWe start by importing the necessary libraries:\n%matplotlib inline\nimport os\nimport numpy as np\nimport calendar\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nimport pooch  # download data / avoid re-downloading\nfrom IPython import get_ipython\nimport lzma  # to process zip file\nimport plotly.express as px\n\nsns.set_palette(\"colorblind\")\npalette = sns.color_palette(\"twilight\", n_colors=12)\npd.options.display.max_rows = 8\nReferences:",
    "crumbs": [
      "Pandas",
      "Bike accident dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_bikes.html#data-loading-and-preprocessing",
    "href": "Courses/Scientific_Data_Libraries/pandas_bikes.html#data-loading-and-preprocessing",
    "title": "Bike accident dataset",
    "section": "Data loading and preprocessing",
    "text": "Data loading and preprocessing\n\nData loading\n\n'''\n# works locally but on the website\n# url = \"https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw\"\nurl_db = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/accidents-velos_2022.csv.xz\"\npath_target = \"./bicycle_db.csv.xz\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url_db, path=path, fname=fname, known_hash=None)\nwith lzma.open(path_target) as f:\n    file_content = f.read().decode('utf-8')\n\n# write the string file_content to a file named fname_uncompressed\nwith open(\"./bicycle_db.csv\", 'w') as f:\n    f.write(file_content)\n'''\n\n'\\n# works locally but on the website\\n# url = \"https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw\"\\nurl_db = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/accidents-velos_2022.csv.xz\"\\npath_target = \"./bicycle_db.csv.xz\"\\npath, fname = os.path.split(path_target)\\npooch.retrieve(url_db, path=path, fname=fname, known_hash=None)\\nwith lzma.open(path_target) as f:\\n    file_content = f.read().decode(\\'utf-8\\')\\n\\n# write the string file_content to a file named fname_uncompressed\\nwith open(\"./bicycle_db.csv\", \\'w\\') as f:\\n    f.write(file_content)\\n'\n\n\n\ndf_bikes = pd.read_csv(\"bicycle_db.csv\", na_values=\"\", low_memory=False,\n                       dtype={'data': str, 'heure': str, 'departement': str})\n\nIn June 2023, the author decided to change the name of the columns, hence we had to define a dictionary to come back to legacy names:\n\nnew2old = {\n    \"hrmn\": \"heure\",\n    \"secuexist\": \"existence securite\",\n    \"grav\": \"gravite accident\",\n    \"dep\": \"departement\",\n}\n\ndf_bikes.rename(columns=new2old, inplace=True)\n\n\nget_ipython().system('head -5 ./bicycle_db.csv')\n\n\npd.options.display.max_columns = 40\ndf_bikes.head()\n\n\n\n\n\n\n\n\nidentifiant accident\ndate\nmois\njour\nheure\ndepartement\ncommune\nlat\nlon\nen agglomeration\ntype intersection\ntype collision\nluminosite\nconditions atmosperiques\ntype route\ncirculation\nnb voies\nprofil long route\ntrace plan route\nlargeur TPC\nlargeur route\netat surface\namenagement\nsituation\ncategorie usager\ngravite accident\nsexe\nage\nmotif deplacement\nexistence securite\nusage securite\nobstacle fixe heurte\nobstacle mobile heurte\nlocalisation choc\nmanoeuvre avant accident\nidentifiant vehicule\ntype autres vehicules\nmanoeuvre autres vehicules\nnombre autres vehicules\n\n\n\n\n0\n201100000004\n2011-09-22\n09 - septembre\n3 - jeudi\n15\n59\n59011\n50.51861\n2.93043\noui\nHors intersection\nTrois véhicules et plus - collisions multiples\nPlein jour\nNormale\nRoute Départementale\nNaN\nNaN\nPlat\nPartie rectiligne\nNaN\n58.0\nnormale\nNaN\nSur chaussée\nConducteur\n1 - Blessé léger\nM\n39-40\nPromenade - loisirs\nCasque\nOui\nNaN\nVéhicule\nNaN\nSans changement de direction\n201100000004A01\nVL seul\nTournant à gauche\n1.0\n\n\n1\n201100000004\n2011-09-22\n09 - septembre\n3 - jeudi\n15\n59\n59011\n50.51861\n2.93043\noui\nHors intersection\nTrois véhicules et plus - collisions multiples\nPlein jour\nNormale\nRoute Départementale\nNaN\nNaN\nPlat\nPartie rectiligne\nNaN\n58.0\nnormale\nNaN\nSur chaussée\nConducteur\n2 - Blessé hospitalisé\nM\n30-31\nPromenade - loisirs\nCasque\nNon\nNaN\nVéhicule\nNaN\nSans changement de direction\n201100000004B02\nVL seul\nTournant à gauche\n1.0\n\n\n2\n201100000006\n2011-11-14\n11 - novembre\n0 - lundi\n17\n59\n59011\n50.52684\n2.93423\noui\nHors intersection\nDeux véhicules - par l’arrière\nNuit avec éclairage public allumé\nNormale\nRoute Départementale\nNaN\nNaN\nNaN\nPartie rectiligne\nNaN\nNaN\nnormale\nNaN\nSur chaussée\nConducteur\n2 - Blessé hospitalisé\nM\n22-23\nDomicile - travail\nCasque\nOui\nNaN\nNaN\nAvant\nNaN\n201100000006A01\nVL seul\nArrêté (hors stationnement)\n1.0\n\n\n3\n201100000020\n2011-01-27\n01 - janvier\n3 - jeudi\n18\n59\n59256\n50.55818\n3.13667\noui\nHors intersection\nDeux véhicules - par le coté\nNuit avec éclairage public allumé\nNormale\nVoie Communale\nNaN\n2.0\nPlat\nPartie rectiligne\nNaN\n60.0\nnormale\nNaN\nSur chaussée\nConducteur\n2 - Blessé hospitalisé\nF\n36-37\nAutre\nEquipement réfléchissant\nOui\nNaN\nVéhicule\nAvant droit\nSans changement de direction\n201100000020B02\nVL seul\nEn stationnement (avec occupants)\n1.0\n\n\n4\n201100000022\n2011-09-01\n09 - septembre\n3 - jeudi\n19\n59\n59256\n50.55448\n3.12660\noui\nHors intersection\nDeux véhicules - frontale\nCrépuscule ou aube\nNormale\nHors réseau public\nNaN\n2.0\nPlat\nPartie rectiligne\nNaN\n60.0\nnormale\nNaN\nSur chaussée\nConducteur\n2 - Blessé hospitalisé\nM\n15-16\nPromenade - loisirs\nCeinture\nNaN\nNaN\nVéhicule\nAvant gauche\nSans changement de direction\n201100000022B02\nVL seul\nSans changement de direction\n1.0\n\n\n\n\n\n\n\n\ndf_bikes['existence securite'].unique()\n\narray(['Casque', 'Equipement réfléchissant', 'Ceinture', 'Autre', nan,\n       'Dispositif enfants'], dtype=object)\n\n\n\ndf_bikes['gravite accident'].unique()\n\narray(['1 - Blessé léger', '2 - Blessé hospitalisé', '3 - Tué',\n       '0 - Indemne'], dtype=object)\n\n\n\n\nHandle missing values\n\ndf_bikes['date'].hasnans\n\nFalse\n\n\n\ndf_bikes['heure'].hasnans\n\nTrue\n\n\nSo arbitrarily we fill missing values with 0 (since apparently there is no time 0 reported…to double check in the source.)\n\ndf_bikes.fillna({'heure':'0'}, inplace=True)\n\n\n\n\n\n\n\nEXERCISE: start/end of the study\n\n\n\nCan you find the starting day and the ending day of the study automatically?\nHint: Sort the data! You can sort the data by time for instance, say with df.sort('Time').\n\n\n\n\nDate and time processing\nCheck the date/time format:\n\ndf_bikes['date'] + ' ' + df_bikes['heure']\n\n0        2011-09-22 15\n1        2011-09-22 15\n2        2011-11-14 17\n3        2011-01-27 18\n             ...      \n35330    2018-03-21 18\n35331    2018-03-31 17\n35332    2018-03-31 17\n35333    2018-07-31 11\nLength: 35334, dtype: object\n\n\n\ntime_improved = pd.to_datetime(\n    df_bikes[\"date\"] + \" \" + df_bikes[\"heure\"],\n    format=\"%Y-%m-%d %H\",\n    errors=\"coerce\",\n)\n\n\ndf_bikes[\"Time\"] = time_improved\n# remove rows with NaT\ndf_bikes.dropna(subset=[\"Time\"], inplace=True)\n# set new index\ndf_bikes.set_index(\"Time\", inplace=True)\n# remove useless columns\ndf_bikes.drop(columns=[\"heure\", \"date\"], inplace=True)\n\n\ndf_bikes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 35334 entries, 2011-09-22 15:00:00 to 2018-07-31 11:00:00\nData columns (total 37 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   identifiant accident        35334 non-null  int64  \n 1   mois                        35334 non-null  object \n 2   jour                        35334 non-null  object \n 3   departement                 35334 non-null  object \n 4   commune                     35334 non-null  object \n 5   lat                         35334 non-null  float64\n 6   lon                         35334 non-null  float64\n 7   en agglomeration            35334 non-null  object \n 8   type intersection           35333 non-null  object \n 9   type collision              35330 non-null  object \n 10  luminosite                  35334 non-null  object \n 11  conditions atmosperiques    35333 non-null  object \n 12  type route                  35323 non-null  object \n 13  circulation                 132 non-null    object \n 14  nb voies                    31054 non-null  float64\n 15  profil long route           32748 non-null  object \n 16  trace plan route            31556 non-null  object \n 17  largeur TPC                 2705 non-null   float64\n 18  largeur route               18503 non-null  float64\n 19  etat surface                33694 non-null  object \n 20  amenagement                 3880 non-null   object \n 21  situation                   32742 non-null  object \n 22  categorie usager            35334 non-null  object \n 23  gravite accident            35334 non-null  object \n 24  sexe                        35334 non-null  object \n 25  age                         35323 non-null  object \n 26  motif deplacement           27579 non-null  object \n 27  existence securite          34789 non-null  object \n 28  usage securite              33690 non-null  object \n 29  obstacle fixe heurte        803 non-null    object \n 30  obstacle mobile heurte      28615 non-null  object \n 31  localisation choc           30129 non-null  object \n 32  manoeuvre avant accident    31396 non-null  object \n 33  identifiant vehicule        35334 non-null  object \n 34  type autres vehicules       30531 non-null  object \n 35  manoeuvre autres vehicules  28143 non-null  object \n 36  nombre autres vehicules     30531 non-null  float64\ndtypes: float64(6), int64(1), object(30)\nmemory usage: 10.2+ MB\n\n\n\ndf_bike2 = df_bikes.loc[\n    :, [\"gravite accident\", \"existence securite\", \"age\", \"sexe\"]\n]\n# df.replace(to_replace={'col1' : {99 : 0}, 'col2' : {99 : 0}}, \n#            inplace=True)\ndf_bike2.replace(to_replace={\"existence securite\": {\"Inconnu\": np.nan}}, inplace=True)\ndf_bike2.dropna(inplace=True)\n\n\n\n\n\n\n\nEXERCISE: Is the helmet saving your life?\n\n\n\nPerform an analysis so that you can check the benefit or not of wearing a helmet to save your life.\nBeware: Preprocessing is needed to use pd.crosstab, pivot_table to avoid issues.\n\n\n\n\n\n\n\n\n\n\ngravite accident\n0 - Indemne\n1 - Blessé léger\n2 - Blessé hospitalisé\n3 - Tué\n\n\nexistence securite\n\n\n\n\n\n\n\n\nAutre\n6.380605\n62.294583\n29.143843\n2.180970\n\n\nCasque\n7.052803\n55.608281\n33.454292\n3.884624\n\n\nCeinture\n3.414634\n15.000000\n69.268293\n12.317073\n\n\nDispositif enfants\n7.317073\n64.634146\n24.390244\n3.658537\n\n\nEquipement réfléchissant\n4.994055\n56.242568\n33.848593\n4.914784\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngravite accident\n0 - Indemne\n1 - Blessé léger\n2 - Blessé hospitalisé\n3 - Tué\n\n\nexistence securite\n\n\n\n\n\n\n\n\nAutre\n6.380605\n62.294583\n29.143843\n2.180970\n\n\nCasque\n7.052803\n55.608281\n33.454292\n3.884624\n\n\nCeinture\n3.414634\n15.000000\n69.268293\n12.317073\n\n\nDispositif enfants\n7.317073\n64.634146\n24.390244\n3.658537\n\n\nEquipement réfléchissant\n4.994055\n56.242568\n33.848593\n4.914784\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE: Are men and women dying equally on a bike?\n\n\n\nPerform an analysis to check differences between men’s and women’s survival.\n\n\n\n\nsexe\nF    0.239911\nM    0.760089\ndtype: float64\n\n\n\n\n\n\n\n\n\n\ngravite accident\n0 - Indemne\n1 - Blessé léger\n2 - Blessé hospitalisé\n3 - Tué\nAll\n\n\nsexe\n\n\n\n\n\n\n\n\n\nF\n13.40564\n27.903906\n19.946115\n15.884194\n23.868538\n\n\nM\n86.59436\n72.096094\n80.053885\n84.115806\n76.131462",
    "crumbs": [
      "Pandas",
      "Bike accident dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_bikes.html#data-visualization",
    "href": "Courses/Scientific_Data_Libraries/pandas_bikes.html#data-visualization",
    "title": "Bike accident dataset",
    "section": "Data visualization",
    "text": "Data visualization\nNote that in the dataset, the information on the level of bike practice by gender is missing.\n\n\n\n\n\n\nEXERCISE: Accident during the week?\n\n\n\nPerform an analysis to check when the accidents are occurring during the week.\n\n\n\nTime series visualization\n\ndf_bikes[\"weekday\"] = df_bikes.index.day_of_week  # Monday=0, Sunday=6\ndf_bikes.groupby(['weekday', df_bikes.index.hour])['sexe'].count()\n\nweekday  Time\n0        0        16\n         1        12\n         2         2\n         3         1\n                ... \n6        20      122\n         21       73\n         22       42\n         23       32\nName: sexe, Length: 168, dtype: int64\n\n\n\ndf_bikes.groupby(['weekday', df_bikes.index.hour])['age'].count()\n\nweekday  Time\n0        0        16\n         1        12\n         2         2\n         3         1\n                ... \n6        20      122\n         21       73\n         22       42\n         23       32\nName: age, Length: 168, dtype: int64\n\n\nThe two last results are the same, no matter if you choose the 'age' or 'sexe' variable.\nCreate a daily profile per day of the week:\n\ndf_polar = (\n    df_bikes.groupby([\"weekday\", df_bikes.index.hour])[\"sexe\"]\n    .count()\n    .reset_index()\n)  # all variable are similar in this sense, sexe could be replaced by age for instance here. XXX to simplify\n\ndf_polar = df_polar.astype({\"Time\": str}, copy=False)\ndf_polar[\"weekday\"] = df_polar[\"weekday\"].apply(lambda x: calendar.day_abbr[x])\ndf_polar.rename(columns={\"sexe\": \"accidents\"}, inplace=True)\n\nDisplay these daily profiles\n\nn_colors = 8  # 7 days, but 8 colors help to have weekends days' color closer\ncolors = px.colors.sample_colorscale(\n    \"mrybm\", [n / (n_colors - 1) for n in range(n_colors)]\n)\n\n\nfig = px.line_polar(\n    df_polar,\n    r=\"accidents\",\n    theta=\"Time\",\n    color=\"weekday\",\n    line_close=True,\n    range_r=[0, 600],\n    start_angle=0,\n    color_discrete_sequence=colors,\n    template=\"seaborn\",\n    title=\"Daily accident profile: weekday effect?\",\n)\n\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nNote\n\n\n\nIn plotly the figure is interactive. If you click on the legend on the right, you can select the day you want to see. It is very convenient to compare days two by two for instance.\n\n\n\n\n\n\n\n\nEXERCISE: Accident during the year\n\n\n\nPerform an analysis to check when the accidents are occurring during the week.\n\n\nCreate a daily profile per month:\ndf_bikes[\"month\"] = df_bikes.index.month  # Janvier=0, .... Decembre=11\n# df_bikes['month'] = df_bikes['month'].apply(lambda x: calendar.month_abbr[x])\ndf_bikes.head()\ndf_bikes_month = (\n    df_bikes.groupby([\"month\", df_bikes.index.hour])[\"age\"]\n    .count()\n    .unstack(level=0)\n)\n\n\n\n\ndf_polar2 = (\n    df_bikes.groupby([\"month\", df_bikes.index.hour])[\"sexe\"]\n    .count()\n    .reset_index()\n)  # all variable are similar in this sense, sexe could be replaced by age for instance here. XXX to simplify\n\ndf_polar2 = df_polar2.astype({\"Time\": str}, copy=False)\ndf_polar2.rename(columns={\"sexe\": \"accidents\"}, inplace=True)\ndf_polar2[\"month\"] = df_polar2[\"month\"].apply(lambda x: calendar.month_abbr[x])\n\nDisplay these daily profiles :\n\n# create a cyclical color scale for 12 values:\nn_colors = 12\ncolors = px.colors.sample_colorscale(\n    \"mrybm\", [n / (n_colors - 1) for n in range(n_colors)]\n)\n\nfig = px.line_polar(\n    df_polar2,\n    r=\"accidents\",\n    theta=\"Time\",\n    color=\"month\",\n    line_close=True,\n    range_r=[0, 410],\n    start_angle=0,\n    color_discrete_sequence=colors,\n    template=\"seaborn\",\n    title=\"Daily accident profile: weekday effect?\",\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\n\n\n\n\nEXERCISE: Accidents by department\n\n\n\nPerform an analysis to check when the accidents are occurring for each department, relative to population size.\n\n\n\n\nGeographic visualization\nIn this part, we will use the geopandas library to visualize the data on a map, along with plotly for interactivity.\n\npath_target = \"./dpt_population.csv\"\nurl = \"https://public.opendatasoft.com/explore/dataset/population-francaise-par-departement-2018/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n\n'/home/bbensaid/Documents/Cours_MCF1/Dev_Logiciel_website/Courses/Scientific_Data_Libraries/dpt_population.csv'\n\n\n\ndf_dtp_pop = pd.read_csv(\"dpt_population.csv\", sep=\";\", low_memory=False)\n\ndf_dtp_pop.replace(to_replace={\"code\": {\"2A\": \"20A\", \"2B\": \"20B\"}}, inplace=True)\ndf_dtp_pop.sort_values(by=[\"Code Département\"], inplace=True)\n\ndf_bikes.replace(to_replace={\"code\": {\"2A\": \"20A\", \"2B\": \"20B\"}}, inplace=True)\ndf_bikes.sort_values(by=[\"departement\"], inplace=True)\n# Clean extra departements\n\ndf_bikes = df_bikes[df_bikes[\"departement\"].isin(df_dtp_pop[\"Code Département\"])]\n\ngd = df_bikes.groupby([\"departement\"], as_index=True, sort=True).size()\n\ndata = {\"code\": gd.index, \"# Accidents per million\": gd.values}\ndf = pd.DataFrame(data)\ndf[\"# Accidents per million\"] = (\n    df[\"# Accidents per million\"].values\n    * 10000.0\n    / df_dtp_pop[\"Population\"].values\n)\n\nWe now need to download the .geojson file containing the geographic information for each department. We will use the pooch library to download the file and store it locally.\n\npath_target = \"./departements.geojson\"\n# url = \"https://raw.githubusercontent.com/gregoiredavid/france-geojson/master/departements-avec-outre-mer.geojson\"\nurl = \"https://raw.githubusercontent.com/gregoiredavid/france-geojson/master/departements-version-simplifiee.geojson\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n\n'/home/bbensaid/Documents/Cours_MCF1/Dev_Logiciel_website/Courses/Scientific_Data_Libraries/departements.geojson'\n\n\nFirst, you have to handle Corsican departments, which are not in the same format as the others.\nimport plotly.express as px\nimport geopandas\n\ndepartement = geopandas.read_file(\"departements.geojson\")\n\ndepartement.replace(to_replace={\"code\": {\"2A\": \"20A\", \"2B\": \"20B\"}}, inplace=True)\n\ndepartement.sort_values(by=[\"code\"], inplace=True)\n\na = [\"0\" + str(i) for i in range(1, 10)]\nb = [str(i) for i in range(1, 10)]\ndict_replace = dict(zip(a, b))\n\n\ndepartement.replace(\n    to_replace={\"code\": {\"20A\": \"2A\", \"20B\": \"2B\", **dict_replace}}, inplace=True\n)\n\ndf.replace(\n    to_replace={\"code\": {\"20A\": \"2A\", \"20B\": \"2B\", **dict_replace}}, inplace=True\n)\n\ndepartement.set_index(\"code\", inplace=True)\nprint(departement[\"nom\"].head(22))\n\n\n\ncode\n1                         Ain\n2                       Aisne\n3                      Allier\n4     Alpes-de-Haute-Provence\n               ...           \n19                    Corrèze\n2A               Corse-du-Sud\n2B                Haute-Corse\n21                  Côte-d'Or\nName: nom, Length: 22, dtype: object\n\n\n\nOnce this is done, you can plot the data on a map.\n\nfig = px.choropleth_mapbox(\n    df,\n    geojson=departement,\n    locations=\"code\",\n    color=\"# Accidents per million\",\n    range_color=(0, df[\"# Accidents per million\"].max()),\n    color_continuous_scale=\"rdbu\",\n    center={\"lat\": 44, \"lon\": 2},\n    zoom=3.55,\n    mapbox_style=\"white-bg\",\n)\nfig.update_traces(selector=dict(type=\"choroplethmapbox\"))\nfig.update_layout(\n    title_text=\"Accidents per million inhabitants by department\",\n    coloraxis_colorbar=dict(thickness=20, orientation=\"h\", y=0.051, x=0.5),\n)\nfig.show()\n\n/tmp/ipykernel_18227/3940695818.py:1: DeprecationWarning:\n\n*choropleth_mapbox* is deprecated! Use *choropleth_map* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nEXERCISE: Accidents by department\n\n\n\nPerform an analysis to check when the accidents are occurring for each department, relative to the area of the departements.\n\n\n\n\n\n\n\n\nEXERCISE: plot DOMs and metropolitan France (hard?)\n\n\n\nPlot DOMs with metropolitan France, so that all departements are visible on the same figure (for instance using this geoson file, and making all departements fitting a small figure).",
    "crumbs": [
      "Pandas",
      "Bike accident dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_bikes.html#references",
    "href": "Courses/Scientific_Data_Libraries/pandas_bikes.html#references",
    "title": "Bike accident dataset",
    "section": "References",
    "text": "References\n\nOther interactive tools for data visualization: Altair, Bokeh. See comparisons by Aarron Geller: link\nAn interesting tutorial on Altair: Altair introduction",
    "crumbs": [
      "Pandas",
      "Bike accident dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is one of the main steps on the way to understanding a dataset. General information on data visualization (beyond Python) can be found in the following list:\n\nA visualization guide from data.europa.eu: The official portal for European data\nThe Python Graph Gallery\nFundamentals of Data Visualization\nData stories can help provide new ideas for your own work: Maarten Lambrechts’s website\nHow to choose your chart by Andrew V. Abela:\n\n.\n\nA blog post by Felipe Curty investigating the Python data visualization landscape\n\nA major difference in the visualization solutions relies on the possibility of performing interactive inspection; otherwise, the solution is said static.\nInteractive tools for data visualization are emerging in Python with plotly, altair, Bokeh, etc. An extensive study by Aarron Geller provides the pros and cons of each method.",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#introduction",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#introduction",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization is one of the main steps on the way to understanding a dataset. General information on data visualization (beyond Python) can be found in the following list:\n\nA visualization guide from data.europa.eu: The official portal for European data\nThe Python Graph Gallery\nFundamentals of Data Visualization\nData stories can help provide new ideas for your own work: Maarten Lambrechts’s website\nHow to choose your chart by Andrew V. Abela:\n\n.\n\nA blog post by Felipe Curty investigating the Python data visualization landscape\n\nA major difference in the visualization solutions relies on the possibility of performing interactive inspection; otherwise, the solution is said static.\nInteractive tools for data visualization are emerging in Python with plotly, altair, Bokeh, etc. An extensive study by Aarron Geller provides the pros and cons of each method.",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#python",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#python",
    "title": "Data Visualization",
    "section": "Python",
    "text": "Python\nThe list is long (and growing) of Python packages for data visualization. We provide some examples in the pandas section of the website, and also in the Scipy course.\n\nGeneric tools\n\nmatplotlib: Visualization with Python\nSource: https://matplotlib.org/.\nThis is the standard library for plots in Python. The documentation is well written and matplotlib should be the default choice for creating static documents (e.g., .pdf or .doc files).\nUsual loading command:\n\nimport matplotlib.pyplot as plt\n\nExample:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nt = np.linspace(0, 2 * np.pi, 1024)\nft1 = np.sin(2 * np.pi * t)\nft2 = np.cos(2 * np.pi * t)\nfig, ax = plt.subplots()\nax.plot(t, ft1, label='sin')\nax.plot(t, ft2, label='cos')\nax.legend(loc='lower right');\n\n\n\n\n\n\n\n\n\n\nseaborn: statistical data visualization\nSource: https://seaborn.pydata.org/.\nseaborn is built over matplotlib and is specifically tailored for data visualization (maptlotlib is a more flexible and general tool). Default settings are usually nicer than one from maptlotlib, especially for standard tools (histograms, KDE, swarmplots, etc.).\nUsual loading command:\n\nimport seaborn as sns\n\nExample:\n\nimport seaborn as sns\nimport pandas as pd\ndf = pd.DataFrame(dict(sin=ft1, cos=ft2))\nsns.set_style(\"whitegrid\")\nax = sns.lineplot(data=df)\nsns.move_legend(ax, \"lower right\")\nsns.despine()\n\n\n\n\n\n\n\n\n\n\nplotly: a graphing library for Python\nSource: https://plotly.com/python/.\nThe force of plotly is that it is interactive and can handle R software or julia on top of Python (it relies on Java Script under the hood).\nUsual loading command:\n\nimport plotly\n\nAlternatively, you can also use plotly.express to use predefined figures:\n\nimport plotly.express as px\n\n\nimport plotly.express as px\nfig = px.line(df)\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nNote\n\n\n\nIn plotly the figure is interactive. If you click on the legend on the right, you can select a curve to activate/deactivate.\n\n\nBut now you can also create a slider to change a parameter, for instance showing the functions\n\\[\n\\begin{align*}\nf_w: t \\to \\sin(2 \\cdot \\pi \\cdot w \\cdot t)\\\\\ng_w: t \\to \\sin(2 \\cdot \\pi \\cdot w \\cdot t)\n\\end{align*}\n\\] for \\(w \\in [-5, 5]\\)\n\n# inspiration from:\n# https://community.plotly.com/t/multiple-traces-with-a-single-slider-in-plotly/16356\nimport plotly.graph_objects as go\nimport numpy as np\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\n\nnum_steps = 101\nslider_range = np.linspace(-5, 5 , num=num_steps)\ntrace_list1 = []\ntrace_list2 = []\n\nfor i, w  in enumerate(slider_range):\n    trace_list1.append(go.Scatter(y=np.sin(2*np.pi*t*w), visible=False, line={'color': 'red'}, name=f\"sin(w * 2 *pi)\"))\n    trace_list2.append(go.Scatter(y=np.cos(2*np.pi*t *w), visible=False, line={'color': 'blue'}, name=f\"cos(w * 2 *pi)\"))\n\nfig = go.Figure(data=trace_list1+trace_list2)\n\n# Initialize display:\nfig.data[51].visible = True\nfig.data[51 + num_steps].visible = True\n\n\nsteps = []\nfor i in range(num_steps):\n    # Hide all traces\n    step = dict(\n        method = 'restyle',\n        args = ['visible', [False] * len(fig.data)],\n        label=f\"{w:.2f}\"\n    )\n    # Enable the two traces we want to see\n    step['args'][1][i] = True\n    step['args'][1][i+num_steps] = True\n\n    # Add step to steps list\n    steps.append(step)\n\nsliders = [dict(\n    active = 50,\n    currentvalue={\"prefix\": \"w = \"},\n    steps = steps,\n)]\n\nfig.layout.sliders = sliders\n\niplot(fig, show_link=False)\n\n        \n        \n        \n\n\n                                                    \n\n\n\n\n\nInteractive tools\n\nShiny: interactive web applications\nSource: https://shiny.posit.co/py/.\nShiny helps you to customize the layout and style of your application and dynamically respond to events, such as a button press, or dropdown selection. It was born and raised in R, but is now adapted to Python. It can also be interfaced easily with Quarto to render the app on your website, using a {shinylive-python} cell; see an example at https://quarto-ext.github.io/shinylive/.\nThe app created is readable, yet the price to pay is the fluidity of the rendering.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| components: [editor, viewer]\n#| folded: true\n#| layout: vertical\n#| standalone: true\n#| viewerHeight: 630\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom shiny import ui, render, App\n\n# Create some random data\nt = np.linspace(0, 2 * np.pi, 1024)\nnum_steps = 101\nslider_range = np.linspace(-5, 5 , num=num_steps)\n\n\napp_ui = ui.page_fixed(\n    ui.h2(\"Playing with sliders\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"w\", \"Frequency\", -5, 5, value=1, step=slider_range[-1]-slider_range[-2]),\n        ),\n            ui.output_plot(\"plot\")\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def plot():\n        fig, ax = plt.subplots()\n        ax.plot(t, np.sin(2*np.pi *input.w() * t), label='sin')\n        ax.plot(t, np.cos(2*np.pi *input.w() * t), label='cos')\n        ax.legend(loc='lower right');\n        return fig\n\n\napp = App(app_ui, server)\n\n\nbokeh: interactive visualizations in the browsers\nSource: http://bokeh.org/.\nUsual loading command:\n\nimport bokeh\n\nAs of today (Oct. 2023), this is not supported in Quarto, so no example is given here. A server is needed (locally or remotely).\n\n\nvega-altair: declarative visualization in Python\nSource: https://altair-viz.github.io/.\nUsual loading command:\n\nimport altair as alt\n\nAs of today (Oct. 2023), this is not supported in Quarto, so no example is given here. A server is needed (locally or remotely).\n\nAn interesting tutorial: Altair introduction\n\n\n\npygal: python charting\nSource: https://www.pygal.org/.\nWe use it mostly for maps, and especially for the map of France with DOMs. For instance, see the course Creating a Python module, such a map is constructed:\n\n    \n\n\nimport pygal\n\nTo access the French map plugin you need the installation step that follows:\npip install pygal_maps_fr",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#animation-display-with-python",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#animation-display-with-python",
    "title": "Data Visualization",
    "section": "Animation display with python",
    "text": "Animation display with python\n\nAnimation with matplotlib\nYou can use FuncAnimation to animate a sequence of images:\n\n%%capture\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display\nfig, ax = plt.subplots()\nxdata, ydata = [], []\n(ln,) = plt.plot([], [], \"ro\")\n\n\ndef init():\n    ax.set_xlim(0, 2 * np.pi)\n    ax.set_ylim(-1, 1)\n    return (ln,)\n\n\ndef update(frame):\n    xdata.append(frame)\n    ydata.append(np.sin(frame))\n    ln.set_data(xdata, ydata)\n    return (ln,)\n\nani = FuncAnimation(\n    fig,\n    update,\n    interval=50,\n    frames=np.linspace(0, 2 * np.pi, 100),\n    init_func=init,\n    blit=True,\n)\n\ndisplay(HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nAnother way of displaying video exists, using html5 video:\n\ndisplay(HTML(ani.to_html5_video()))\n\nReferences:\n\nMatplotlib Animations / JavaScript Widgets by Louis Tiao",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#animation-with-plotly",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#animation-with-plotly",
    "title": "Data Visualization",
    "section": "Animation with plotly",
    "text": "Animation with plotly\nmatplotlib works fine for advanced tuning, but is harder for simple tasks. So just try plotly for basic animations:\n\nimport plotly.express as px\nfrom plotly.offline import plot\n\ndf = px.data.gapminder()\nfig = px.scatter(\n    df,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    animation_frame=\"year\",\n    animation_group=\"country\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=55,\n    range_x=[100, 100000],\n    range_y=[25, 90],\n)\nfig.show(\"notebook\")\n\n        \n        \n        \n\n\n                                                    \n\n\n\nSpatial visualization\n\nipyleaflet\n\nfrom ipyleaflet import Map, Marker, basemaps, basemap_to_tiles\nMontpellier_gps = (43.610769, 3.876716)\nm = Map(\n  basemaps=basemaps.OpenStreetMap.Mapnik,\n  center=Montpellier_gps,\n  zoom=6\n)\nm.add_layer(Marker(location=Montpellier_gps))\nm\n\n\n\n\n\n\nfolium\n\nimport folium\n\nm = folium.Map(\n    location=Montpellier_gps,\n    control_scale=True,\n    zoom_start=6\n)\n\nfolium.Marker(\n    location=Montpellier_gps,\n    tooltip=\"Click me!\",\n    popup=\"Montpellier\",\n    icon=folium.Icon(icon=\"certificate\", color=\"orange\"),\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nlonboard\nThis is a fast map visualization with Python for large datasets lonboard\n\n\n\n3D visualization\n\nvedo: scientific analysis and visualization of 3D objects\nSource: https://vedo.embl.es/\nThis is a Python module for scientific analysis of 3D objects and point clouds based on VTK (C++) and numpy.",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#r-software",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#r-software",
    "title": "Data Visualization",
    "section": "R software",
    "text": "R software\nXXX TODO.",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Visualization.html#java-script",
    "href": "Courses/Scientific_Data_Libraries/Visualization.html#java-script",
    "title": "Data Visualization",
    "section": "Java Script",
    "text": "Java Script\nXXX TODO. Out of the scope of this course, yet more powerful.\n\nD3JS\n\n\nObservable\nThis is of interest as Quarto can directly read such kinds of figures.\n\nviewof inputs = Inputs.form([\n      Inputs.range([-5, 5], {value: 0.5, step: 0.1, label: tex`\\text{frequency} ~\\omega`}),\n    ])\n\nplt = Plot.plot({\n      color: {\n      legend: true\n    },\n    x: {\n      label: \"x\",\n    //   axis: true\n    },\n    y: {\n    //   axis: true,\n      domain: [-1.2, 1.2]\n    },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.ruleX([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 }),\n      Plot.line(data, {x: \"x\", y: \"y\", stroke : \"type\", strokeWidth: 2})\n    ]\n  })\n\n\ndata = {\n  const x = d3.range(-10, 10, 0.01);\n  const sins = x.map(x =&gt; ({x: x, y: Math.sin(- x * mu), type: \"sin(w .)\"}));\n  const coss = x.map(x =&gt; ({x: x, y: Math.cos(- x * mu), type: \"cos(w .)\"}));\n  return sins.concat(coss)\n\n}\n\nmu = inputs[0]",
    "crumbs": [
      "Generality",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html",
    "title": "Numpy cheat sheet",
    "section": "",
    "text": "Disclaimer: this course is adapted from the work by Nicolas Rougier: https://github.com/rougier/numpy-tutorial.\nLet us start with a preliminary remark concerning the random part. One is expected to run a command like\nbefore anything, to initialize the random generator rng.",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#matrix-creation",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#matrix-creation",
    "title": "Numpy cheat sheet",
    "section": "Matrix creation",
    "text": "Matrix creation\n\nCreation: vector case\n\n\n\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\n x = np.zeros(9) \n\n\n\n\n\n\n\n x = np.ones(9)\n\n\n\n\n\n\n\n x = np.full(9, 0.5)\n\n\n\n\n\n\n\n x = np.zeros(9)\n x[2] = 1\n        \n\n\n\n\n\n\n\n x = np.arange(9)\n\n\n\n\n\n\n\n x[::-1]\n\n\n\n\n\n\n\n x = rng.random(9)\n\n\n\n\n\n\n\n\n\nCreation: matrix case\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\n M = np.zeros((5, 9)) \n\n\n\n\n\n\n\n M = np.ones((5, 9))\n\n\n\n\n\n\n\n M = np.zeros((5, 9))\n M[0, 2] = 0.5\n M[1, 0] = 1.\n M[2, 1] = 0.4\n            \n\n\n\n\n\n\n\n M = np.arange(45).reshape((5, 9))\n\n\n\n\n\n\n\n M = rng.random((5, 9))\n\n\n\n\n\n\n\n M = np.eye(5, 9)\n\n\n\n\n\n\n\n M = np.diag(np.arange(5))\n\n\n\n\n\n\n\n\n\n M = np.diag(np.arange(3), k=2)\n\n\n\n\n\n\n\n\n\n\n\nMeshgrid (🇫🇷: maillage)\nnx, ny = (8, 3)\nx = np.linspace(0, 1, nx)\ny = np.linspace(0, 1, ny)\nxx, yy = np.meshgrid(x, y)\n\n\n\n\nx\n\n\ny\n\n\nxx\n\n\nyy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreation: tensor cases\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\n T = np.zeros((3, 5, 9)) \n\n\n\n\n\n\n\n T = np.ones((3, 5, 9))\n\n\n\n\n\n\n\n T = np.arange(135).reshape(3, 5, 9)\n\n\n\n\n\n\n\n T = rng.random((3, rows, cols))\n\n\n\n\n\n\n\n\nn1, n2 = 5, 3\nones = np.ones((n2, n1))\nones = ones[:, :, np.newaxis]\nlnsp = np.linspace(0.01, 0.99, 8)\nT = ones * lnsp",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#matrix-reshaping",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#matrix-reshaping",
    "title": "Numpy cheat sheet",
    "section": "Matrix reshaping",
    "text": "Matrix reshaping\nWe start here with\nM = np.zeros((3, 4))\nM[2, 2] = 1\n\n\n\nStarting from the previous matrix, we can reshape it in different ways:\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\nM = M.reshape(4, 3)\n\n\n\n\n\n\n\nM = M.reshape(12, 1)\n\n\n\n\n\n\n\nM = M.reshape(1, 12)\n\n\n\n\n\n\n\nM = M.reshape(6, 2)\n\n\n\n\n\n\n\nM = M.reshape(2, 6)",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#slicing",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#slicing",
    "title": "Numpy cheat sheet",
    "section": "Slicing",
    "text": "Slicing\nStart from a zero matrix:\nM = np.zeros((5, 9))\n\n\n\nStarting from the previous matrix, we can slice it in different ways:\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\nM[...] = 1 \n\n\n\n\n\n\n\nM[:, ::2] = 1\n\n\n\n\n\n\n\nM[::2, :] = 1\n\n\n\n\n\n\n\nM[1, 1] = 1\n\n\n\n\n\n\n\nM[:, 0] = 1\n\n\n\n\n\n\n\nM[0, :] = 1\n\n\n\n\n\n\n\nM[2:; 2:] = 1\n\n\n\n\n\n\n\nM[:-2:, :-2] = 1\n\n\n\n\n\n\n\nM[2:4, 2:4] = 1\n\n\n\n\n\n\n\nM[::2, ::2] = 1\n\n\n\n\n\n\n\nM[3::2, 3::2] = 1",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#operations-on-matrices",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#operations-on-matrices",
    "title": "Numpy cheat sheet",
    "section": "Operations on matrices",
    "text": "Operations on matrices\nStart from a simple matrix:\nrows, cols = 3, 6\nM = np.linspace(0, 1, rows * cols).reshape(rows, cols)\n\n\n\nStarting from the previous matrix, we can apply the following operations:\n\n\n\n\nCode\n\n\nResult\n\n\n\n\n\n\n M.T \n\n\n\n\n\n\n\n\n\n M[::-1, :] \n\n\n\n\n\n\n\n\n\n M[:, ::-1] \n\n\n\n\n\n\n\n\n\nnp.where(M &gt; 0.5, 0, 1) \n\n\n\n\n\n\n\n\n\nnp.maximum(M, 0.5) \n\n\n\n\n\n\n\n\n\nnp.minimum(M, 0.5) \n\n\n\n\n\n\n\n\n\nnp.mean(M, axis=0) \n\n\n\n\n\n\n\n\n\nnp.mean(M, axis=1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the last operations note that the dimensions of the matrices are reduced, so you create a vector as a result, with dimensions (6,) or (3,) respectively, when computing the mean along the 0-axis (column-wise mean), respectively along the 1-axis (row-wise mean).",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#broadcasting",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#broadcasting",
    "title": "Numpy cheat sheet",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting allows the addition of matrices of different sizes (though this is mathematically wrong), by repeating the smaller ones along the missing dimensions. The only requirement is that the trailing (i.e, rightmost) dimensions match, somehow.\n\n\n\n\nM\n\n\nN\n\n\nM+N",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Numpy.html#resources",
    "href": "Courses/Scientific_Data_Libraries/Numpy.html#resources",
    "title": "Numpy cheat sheet",
    "section": "Resources",
    "text": "Resources\n\nThis work is deeply inspired and adapted from the great work by Nicolas Rougier: https://github.com/rougier/numpy-tutorial",
    "crumbs": [
      "Scientific Libraries",
      "Numpy cheat sheet"
    ]
  },
  {
    "objectID": "Courses/IDEs/VSCode.html",
    "href": "Courses/IDEs/VSCode.html",
    "title": "Integrated Development Environment",
    "section": "",
    "text": "An Integrated Development Environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger.",
    "crumbs": [
      "IDEs",
      "Integrated Development Environment"
    ]
  },
  {
    "objectID": "Courses/IDEs/VSCode.html#pythons-specific-ide",
    "href": "Courses/IDEs/VSCode.html#pythons-specific-ide",
    "title": "Integrated Development Environment",
    "section": "Python’s specific IDE",
    "text": "Python’s specific IDE\nThere are many IDEs for Python, but none are perfect, and there is no consensus in the Python community. There is no real “canonical” choice as Rstudio is the one for R users.\nAs Python is a real jackknife programming language, depending on your goal (data scientific program, web development, etc.) you may choose a specific IDE for a particular task.\n\nScientific computing: Pyzo, Spyder.\nGeneric: PyCharm, VSCode.\n\nWe warmly recommend you use an IDE, and we will mostly describe VSCode in what follows.",
    "crumbs": [
      "IDEs",
      "Integrated Development Environment"
    ]
  },
  {
    "objectID": "Courses/IDEs/VSCode.html#vscodecodium",
    "href": "Courses/IDEs/VSCode.html#vscodecodium",
    "title": "Integrated Development Environment",
    "section": "VSCode/Codium",
    "text": "VSCode/Codium\nFor instance, you can use VSCode. This is a powerful, cross-platform IDE that comes with many extensions.\nOn the FdS-Linux box, there is a fork of VSCode called vscodium. You may launch it via the GUI or through the following command line:\n$ vscodium\nor:\n$ code\n\nInstall a VSCode extension\nWe will install the Python extension. To install it:\n\nOpen VSCode.\nOpen the Extensions tab (left bar of the VSCode window or simply press Ctrl+Shift+X).\nType Python to find the Python extension from Microsoft.\nClick the Install button, then the Enable button.\n\nor:\n\nOpen VSCode.\nPress Ctrl+P to open the Quick Open dialog.\nType ext install ms-python.python to find the extension.\nClick the Install button, then the Enable button.\n\nor:\n\nRun in a terminal:\n\n$ vscodium --install-extension ms-python.python\n\n\n\n\n\n\nEXERCISE: Installation on your machine\n\n\n\n\nInstall the Python extension in your VSCode.\n\n\n\n\n\nAn advanced text editor\nThe keyboard shortcuts Reference guide is available in the help menu (or with Ctrl+K Ctrl+R shortcut). It can be very useful to learn some shortcuts. For instance:\n\nLearn how multicursors work (e.g., search for an occurrence with Ctrl+d).\nCreate aligned multicursors with Ctrl+Shift.\nLearn how to move an entire line with Alt+up.\netc.\n\n\n\nUsing VSCode as a Python IDE\nReference: VSCode docs for Python\nThis part is dedicated to setting up VSCode to use it as a Python IDE. You should have a working VSCode (with Python extension) and anaconda program.\n\n\n\n\n\n\nEXERCISE: VSCode and Python\n\n\n\n\nStart VSCode in a project (workspace) folder: Using a command prompt or terminal, create an empty folder called test_dir, navigate into it, and open VSCode (vscodium) in that folder (.) by entering the following commands:\ncd ~\nmkdir test_dir\ncd hello\ncode\nNote: If you’re using an Anaconda distribution, be sure to use an Anaconda command prompt.\nBy starting VSCode in a folder, that folder becomes your “workspace”. VSCode stores settings that are specific to that workspace in the (hidden) subfolder .vscode/settings.json, which are separate from user settings that are stored globally.\nAlternatively, you can run VSCode through the operating system UI, then use File &gt; Open Folder to open the project folder.\nSelect a Python interpreter: Python is an interpreted language; to run Python code, you must tell VSCode which interpreter to use.\nFrom within VSCode, select a Python 3 interpreter by opening the Command Palette (Ctrl+Shift+P), start typing the Python: Select Interpreter command to search, then select the command. You can also use the Select Python Environment option on the Status Bar if available.\nNext, you have to learn how to debug a simple Python script, see the VSCode help on debugging for that.",
    "crumbs": [
      "IDEs",
      "Integrated Development Environment"
    ]
  },
  {
    "objectID": "Courses/IDEs/VSCode.html#recommended-extensions-for-vs-code",
    "href": "Courses/IDEs/VSCode.html#recommended-extensions-for-vs-code",
    "title": "Integrated Development Environment",
    "section": "Recommended extensions for VS Code",
    "text": "Recommended extensions for VS Code\n\nPython: Python.\nMarkdown: Markdown All in One.\nlinter/flake8: cornflakes.\nSpell check: Grammarly, SpellChecker or LTex.\nLive Share to collaboratively edit and debug with others in real-time, regardless of your programming language.\nQuarto.\nLaTeX: LaTeX Workshop.\netc.",
    "crumbs": [
      "IDEs",
      "Integrated Development Environment"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html",
    "href": "Courses/OPP/Classes_Exceptions.html",
    "title": "Classes & Exceptions",
    "section": "",
    "text": "Classes are central elements in Object-oriented programming (OOP)\nA class structure defines an object, its properties, and all the operations one can apply to it. Moreover, it allows the creation of multiple instances of the same object, each with its own properties, and to apply the same operations to all of them. Another key concept is the one of inheritance, defining a new class from an existing one by simply inheriting all its properties and operations.\nIn Python, a class contains attributes (variables) and methods (functions). Syntactically, it is defined similarly to a function, replacing the def keyword with class and requiring a colon : at the end of the first line. The name of a class should be CamelCase1 (or CapWords).\nFor instance, in sklearn, the Logistic Regression class is named LogisticRegression, and can be loaded and investigated as follows:\n\nfrom sklearn.linear_model import LogisticRegression\ndir(LogisticRegression)\n\n\n\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_check_feature_names',\n '_check_n_features',\n '_estimator_type',\n '_get_param_names',\n '_get_tags',\n '_more_tags',\n '_predict_proba_lr',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_validate_data',\n 'decision_function',\n 'densify',\n 'fit',\n 'get_params',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'score',\n 'set_params',\n 'sparsify']\n\n\n\n\nIn particular, you can already discover the attributes and methods shared to all classes (e.g., __class__, __init__, __doc__,), and the one specific to the LogisticRegression class (e.g., fit, get_params, predict, etc.).\nUsually, a class contains some class methods that can be seen as functions inside the class.\n\nThe first argument of a (non-static) method is usually called self: it is a mandatory element. This self argument is for self-reference. Self is a convention and not a Python keyword, so any other name can be used instead of self (e.g., this or me or in_place_of_self).\nSome method names have a specific meaning, for instance:\n\n__init__: name of the method invoked when creating the object (instantiation)\n__call__: method invoked when calling the object\n__str__: method invoked when a class has a representation as a string, e.g., when passing it to the print function\nsee Python documentation for more special names.\n\n\n\n\nLet us define a simple Point class, representing a point in the plane (i.e., a point in \\(\\mathbb{R}^2\\)):\n\nclass Point(object):\n    \"\"\"A class to represent planar points.\"\"\"\n\n    def __init__(self, x, y):\n        \"\"\"Create a new point (x, y).\"\"\"\n        self.x = x\n        self.y = y\n\n    def translate(self, dx, dy):\n        \"\"\"Translate the point by dx and dy.\"\"\"\n        self.x += dx\n        self.y += dy\n\n    def __str__(self):\n        return f\"Point: [{self.x:.3f}, {self.y:.3f}]\"\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are not familiar with printing in Python, start with the new f-strings format; see for instance: https://zetcode.com/python/fstring/.\n\n\nTo create a new instance of the class Point you run the following code:\n\np1 = Point(x=0, y=0)  # call __init__ ;\n\nNow, you can access the attributes of the object p1:\n\nprint(p1.x, p1.y)\nprint(f\"{p1}\")  # call __str__\nprint(p1)\nprint(p1.__doc__)\n\n0 0\nPoint: [0.000, 0.000]\nPoint: [0.000, 0.000]\nA class to represent planar points.\n\n\nTo apply our translate method to the point p1:\n\np1.translate(dx=1, dy=1)\nprint(p1.translate)\nprint(p1)\nprint(type(p1))\n\n&lt;bound method Point.translate of &lt;__main__.Point object at 0x7cb8dc31ff10&gt;&gt;\nPoint: [1.000, 1.000]\n&lt;class '__main__.Point'&gt;\n\n\nImplicitly, the previous command is equivalent to Point.translate(p1, dx=1, dy=1). Indeed, you can check the output of the following command:\n\np1 = Point(x=0, y=0)\nPoint.translate(p1, dx=1, dy=1)\nprint(p1)\n\nPoint: [1.000, 1.000]\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might have already used that syntax with the “dot” (.) notation in numpy, for instance when executing a numpy function as follows\n\nimport numpy as np\nrng = np.random.default_rng(seed=12345)\na = rng.random((3, 3))\na.mean(axis=0)\n\narray([0.50063315, 0.29820069, 0.60097848])\n\n\nIn that case, a was an instance of the numpy.ndarray class (see numpy documentation for details on this class):\n\ntype(a)\n\nnumpy.ndarray\n\n\n\n\n\n\n\nMyClass(arg1, arg2, ...) is a shorthand for MyClass.__call__(arg1, arg2, ...), so this allows writing classes where the instances behave like functions and can be called like a function\n\nclass Sum:\n    def __init__(self):\n        print(\"Instance Created\")\n\n    # Defining __call__ method\n    def __call__(self, a, b):\n        print(a + b)\n\n# Instance created\nsum_as_a_function = Sum()\n\n# __call__ method will be called\nsum_as_a_function(10, 20)\n\nInstance Created\n30\n\n\nA test function of interest is isinstance which allows checking if an object is of the correct class. For instance, one can check if sum_as_a_function is an instance of Sum\n\nisinstance(sum_as_a_function, Sum)\n\nTrue\n\n\nor an instance of Point:\n\nisinstance(sum_as_a_function, Point)\n\nFalse\n\n\nAnother example of a callable is given below with a simple Counter class:\n\nclass Counter:\n    \"\"\"A simple counter class.\"\"\"\n\n    def __init__(self):\n        self.count = 0\n\n    def increment(self):\n        self.count += 1\n\n    def __call__(self):\n        self.increment()\n\nThen, performing the following commands you can check how the __call__ method is called:\n\ncounter = Counter()\nprint(counter.count)\ncounter.increment()\nprint(counter.count)\ncounter()\nprint(counter.count)\n\n0\n1\n2\n\n\nIn particular, note that a method of a class can modify the state of a particular instance. This does not alter the other instantiations of the class. \n\n\n\n\n\n\nTo go further\n\n\n\nThis example and more details on callable are described in detail in this Real Python post.\n\n\n\n\n\n\n\n\nEXERCISE: Gaussian class\n\n\n\nImplement a class Gaussian with attributes mean and std with a method\n\n__str__ returning a string with the expression of the density\n__eq__ testing the equality of two instances. You should use numpy.isclose()\n__add__ implementing the (left) addition of independent Gaussian, or more precisely their pdfs (probability density functions)\n__radd__ implementing the (right) addition of independent Gaussian, or more precisely their pdfs (probability density functions)\n\nNote: when executing a+b what really happens is that the add method of the a: object is called a.__add__(b). See stackoverflow for more details on the __add__ and __radd__ methods.\n\ng1 = Gaussian(0.0, 1.0)\ng2 = Gaussian(1.0, 2.0)\ng3 = Gaussian(2.0, 2.0)\ng4 = Gaussian(3.0, 3.0)\n\nprint(g1)\nprint(g2)\nprint(g3)\nprint(g4)\nprint(g2 + g1)\nprint(sum([g1, g2, g3]))\nprint(sum([g1, g2, g3]) == g4)\n\npdf: exp(-(x - 0.000)^2 / (1.000*2^2)) / sqrt(2 * pi * 1.000^2)\npdf: exp(-(x - 1.000)^2 / (2.000*2^2)) / sqrt(2 * pi * 2.000^2)\npdf: exp(-(x - 2.000)^2 / (2.000*2^2)) / sqrt(2 * pi * 2.000^2)\npdf: exp(-(x - 3.000)^2 / (3.000*2^2)) / sqrt(2 * pi * 3.000^2)\npdf: exp(-(x - 1.000)^2 / (2.236*2^2)) / sqrt(2 * pi * 2.236^2)\npdf: exp(-(x - 3.000)^2 / (3.000*2^2)) / sqrt(2 * pi * 3.000^2)\nTrue\n\n\n\n\nAn operator like + may have a different meaning depending on the context (e.g., addition of numbers, concatenation of strings, etc.). This is called operator overloading, or sometimes ad hoc polymorphism.\n\n\n\nClasses can inherit methods from other classes. You can use super (Latin word for “above”) to access the parent class from a child class. This is useful when you want to extend the functionality of the inherited method.\nA simple test consists of checking whether a class has inherited from another one. issubclass allows one to check this heritage property. For instance, we can check that the following IsoGaussian is a subclass of the Gaussian class we have created above:\n\nclass IsoGaussian(Gaussian):\n    def __init__(self, mean):\n        super().__init__(mean, 1.0)\n\n\ngg1 = IsoGaussian(3)\nprint(gg1)\nprint(issubclass(IsoGaussian, Gaussian))\n\npdf: exp(-(x - 3.000)^2 / (1.000*2^2)) / sqrt(2 * pi * 1.000^2)\nTrue\n\n\n\n\n\n\n\n\nTo go further\n\n\n\nFor more information on super, see for instance this Real Python Tutorials.\n\n\n\n\n\n\n\n\nEXERCISE: inheritance\n\n\n\nWhat is the inheritance for the LogisticRegression class in scikit-learn? In particular, not that inheritance could be multiple, i.e.. a class can inherit from several classes.\nHint: use the __bases__ attribute of the class.\n\nfrom sklearn.linear_model import LogisticRegression\n\na = LogisticRegression()\nprint(a.__class__.__bases__)\n\n(&lt;class 'sklearn.linear_model._base.LinearClassifierMixin'&gt;, &lt;class 'sklearn.linear_model._base.SparseCoefMixin'&gt;, &lt;class 'sklearn.base.BaseEstimator'&gt;)\n\n\n\n\nMany other examples can be found in the scikit-learn package (see for instance the module Linear Model often used in machine learning or statistics).",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#classes",
    "href": "Courses/OPP/Classes_Exceptions.html#classes",
    "title": "Classes & Exceptions",
    "section": "",
    "text": "Classes are central elements in Object-oriented programming (OOP)\nA class structure defines an object, its properties, and all the operations one can apply to it. Moreover, it allows the creation of multiple instances of the same object, each with its own properties, and to apply the same operations to all of them. Another key concept is the one of inheritance, defining a new class from an existing one by simply inheriting all its properties and operations.\nIn Python, a class contains attributes (variables) and methods (functions). Syntactically, it is defined similarly to a function, replacing the def keyword with class and requiring a colon : at the end of the first line. The name of a class should be CamelCase1 (or CapWords).\nFor instance, in sklearn, the Logistic Regression class is named LogisticRegression, and can be loaded and investigated as follows:\n\nfrom sklearn.linear_model import LogisticRegression\ndir(LogisticRegression)\n\n\n\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_check_feature_names',\n '_check_n_features',\n '_estimator_type',\n '_get_param_names',\n '_get_tags',\n '_more_tags',\n '_predict_proba_lr',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_validate_data',\n 'decision_function',\n 'densify',\n 'fit',\n 'get_params',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'score',\n 'set_params',\n 'sparsify']\n\n\n\n\nIn particular, you can already discover the attributes and methods shared to all classes (e.g., __class__, __init__, __doc__,), and the one specific to the LogisticRegression class (e.g., fit, get_params, predict, etc.).\nUsually, a class contains some class methods that can be seen as functions inside the class.\n\nThe first argument of a (non-static) method is usually called self: it is a mandatory element. This self argument is for self-reference. Self is a convention and not a Python keyword, so any other name can be used instead of self (e.g., this or me or in_place_of_self).\nSome method names have a specific meaning, for instance:\n\n__init__: name of the method invoked when creating the object (instantiation)\n__call__: method invoked when calling the object\n__str__: method invoked when a class has a representation as a string, e.g., when passing it to the print function\nsee Python documentation for more special names.\n\n\n\n\nLet us define a simple Point class, representing a point in the plane (i.e., a point in \\(\\mathbb{R}^2\\)):\n\nclass Point(object):\n    \"\"\"A class to represent planar points.\"\"\"\n\n    def __init__(self, x, y):\n        \"\"\"Create a new point (x, y).\"\"\"\n        self.x = x\n        self.y = y\n\n    def translate(self, dx, dy):\n        \"\"\"Translate the point by dx and dy.\"\"\"\n        self.x += dx\n        self.y += dy\n\n    def __str__(self):\n        return f\"Point: [{self.x:.3f}, {self.y:.3f}]\"\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are not familiar with printing in Python, start with the new f-strings format; see for instance: https://zetcode.com/python/fstring/.\n\n\nTo create a new instance of the class Point you run the following code:\n\np1 = Point(x=0, y=0)  # call __init__ ;\n\nNow, you can access the attributes of the object p1:\n\nprint(p1.x, p1.y)\nprint(f\"{p1}\")  # call __str__\nprint(p1)\nprint(p1.__doc__)\n\n0 0\nPoint: [0.000, 0.000]\nPoint: [0.000, 0.000]\nA class to represent planar points.\n\n\nTo apply our translate method to the point p1:\n\np1.translate(dx=1, dy=1)\nprint(p1.translate)\nprint(p1)\nprint(type(p1))\n\n&lt;bound method Point.translate of &lt;__main__.Point object at 0x7cb8dc31ff10&gt;&gt;\nPoint: [1.000, 1.000]\n&lt;class '__main__.Point'&gt;\n\n\nImplicitly, the previous command is equivalent to Point.translate(p1, dx=1, dy=1). Indeed, you can check the output of the following command:\n\np1 = Point(x=0, y=0)\nPoint.translate(p1, dx=1, dy=1)\nprint(p1)\n\nPoint: [1.000, 1.000]\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might have already used that syntax with the “dot” (.) notation in numpy, for instance when executing a numpy function as follows\n\nimport numpy as np\nrng = np.random.default_rng(seed=12345)\na = rng.random((3, 3))\na.mean(axis=0)\n\narray([0.50063315, 0.29820069, 0.60097848])\n\n\nIn that case, a was an instance of the numpy.ndarray class (see numpy documentation for details on this class):\n\ntype(a)\n\nnumpy.ndarray\n\n\n\n\n\n\n\nMyClass(arg1, arg2, ...) is a shorthand for MyClass.__call__(arg1, arg2, ...), so this allows writing classes where the instances behave like functions and can be called like a function\n\nclass Sum:\n    def __init__(self):\n        print(\"Instance Created\")\n\n    # Defining __call__ method\n    def __call__(self, a, b):\n        print(a + b)\n\n# Instance created\nsum_as_a_function = Sum()\n\n# __call__ method will be called\nsum_as_a_function(10, 20)\n\nInstance Created\n30\n\n\nA test function of interest is isinstance which allows checking if an object is of the correct class. For instance, one can check if sum_as_a_function is an instance of Sum\n\nisinstance(sum_as_a_function, Sum)\n\nTrue\n\n\nor an instance of Point:\n\nisinstance(sum_as_a_function, Point)\n\nFalse\n\n\nAnother example of a callable is given below with a simple Counter class:\n\nclass Counter:\n    \"\"\"A simple counter class.\"\"\"\n\n    def __init__(self):\n        self.count = 0\n\n    def increment(self):\n        self.count += 1\n\n    def __call__(self):\n        self.increment()\n\nThen, performing the following commands you can check how the __call__ method is called:\n\ncounter = Counter()\nprint(counter.count)\ncounter.increment()\nprint(counter.count)\ncounter()\nprint(counter.count)\n\n0\n1\n2\n\n\nIn particular, note that a method of a class can modify the state of a particular instance. This does not alter the other instantiations of the class. \n\n\n\n\n\n\nTo go further\n\n\n\nThis example and more details on callable are described in detail in this Real Python post.\n\n\n\n\n\n\n\n\nEXERCISE: Gaussian class\n\n\n\nImplement a class Gaussian with attributes mean and std with a method\n\n__str__ returning a string with the expression of the density\n__eq__ testing the equality of two instances. You should use numpy.isclose()\n__add__ implementing the (left) addition of independent Gaussian, or more precisely their pdfs (probability density functions)\n__radd__ implementing the (right) addition of independent Gaussian, or more precisely their pdfs (probability density functions)\n\nNote: when executing a+b what really happens is that the add method of the a: object is called a.__add__(b). See stackoverflow for more details on the __add__ and __radd__ methods.\n\ng1 = Gaussian(0.0, 1.0)\ng2 = Gaussian(1.0, 2.0)\ng3 = Gaussian(2.0, 2.0)\ng4 = Gaussian(3.0, 3.0)\n\nprint(g1)\nprint(g2)\nprint(g3)\nprint(g4)\nprint(g2 + g1)\nprint(sum([g1, g2, g3]))\nprint(sum([g1, g2, g3]) == g4)\n\npdf: exp(-(x - 0.000)^2 / (1.000*2^2)) / sqrt(2 * pi * 1.000^2)\npdf: exp(-(x - 1.000)^2 / (2.000*2^2)) / sqrt(2 * pi * 2.000^2)\npdf: exp(-(x - 2.000)^2 / (2.000*2^2)) / sqrt(2 * pi * 2.000^2)\npdf: exp(-(x - 3.000)^2 / (3.000*2^2)) / sqrt(2 * pi * 3.000^2)\npdf: exp(-(x - 1.000)^2 / (2.236*2^2)) / sqrt(2 * pi * 2.236^2)\npdf: exp(-(x - 3.000)^2 / (3.000*2^2)) / sqrt(2 * pi * 3.000^2)\nTrue\n\n\n\n\nAn operator like + may have a different meaning depending on the context (e.g., addition of numbers, concatenation of strings, etc.). This is called operator overloading, or sometimes ad hoc polymorphism.\n\n\n\nClasses can inherit methods from other classes. You can use super (Latin word for “above”) to access the parent class from a child class. This is useful when you want to extend the functionality of the inherited method.\nA simple test consists of checking whether a class has inherited from another one. issubclass allows one to check this heritage property. For instance, we can check that the following IsoGaussian is a subclass of the Gaussian class we have created above:\n\nclass IsoGaussian(Gaussian):\n    def __init__(self, mean):\n        super().__init__(mean, 1.0)\n\n\ngg1 = IsoGaussian(3)\nprint(gg1)\nprint(issubclass(IsoGaussian, Gaussian))\n\npdf: exp(-(x - 3.000)^2 / (1.000*2^2)) / sqrt(2 * pi * 1.000^2)\nTrue\n\n\n\n\n\n\n\n\nTo go further\n\n\n\nFor more information on super, see for instance this Real Python Tutorials.\n\n\n\n\n\n\n\n\nEXERCISE: inheritance\n\n\n\nWhat is the inheritance for the LogisticRegression class in scikit-learn? In particular, not that inheritance could be multiple, i.e.. a class can inherit from several classes.\nHint: use the __bases__ attribute of the class.\n\nfrom sklearn.linear_model import LogisticRegression\n\na = LogisticRegression()\nprint(a.__class__.__bases__)\n\n(&lt;class 'sklearn.linear_model._base.LinearClassifierMixin'&gt;, &lt;class 'sklearn.linear_model._base.SparseCoefMixin'&gt;, &lt;class 'sklearn.base.BaseEstimator'&gt;)\n\n\n\n\nMany other examples can be found in the scikit-learn package (see for instance the module Linear Model often used in machine learning or statistics).",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#exceptions",
    "href": "Courses/OPP/Classes_Exceptions.html#exceptions",
    "title": "Classes & Exceptions",
    "section": "Exceptions",
    "text": "Exceptions\nThis section is inspired by Fabien Maussion’s lecture on Scientific Programming, and describes how to handle exceptions in python.\n\nIn python errors are handled through Exceptions\nAn error throws an Exception interrupting the normal code execution\nExecution can overpass such an issue inside a bloc with try - except\nA typical use case: stop the program when an error occurs:\n\ndef my_function(arguments):\n    if not verify(arguments):\n        raise Exception(\"Invalid arguments\")\n\n    # ...\n    # Keep working if the exception is not raised\n\n\n\n\n\n\nTo go further\n\n\n\nThe list of possible errors is available here: https://docs.python.org/3/library/exceptions.html#bltin-exceptions and includes NameError, ImportError, AssertionError etc.\n\n\n\ntry - except - finally syntax\ntry - except - finally is a syntax to handle exceptions in python, and it helps prevent errors from stopping a program. The syntax is the following:\ntry:\n    # Part 1: Normal code goes here\nexcept:\n    # Part 2: Code for error handling goes here\n    # This code is not executed unless Part 1\n    # above generated an error\nfinally:\n    # Optional: This clause is executed no matter what,\n    # and is generally used to release external resources.\nLet us consider the following example to understand the syntax better:\n\ntry:\n    print(\"test_var testing:\")\n    e = 4\n    print(test_var)  # raise an error: the test_var variable is not defined\nexcept NameError:\n    print(\"Caught an exception: test_var does not exist\")\nfinally:\n    print(\"This code is executed no matter what\")\n\nprint(\"The program keep continuing... it does not freeze!\")\nprint(f\"Beware! the affectation step: e = {e} was executed.\")\n\ntest_var testing:\nCaught an exception: test_var does not exist\nThis code is executed no matter what\nThe program keep continuing... it does not freeze!\nBeware! the affectation step: e = 4 was executed.\n\n\nTo obtain some information on the error: it is possible to access the instance of the Exception class thrown by the program through the following syntax:\n\ntry:\n    print(\"test\")\n    print(testtt)  # Error: the variable testtt is not defined\nexcept Exception as name_exception:\n    print(\"Caught an exception:\", name_exception)\n\ntest\nCaught an exception: name 'testtt' is not defined\n\n\nA common use case is to test if the import of a package was successful or not. For instance, if you try loading a package pooooch that is not installed on your machine, the following code will raise an error:\n\nimport pooooch\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [20], line 1\n----&gt; 1 import pooooch\n\nModuleNotFoundError: No module named 'pooooch'\n\n\n\nIf you would like the program to continue even if the package is not installed, you can use the following syntax:\n\ntry:\n    import pooch\nexcept Exception as e:\n    print(e)\n\nAn exhaustive list of exceptions be found here: https://docs.python.org/3/library/exceptions.html.\n\n\nThe with statement\n\n\n\n\n\n\nImportant\n\n\n\nYou have to run the following lines at a location where a directory called scripts/ containing a function hello-world.py exists. If you have not cloned the course repository, the file used in the lecture is available here. You can, of course, create your own if you prefer.\n\n\n\nfname = \"./scripts/hello-world.py\"\n\nwith open(fname) as file:  # Use file to refer to the file object\n    data = file.read()\n    print(data)\n    # at the end of the code chunk, the file.__exit__() method is called\n    # (i.e., file.close() is executed automatically)\n\ntatjpjepj.\n\n\n\nNow, let us try with a file that does not exist:\n\nfname = \"scripts/hello-world_do_not_exist.py\"\ntry:\n    # 1/0  # Uncomment at some point and run\n    file = open(fname)\n    data = file.read()\n    print(data)\nexcept FileNotFoundError:\n    print(\"File not found!\")\nexcept (RuntimeError, TypeError, NameError, ZeroDivisionError):\n    print(\"Specific Error message 2\")\nfinally:\n    file.close()  # Important line to release access to the file!\n\nFile not found!\n\n\n\n\n\n\n\n\nEXERCISE: Improving the Gaussian class\n\n\n\nCreate a sub-class GaussianBis where you check if the user has provided float arguments (see also assert and isinstance routines). Print a custom explicit error message if it is not the case.",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#scope",
    "href": "Courses/OPP/Classes_Exceptions.html#scope",
    "title": "Classes & Exceptions",
    "section": "Scope",
    "text": "Scope\nThe scope of a variable is the part of the program where the variable is accessible. In python, the scope of a variable is defined by its location in the code (i.e., where you define it).\ne = 0\nprint(e)\n\nfor i in range(1):\n    e = 1\n\nprint(e)\n\n\ndef f():\n    e = 2\n\n\nprint(e)\nConclusion: e is only “visible” inside the function definition. See https://realpython.com/python-scope-legb-rule/ for more information on this topic.",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#manipulate-file-names-across-platforms",
    "href": "Courses/OPP/Classes_Exceptions.html#manipulate-file-names-across-platforms",
    "title": "Classes & Exceptions",
    "section": "Manipulate file names across platforms",
    "text": "Manipulate file names across platforms\nEach OS (Linux, Windows, Mac, etc.) might have a different syntax to describe a file path. The following would avoid naming conflict due to each OS syntax and could be important for your project if you work with colleagues having a different OS from yours.\n\nimport os\n\nprint(os.path.join('~', 'work', 'src'))\nprint(os.path.join(os.getcwd(), 'new_directory'))\nos.path.expanduser?\nprint(os.path.expanduser(os.path.join('~', 'work', 'src')))\n\n~/work/src\n/home/bbensaid/Documents/Cours_MCF1/Dev_Logiciel_website/Courses/OPP/new_directory\n/home/bbensaid/work/src\n\n\n\n\n\n\n\n\nEXERCISE: Create a bunch of files\n\n\n\nWrite a simple script that creates, in the sub-directory scripts, the following text files: myDb_000.txt, myDb_001.txt, myDb_002.txt, …, myDb_049.txt. The i-th file should contain a single line with the average of the i first digits of pi.\nHint:\n\nYou might need zero padding.\nYou can check what the following code does.\n\nfile = open(\"copy.txt\")\nfile.write(\"Your text goes here\")\nfile.close()\n\nYou might also need some precision for the digits of \\(\\pi\\), hence using mpmath instead of numpy. The following code shows elements of help:\n\n\nfrom mpmath import mp\n\nn_tot = 10\nmp.dps = n_tot  # set number of digits\nprint(mp.pi)\n\nif not os.path.isdir(\"script\"):\n    os.mkdir(\"script\")\n\n\nfor i in range(2, n_tot + 2):\n    print(f\"{i:0{i}}\")\n\n3.141592654\n02\n003\n0004\n00005\n000006\n0000007\n00000008\n000000009\n0000000010\n00000000011",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#references",
    "href": "Courses/OPP/Classes_Exceptions.html#references",
    "title": "Classes & Exceptions",
    "section": "References",
    "text": "References\n\nPython official web page and its style and writing recommendation\nThink Python - A free book by Allen Downey.\nPython Essential Reference - a good reference for general Python coding\nPython Data Science Handbook - an excellent book for data science in Python by Jake VanderPlas (associated notebook available online)",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/OPP/Classes_Exceptions.html#footnotes",
    "href": "Courses/OPP/Classes_Exceptions.html#footnotes",
    "title": "Classes & Exceptions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCamelCase (🇫🇷: casse de chameau): the name comes from the “bumpy” look of its letters as in the Wikipedia illustration below ↩︎",
    "crumbs": [
      "OOP",
      "Classes & Exceptions"
    ]
  },
  {
    "objectID": "Courses/Test_Python/CI.html",
    "href": "Courses/Test_Python/CI.html",
    "title": "Continuous Integration",
    "section": "",
    "text": "In software engineering, continuous integration (CI) is the practice of merging all developers’ working copies to a shared mainline regularly. It is often split into three steps:\nA CI pipeline runs commands on some virtual machine automatically.",
    "crumbs": [
      "Testing Tools",
      "Continuous Integration"
    ]
  },
  {
    "objectID": "Courses/Test_Python/CI.html#references",
    "href": "Courses/Test_Python/CI.html#references",
    "title": "Continuous Integration",
    "section": "References",
    "text": "References\n\nGithub actions doc",
    "crumbs": [
      "Testing Tools",
      "Continuous Integration"
    ]
  },
  {
    "objectID": "Courses/Test_Python/TimeMemory.html",
    "href": "Courses/Test_Python/TimeMemory.html",
    "title": "Sparse matrices, graphs and maps",
    "section": "",
    "text": "Sparse matrices are useful to handle potentially huge matrices with structures. The most standard structure is sparsity, i.e., matrices that have only a few non-zero coefficients.\nThe most common formats are\n\nCOO_matrix(arg1[, shape, dtype, copy]): A sparse matrix in COOrdinate format.\ncsr_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Row matrix (default format in scipy)\ncsc_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Column matrix\n\nBelow are examples of common scenarios where sparse matrices are often used:\n\n\nA matrix might encode the presence/absence in a text of a word from a dictionary (say the set of French words). The number 0 (resp. 1) represents the presence (resp. absence) of a word in a text.\n\n\n\nTo handle categorical data encoding as strings, many algorithms require that you first transform such variables as numerical vectors. The simplest protocol is to create one column to represent each modality of the categorical variable and to set the value to 1 if the observation is of that modality, and 0 otherwise. This is called one-hot encoding or dummy encoding. By construction, this can be encoded into a sparse (binary) matrix.\nAn example with pandas and sklearn is given below (recall that 0 represents False and 1 represents True):\n\nimport os\nimport pandas as pd\nimport pooch  # download data / avoid re-downloading\nfrom sklearn import datasets\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\nurl = \"http://josephsalmon.eu/enseignement/datasets/titanic.csv\"\npath_target = \"./titanic.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n\ncategorical_column = ['Embarked']\ndf_titanic_raw = pd.read_csv(\"titanic.csv\", usecols=categorical_column)\nprint(df_titanic_raw.info())\nprint(df_titanic_raw.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Embarked  889 non-null    object\ndtypes: object(1)\nmemory usage: 7.1+ KB\nNone\n  Embarked\n0        S\n1        C\n2        S\n3        S\n4        S\n\n\npandas approach with get_dummies:\n\ndummies = pd.get_dummies(df_titanic_raw.Embarked, sparse=True)\ndummies.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype              \n---  ------  --------------  -----              \n 0   C       1 non-null      Sparse[bool, False]\n 1   Q       1 non-null      Sparse[bool, False]\n 2   S       1 non-null      Sparse[bool, False]\ndtypes: Sparse[bool, False](3)\nmemory usage: 4.5 KB\n\n\nsklearn approach with OneHotEncoder:\n\ntransformer=make_column_transformer((OneHotEncoder(), ['Embarked']), remainder=\"passthrough\")\nmatrix_embarked = transformer.fit_transform(df_titanic_raw)\nprint(matrix_embarked)\nprint(type(matrix_embarked))\n\n\n\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 891 stored elements and shape (891, 4)&gt;\n  Coords    Values\n  (0, 2)    1.0\n  (1, 0)    1.0\n  (2, 2)    1.0\n  (3, 2)    1.0\n  (4, 2)    1.0\n  (5, 1)    1.0\n  (6, 2)    1.0\n  (7, 2)    1.0\n  (8, 2)    1.0\n  (9, 0)    1.0\n  (10, 2)   1.0\n  (11, 2)   1.0\n  (12, 2)   1.0\n  (13, 2)   1.0\n  (14, 2)   1.0\n  (15, 2)   1.0\n  (16, 1)   1.0\n  (17, 2)   1.0\n  (18, 2)   1.0\n  (19, 0)   1.0\n  (20, 2)   1.0\n  (21, 2)   1.0\n  (22, 1)   1.0\n  (23, 2)   1.0\n  (24, 2)   1.0\n  : :\n  (866, 0)  1.0\n  (867, 2)  1.0\n  (868, 2)  1.0\n  (869, 2)  1.0\n  (870, 2)  1.0\n  (871, 2)  1.0\n  (872, 2)  1.0\n  (873, 2)  1.0\n  (874, 0)  1.0\n  (875, 0)  1.0\n  (876, 2)  1.0\n  (877, 2)  1.0\n  (878, 2)  1.0\n  (879, 0)  1.0\n  (880, 2)  1.0\n  (881, 2)  1.0\n  (882, 2)  1.0\n  (883, 2)  1.0\n  (884, 2)  1.0\n  (885, 1)  1.0\n  (886, 2)  1.0\n  (887, 2)  1.0\n  (888, 2)  1.0\n  (889, 0)  1.0\n  (890, 1)  1.0\n&lt;class 'scipy.sparse._csr.csr_matrix'&gt;\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe differences between the two approaches are: - the encoding of absence/presence (True/False vs. 0/1) - the output created handles missing values differently by default\n\n\n\n\n\nFor instance, when very distant particles/objects have little influence, their value can be set to zero when representing physical quantities (e.g., heat diffusion, fluid mechanics, electro/magnetism, etc.)\n\n\n\nGraphs are naturally represented by adjacency or incidence matrices (cf. below), and therefore beyond the graphs, maps! We illustrate that in the last section of the course.\nReferences:\n\nScipy Lectures: why sparse matrices\nSparse data structures in Python, by Artem Golubin\nIntroduction to Sparse Matrices in Python with SciPy, by cmdlinetips\nScipy doc on sparse matrices\n\n\n\n\n\nfrom scipy import sparse\nfrom scipy.sparse import isspmatrix\n\nId = sparse.eye(3)\nprint(Id.toarray())\nprint(f'Q: Is the matrix Id is sparse?\\nA: {isspmatrix(Id)}')\n\nn1 = 9\nn2 = 9\nmat_rnd = sparse.rand(n1, n2, density=0.25, format=\"csr\",\n                      random_state=42)\nprint(mat_rnd.toarray())\nprint(f'Q: Is the matrix mat_rnd sparse?\\nA: {isspmatrix(mat_rnd)}')\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nQ: Is the matrix Id is sparse?\nA: True\n[[0.54269608 0.         0.07455064 0.         0.         0.11586906\n  0.         0.         0.        ]\n [0.         0.77224477 0.         0.98688694 0.         0.\n  0.33089802 0.         0.86310343]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.        ]\n [0.         0.81546143 0.         0.28093451 0.         0.\n  0.         0.         0.        ]\n [0.00552212 0.         0.14092422 0.80219698 0.06355835 0.70685734\n  0.         0.77127035 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.35846573 0.        ]\n [0.         0.         0.         0.72900717 0.         0.\n  0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.62329813 0.19871568 0.        ]\n [0.         0.         0.         0.07404465 0.         0.\n  0.         0.         0.        ]]\nQ: Is the matrix mat_rnd sparse?\nA: True\n\n\nA matrix-vector product can be performed as usual, with the @ operators:\n\nimport numpy as np\nv = np.random.rand(n2)\nmat_rnd@v\n\narray([0.61268795, 2.07114702, 0.        , 0.92608394, 1.49826155,\n       0.11229136, 0.63055999, 0.48868782, 0.06404545])\n\n\n\n\n\n\n\n\nEXERCISE: Linear models & sparse matrices\n\n\n\nCreate a function that can fit ordinary least squares for sparse matrices (or not). In particular, handle the usual pre-processing step of standardizing the columns of the design matrix (i.e., centering columns and dividing by standard deviation)? Beware: often you cannot center a sparse matrix without making it non-sparse.\n\n\nThe choice of a sparse format depends on the nature and structure of the data. For instance: - csc_matrix is more efficient for slicing by column, - csr_matrix is more efficient for slicing by row.",
    "crumbs": [
      "Testing Tools",
      "Sparse matrices, graphs and maps"
    ]
  },
  {
    "objectID": "Courses/Test_Python/TimeMemory.html#memory-efficiency-sparse-matrices",
    "href": "Courses/Test_Python/TimeMemory.html#memory-efficiency-sparse-matrices",
    "title": "Sparse matrices, graphs and maps",
    "section": "",
    "text": "Sparse matrices are useful to handle potentially huge matrices with structures. The most standard structure is sparsity, i.e., matrices that have only a few non-zero coefficients.\nThe most common formats are\n\nCOO_matrix(arg1[, shape, dtype, copy]): A sparse matrix in COOrdinate format.\ncsr_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Row matrix (default format in scipy)\ncsc_matrix(arg1[, shape, dtype, copy]): Compressed Sparse Column matrix\n\nBelow are examples of common scenarios where sparse matrices are often used:\n\n\nA matrix might encode the presence/absence in a text of a word from a dictionary (say the set of French words). The number 0 (resp. 1) represents the presence (resp. absence) of a word in a text.\n\n\n\nTo handle categorical data encoding as strings, many algorithms require that you first transform such variables as numerical vectors. The simplest protocol is to create one column to represent each modality of the categorical variable and to set the value to 1 if the observation is of that modality, and 0 otherwise. This is called one-hot encoding or dummy encoding. By construction, this can be encoded into a sparse (binary) matrix.\nAn example with pandas and sklearn is given below (recall that 0 represents False and 1 represents True):\n\nimport os\nimport pandas as pd\nimport pooch  # download data / avoid re-downloading\nfrom sklearn import datasets\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\nurl = \"http://josephsalmon.eu/enseignement/datasets/titanic.csv\"\npath_target = \"./titanic.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n\ncategorical_column = ['Embarked']\ndf_titanic_raw = pd.read_csv(\"titanic.csv\", usecols=categorical_column)\nprint(df_titanic_raw.info())\nprint(df_titanic_raw.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Embarked  889 non-null    object\ndtypes: object(1)\nmemory usage: 7.1+ KB\nNone\n  Embarked\n0        S\n1        C\n2        S\n3        S\n4        S\n\n\npandas approach with get_dummies:\n\ndummies = pd.get_dummies(df_titanic_raw.Embarked, sparse=True)\ndummies.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype              \n---  ------  --------------  -----              \n 0   C       1 non-null      Sparse[bool, False]\n 1   Q       1 non-null      Sparse[bool, False]\n 2   S       1 non-null      Sparse[bool, False]\ndtypes: Sparse[bool, False](3)\nmemory usage: 4.5 KB\n\n\nsklearn approach with OneHotEncoder:\n\ntransformer=make_column_transformer((OneHotEncoder(), ['Embarked']), remainder=\"passthrough\")\nmatrix_embarked = transformer.fit_transform(df_titanic_raw)\nprint(matrix_embarked)\nprint(type(matrix_embarked))\n\n\n\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 891 stored elements and shape (891, 4)&gt;\n  Coords    Values\n  (0, 2)    1.0\n  (1, 0)    1.0\n  (2, 2)    1.0\n  (3, 2)    1.0\n  (4, 2)    1.0\n  (5, 1)    1.0\n  (6, 2)    1.0\n  (7, 2)    1.0\n  (8, 2)    1.0\n  (9, 0)    1.0\n  (10, 2)   1.0\n  (11, 2)   1.0\n  (12, 2)   1.0\n  (13, 2)   1.0\n  (14, 2)   1.0\n  (15, 2)   1.0\n  (16, 1)   1.0\n  (17, 2)   1.0\n  (18, 2)   1.0\n  (19, 0)   1.0\n  (20, 2)   1.0\n  (21, 2)   1.0\n  (22, 1)   1.0\n  (23, 2)   1.0\n  (24, 2)   1.0\n  : :\n  (866, 0)  1.0\n  (867, 2)  1.0\n  (868, 2)  1.0\n  (869, 2)  1.0\n  (870, 2)  1.0\n  (871, 2)  1.0\n  (872, 2)  1.0\n  (873, 2)  1.0\n  (874, 0)  1.0\n  (875, 0)  1.0\n  (876, 2)  1.0\n  (877, 2)  1.0\n  (878, 2)  1.0\n  (879, 0)  1.0\n  (880, 2)  1.0\n  (881, 2)  1.0\n  (882, 2)  1.0\n  (883, 2)  1.0\n  (884, 2)  1.0\n  (885, 1)  1.0\n  (886, 2)  1.0\n  (887, 2)  1.0\n  (888, 2)  1.0\n  (889, 0)  1.0\n  (890, 1)  1.0\n&lt;class 'scipy.sparse._csr.csr_matrix'&gt;\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe differences between the two approaches are: - the encoding of absence/presence (True/False vs. 0/1) - the output created handles missing values differently by default\n\n\n\n\n\nFor instance, when very distant particles/objects have little influence, their value can be set to zero when representing physical quantities (e.g., heat diffusion, fluid mechanics, electro/magnetism, etc.)\n\n\n\nGraphs are naturally represented by adjacency or incidence matrices (cf. below), and therefore beyond the graphs, maps! We illustrate that in the last section of the course.\nReferences:\n\nScipy Lectures: why sparse matrices\nSparse data structures in Python, by Artem Golubin\nIntroduction to Sparse Matrices in Python with SciPy, by cmdlinetips\nScipy doc on sparse matrices\n\n\n\n\n\nfrom scipy import sparse\nfrom scipy.sparse import isspmatrix\n\nId = sparse.eye(3)\nprint(Id.toarray())\nprint(f'Q: Is the matrix Id is sparse?\\nA: {isspmatrix(Id)}')\n\nn1 = 9\nn2 = 9\nmat_rnd = sparse.rand(n1, n2, density=0.25, format=\"csr\",\n                      random_state=42)\nprint(mat_rnd.toarray())\nprint(f'Q: Is the matrix mat_rnd sparse?\\nA: {isspmatrix(mat_rnd)}')\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nQ: Is the matrix Id is sparse?\nA: True\n[[0.54269608 0.         0.07455064 0.         0.         0.11586906\n  0.         0.         0.        ]\n [0.         0.77224477 0.         0.98688694 0.         0.\n  0.33089802 0.         0.86310343]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.        ]\n [0.         0.81546143 0.         0.28093451 0.         0.\n  0.         0.         0.        ]\n [0.00552212 0.         0.14092422 0.80219698 0.06355835 0.70685734\n  0.         0.77127035 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.35846573 0.        ]\n [0.         0.         0.         0.72900717 0.         0.\n  0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.62329813 0.19871568 0.        ]\n [0.         0.         0.         0.07404465 0.         0.\n  0.         0.         0.        ]]\nQ: Is the matrix mat_rnd sparse?\nA: True\n\n\nA matrix-vector product can be performed as usual, with the @ operators:\n\nimport numpy as np\nv = np.random.rand(n2)\nmat_rnd@v\n\narray([0.61268795, 2.07114702, 0.        , 0.92608394, 1.49826155,\n       0.11229136, 0.63055999, 0.48868782, 0.06404545])\n\n\n\n\n\n\n\n\nEXERCISE: Linear models & sparse matrices\n\n\n\nCreate a function that can fit ordinary least squares for sparse matrices (or not). In particular, handle the usual pre-processing step of standardizing the columns of the design matrix (i.e., centering columns and dividing by standard deviation)? Beware: often you cannot center a sparse matrix without making it non-sparse.\n\n\nThe choice of a sparse format depends on the nature and structure of the data. For instance: - csc_matrix is more efficient for slicing by column, - csr_matrix is more efficient for slicing by row.",
    "crumbs": [
      "Testing Tools",
      "Sparse matrices, graphs and maps"
    ]
  },
  {
    "objectID": "Courses/Test_Python/TimeMemory.html#graphs-and-sparsity",
    "href": "Courses/Test_Python/TimeMemory.html#graphs-and-sparsity",
    "title": "Sparse matrices, graphs and maps",
    "section": "Graphs and sparsity",
    "text": "Graphs and sparsity\nA classical field for the application of sparse matrices is when using graphs: although the number of nodes can be huge, each node of a graph is in general not connected to all nodes. Let us represent a graph by its adjacency matrix:\n\nDefinition: adjacency matrix\nSuppose that \\(G=(V,E)\\) is a graph, where \\(\\left|V\\right|=n\\). Suppose that the vertices of \\(G\\) are arbitrarily numbered \\(v_1,\\ldots,v_n.\\) The adjacency matrix \\(A\\) of \\(G\\) is the matrix \\(n \\times n\\) of general term:\n\\[\nA_{{i,j}}=\n\\left\\{\n     \\begin{array}{rl}\n         1, & \\text{if } (v_i,v_j) \\in E \\\\\n         0, & \\text{o.w.}\n      \\end{array}\n\\right.\n\\]\nNote that instead of 1, the value could vary on a per-edge basis (cf. Figure 1).\nFor standard graph manipulation the package networkx is very useful.\n\nimport networkx as nx\n\nIt allows one to define a graph very easily:\n\nG = nx.Graph()\nG.add_edge('A', 'B', weight=4)\nG.add_edge('A', 'C', weight=3)\nG.add_edge('B', 'D', weight=2)\nG.add_edge('C', 'D', weight=4)\nG.add_edge('D', 'A', weight=2)\n\nand then visualize it:\n\nimport matplotlib.pyplot as plt\nmy_seed = 44\nnx.draw_networkx(\n    G, with_labels=True,\n    node_size=1000,\n    pos=nx.spring_layout(G, seed=my_seed)\n)\n\nlabels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edges(\n    G,\n    pos=nx.spring_layout(G, seed=my_seed),\n    width=[6 * i for i in list(labels.values())],\n)\nnx.draw_networkx_edge_labels(\n    G, pos=nx.spring_layout(G, seed=my_seed),\n    edge_labels=labels\n)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Plot a simple graph\n\n\n\n\n\nYou can get (a variant of) the adjacency matrix using\n\nA = nx.adjacency_matrix(G)\nprint(A.todense(), f\"Q: Is A spase?\\nA: {isspmatrix(A)}\")\n\n[[0 4 3 2]\n [4 0 0 2]\n [3 0 0 4]\n [2 2 4 0]] Q: Is A spase?\nA: False\n\n\nwhere the adjacency matrix is stored in a sparse format, and can also encode the weights of the edges.\nWith networkx, it is simple to get the shortest path between two nodes in a graph using the shortest_path function:\n\nshortest_path = nx.shortest_path(G, 'C', 'B', weight='weight')\nstr_shortest_path = ' -&gt; '.join(shortest_path)\nprint(f\"The shortest path between C and B is:\\n {str_shortest_path}\")\n\nThe shortest path between C and B is:\n C -&gt; D -&gt; B\n\n\n\n\nDefinition : incidence matrix\nLet \\(G = (V,E)\\) be a (non-oriented) graph with \\(n\\) vertices, \\(V = [1,\\dots,n]\\), and \\(p\\) edges, \\(E = [1,\\dots,p]\\). The graph can be represented by its vertex-edge incidence matrix \\(D^\\top \\in \\mathbb{R}^{p \\times n}\\) defined by\n\\[\n(D^\\top)_{{e,v}} =\n\\left\\{\n     \\begin{array}{rl}\n    + 1, & \\text{if } v = \\min(i,j) \\\\\n    -1, & \\text{si } v = \\max(i,j) \\\\\n    0, & \\text{sinon}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(e = (i,j)\\).\n\n\nDefinition : Laplacian matrix\nThe matrix \\(L=D D^\\top\\) is the so-called graph Laplacian of \\(G\\)\n\nD = nx.incidence_matrix(G, oriented=True).T\nL = nx.laplacian_matrix(G)\nprint(isspmatrix(D))\nprint(D.todense(), L.todense())\n\nFalse\n[[-1.  1.  0.  0.]\n [-1.  0.  1.  0.]\n [-1.  0.  0.  1.]\n [ 0. -1.  0.  1.]\n [ 0.  0. -1.  1.]] [[ 9 -4 -3 -2]\n [-4  6  0 -2]\n [-3  0  7 -4]\n [-2 -2 -4  8]]\n\n\n\n\nMore graph visualizations\n\ng = nx.karate_club_graph()\n\n# Return a list of tuples each tuple is (node, deg)\nlist_degree = list(g.degree())\n# Build a node list and corresponding degree list\nnodes, degree = map(list, zip(*list_degree))\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nnx.draw(\n    g,\n    ax=ax,\n    nodelist=nodes,\n    node_size=[(v * 30) + 1 for v in degree],\n    width=4,\n    alpha=0.7,\n    edgecolors=\"white\",\n    node_color=\"#1f78b4\",\n    edge_color=\"#1f78b4\",\n)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBack to sparse matrices\n\nA = nx.adjacency_matrix(g).T\nprint(A.todense())\n\nfig, ax = plt.subplots()\nax = plt.spy(A)\nprint(f\"Pourcentage of active edges: {(g.number_of_edges() / g.number_of_nodes()**2) * 100:.2f} %\")\n\n[[0 4 5 ... 2 0 0]\n [4 0 6 ... 0 0 0]\n [5 6 0 ... 0 2 0]\n ...\n [2 0 0 ... 0 4 4]\n [0 0 2 ... 4 0 5]\n [0 0 0 ... 4 5 0]]\nPourcentage of active edges: 6.75 %\n\n\n\n\n\n\n\n\n\nRemark: a possible visualization with Javascript (not so stable though, can be skipped)\n\nInteractive Networks with networkx and d3\nner2sna: Entity Extraction and Network Analysis\n\n\n\nPlanar graphs and maps\nOpen Street Map can interface with networkx using the osmnx package.\n\nThe osmnx package\nKnown bug for old versions: - Cannot import name ‘CRS’ from ‘pyproj’ in osmnx\n\nTypeError: argument of type ‘CRS’ is not iterable” with osmnx\n\nSo pick version 0.14 at least conda install osmnx&gt;=0.14 or pip install osmnx&gt;=0.10.\nFor Windows users, there might be some trouble with installing the fiona package, see:\n\nInstalling geopandas:” A GDAL API version must be specified (anaconda)\nInstall fiona on Windows\n\nSpecial case for osmnx on Windows follow the next step in order:\n\npip install osmnx\npip install Rtree\nconda install -c conda-forge libspatialindex=1.9.3\npip install osmnx\nInstall all packages required up to fiona.\nconda install -c conda-forge geopandas\nSay yes to everything\nOnce done, launch pip install osmnx==1.0.1\n\n\nimport osmnx as ox\nox.settings.use_cache=True\nox.__version__\n\n'2.0.5'\n\n\n\nG = ox.graph_from_place('Montpellier, France', network_type='bike')\nprint(f\"nb edges: {G.number_of_edges()}\")\nprint(f\"nb nodes: {G.number_of_nodes()}\")\n\nnb edges: 33857\nnb nodes: 15343\n\n\n\nfig, ax = ox.plot_graph(G)\n\n\n\n\n\n\n\n\nYou also need to install the package folium for the visualization of the maps.\n\nimport folium\n\n\nmap_osm = folium.Map(location=[43.610769, 3.876716])\n\n\nmap_osm.add_child(folium.RegularPolygonMarker(location=[43.610769, 3.876716],\n                  fill_color='#132b5e', radius=5))\nmap_osm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nD = nx.incidence_matrix(G, oriented=True).T\n\n\nelement = np.zeros(1, dtype=float)\nmem = np.prod(D.shape) * element.data.nbytes / (1024**2)\nprint('Size of full matrix with zeros: {0:3.2f}  MB'.format(mem))\n\nprint('Size of sparse matrix: {0:3.2f}  MB'.format(D.data.nbytes/(1024**2) ))\n\nprint('Ratio  of full matrix size / sparse: {0:3.2f}%'.format(100 * D.data.nbytes / (1024**2 * mem)))\nprint(isspmatrix(D))\n\nSize of full matrix with zeros: 3963.23  MB\nSize of sparse matrix: 0.51  MB\nRatio  of full matrix size / sparse: 0.01%\nFalse\n\n\nAlternatively: you can uncomment the following line, and check that the size of a similar matrix (with a non-sparse format) would be huge,\n&gt;&gt;&gt; Size of a full matrix encoding the zeros: 4 gB\nCreate a matrix of similar size. BEWARE: This creates a huge matrix:\n\nM = np.random.randn(G.number_of_nodes(), G.number_of_nodes())\nprint('Size of a full encoding the zeros: {0:3.2f}  MB'.format(M.nbytes/(1024**2)))\n\nSize of a full encoding the zeros: 1796.02  MB\n\n\nNow you can check the memory usage of the sparse matrix representation associated:\n\nprint(\" {0:.2} % of edges only are needed to represent the graph of Montpellier\".format(100 * G.number_of_edges() / G.number_of_nodes() ** 2))\n\n 0.014 % of edges only are needed to represent the graph of Montpellier\n\n\nHopefully, this is convincing enough to use sparse matrices when dealing with graphs and maps!\n\n\nVisualize the shortest path between two points\nReferences:\n\nOpenStreetMap Roads Data Using osmnx\n\n\norigin = ox.geocoder.geocode('Place Eugène Bataillon, Montpellier, France')\ndestination = ox.geocoder.geocode('Maison du Lez, Montpellier, France')\n\norigin_node = ox.nearest_nodes(G, origin[1], origin[0])\ndestination_node = ox.nearest_nodes(G, destination[1], destination[0])\n\nprint(origin)\nprint(destination)\nroute = ox.routing.shortest_path(G, origin_node, destination_node)\nroute_back = ox.routing.shortest_path(G, destination_node, origin_node)\n\n(43.6308638, 3.8609485)\n(43.6103224, 3.8966295)\n\n\n\nfig, ax = ox.plot_graph_routes(G, [route, route_back], route_linewidth=6, route_colors=['red', 'blue'], node_size=0)\n\n\n\n\n\n\n\n\nVisualize the same routes, but on a map:\n\n'''\nroute_edges = ox.utils_graph.route_to_gdf(G, route)\nroute_back_edges = ox.utils_graph.route_to_gdf(G, route_back)\n\nm = route_edges.explore(color=\"red\", style_kwds={\"weight\": 5, \"opacity\": 0.75})\nm = route_back_edges.explore(m=m, color=\"blue\", style_kwds={\"weight\": 5, \"opacity\": 0.75})\nm\n'''\n\n'\\nroute_edges = ox.utils_graph.route_to_gdf(G, route)\\nroute_back_edges = ox.utils_graph.route_to_gdf(G, route_back)\\n\\nm = route_edges.explore(color=\"red\", style_kwds={\"weight\": 5, \"opacity\": 0.75})\\nm = route_back_edges.explore(m=m, color=\"blue\", style_kwds={\"weight\": 5, \"opacity\": 0.75})\\nm\\n'\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that even if we call G a graph it is rather a multigraph (there might be several edges between two nodes):\n\nG.is_multigraph()\n\nTrue\n\n\n\n\nReferences:\n\nOSMnx: Python for Street Networks\nNetwork analysis in Python\nhttps://autogis-site.readthedocs.io/en/latest/index.html",
    "crumbs": [
      "Testing Tools",
      "Sparse matrices, graphs and maps"
    ]
  },
  {
    "objectID": "Courses/Git/Workflow.html",
    "href": "Courses/Git/Workflow.html",
    "title": "Workflows",
    "section": "",
    "text": "This part is not complete but for now, it gives some notions of a workflow.\n\n\nThe different collaborators can have different git using habits. A workflow is a set of recommandations to follow in order to use Git efficiently. In other words, it is a set of guidelines for all the collaborators.\nA basic worflow is the Feature Branch Workflow: a branch is created for each functionality and once implemented, it is merged with the main branch. GitFlow is an extension of this workflow.\n\n\n\n\n\nGitFlow is based on the Feature Branch Workflow (one branch=one functionality/feature) but it adds:\n\na branch called develop which integrates the feature branches\ndifferent release branches that gather a set of features that are considered as finished\nmain branch that contains only code coming from release branches\nhotfix branches to solve bugs coming from the main branch\n\n\n\n\nGitFlow organization\n\n\n\n\n\nThere is a GitFlow plug-in to make easier the use of this workflow:\n\ngit flow init initiates the project, create the main branch and the branch develop\ngit flow feature start namefeature creates a new branch from the branch develop.\ngit flow feature finish namefeature merges the feature branch with the develop one.\ngit flow release start 0.0.1 creates the release and git flow release finish 0.0.1 integrates the feature to the main branch.",
    "crumbs": [
      "Git",
      "Workflows"
    ]
  },
  {
    "objectID": "Courses/Git/Workflow.html#what-is-a-workflow",
    "href": "Courses/Git/Workflow.html#what-is-a-workflow",
    "title": "Workflows",
    "section": "",
    "text": "The different collaborators can have different git using habits. A workflow is a set of recommandations to follow in order to use Git efficiently. In other words, it is a set of guidelines for all the collaborators.\nA basic worflow is the Feature Branch Workflow: a branch is created for each functionality and once implemented, it is merged with the main branch. GitFlow is an extension of this workflow.",
    "crumbs": [
      "Git",
      "Workflows"
    ]
  },
  {
    "objectID": "Courses/Git/Workflow.html#gitflow",
    "href": "Courses/Git/Workflow.html#gitflow",
    "title": "Workflows",
    "section": "",
    "text": "GitFlow is based on the Feature Branch Workflow (one branch=one functionality/feature) but it adds:\n\na branch called develop which integrates the feature branches\ndifferent release branches that gather a set of features that are considered as finished\nmain branch that contains only code coming from release branches\nhotfix branches to solve bugs coming from the main branch\n\n\n\n\nGitFlow organization\n\n\n\n\n\nThere is a GitFlow plug-in to make easier the use of this workflow:\n\ngit flow init initiates the project, create the main branch and the branch develop\ngit flow feature start namefeature creates a new branch from the branch develop.\ngit flow feature finish namefeature merges the feature branch with the develop one.\ngit flow release start 0.0.1 creates the release and git flow release finish 0.0.1 integrates the feature to the main branch.",
    "crumbs": [
      "Git",
      "Workflows"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartII.html",
    "href": "Courses/Git/Collaborative_PartII.html",
    "title": "Collaborative Git (PartII)",
    "section": "",
    "text": "In this part, you will learn basically how to handle conflicts.\n\n\nLet’s imagine you have added some new functionalities to your preprocessing code (branch preprocess_test):\n$ echo \"one-hot encoding\" &gt;&gt; preprocessing_test.py\nAdd it to the index and commit it.\nWhile you have added this preprocess, your colleague has focused on a new method to optimize the model:\n$ cd ../colleague_computer\n$ git checkout -b new_opti__model\nHe/she changes the script as follows:\n$ echo \"new opti method: 90% Accuracy\" &gt;&gt; first_model.py\n$ git commit -am \"new opti method\"\n$ git push\nThis creates a PR and you accept the merge pull request.\nAfter that, you continue your work on preprocessing:\n$ cd ../mycomputer\n$ echo \"improve encoding\" &gt;&gt; preprocessing_test.py\n$ git commit -am \"improve encoding\"\n$ git push \nBut you do not have the last code of your colleague (you do not have pull the main branch) and you may think that you overwrite his/her file. This is wrong since you require Git to merge the “commits”: you have not modified/committed the file “first_model.py”. So you can create a PR and merge it. In this situation, no conflict exists. Let’s explore a conflictual situation where you and your colleague modify the same file.\n\n\n\nYou come the morning to work and retrieve the last modifications on the main branch (as well as your colleague). Create a new branch and modify “first_model.py”. During this time, your colleague modify the file “first_model.py” (a different one): add, commit, push, PR. Validate the modifications of your colleague and finish your own (add, commit, push, PR). After Pull Request, the following message appears: “this branch has conflicts that must be resolved”.\n\n\n\n\n\nClick on resolve conflicts. You will see the modifications brought by your branch and the other changes. Once you have solved the conflicts, click on mark as resolved.\n\n\n\nWhen you merge the files, Git indicates the presence of a conflict. Three possibilities exist:\n\nopen the file locally and solve the conflict.\npreserve the local modifications with git checkout --ours file\noverwrite the local modifications and preserve the remote ones with git checkout --theirs file\n\n\n\n\n\nYou have created a branch “Branch1” but during this time that main branch has been updated (let’s say the correction of a major bug). You have two possibilies:\n\non the branch “Branch1”, you merge the main branch. The drawback of this strategy is to create a complex historic.\nwith the command git rebase you overwrite the historic. Let’s illustrate this command.\n\nCreate a new branch “Branch1” and a new file “branch1_file.txt” and commit it. Return on the main branch, create a new file and commit it. Return on the branch “Branch1” and type:\n$ git rebase main\nThis indicates that the last commit of main becomes the parent of the branch “Branch1”.\nWarning: Do not rebase a public branch!",
    "crumbs": [
      "Git",
      "Collaborative Git (PartII)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartII.html#conflict-or-not",
    "href": "Courses/Git/Collaborative_PartII.html#conflict-or-not",
    "title": "Collaborative Git (PartII)",
    "section": "",
    "text": "Let’s imagine you have added some new functionalities to your preprocessing code (branch preprocess_test):\n$ echo \"one-hot encoding\" &gt;&gt; preprocessing_test.py\nAdd it to the index and commit it.\nWhile you have added this preprocess, your colleague has focused on a new method to optimize the model:\n$ cd ../colleague_computer\n$ git checkout -b new_opti__model\nHe/she changes the script as follows:\n$ echo \"new opti method: 90% Accuracy\" &gt;&gt; first_model.py\n$ git commit -am \"new opti method\"\n$ git push\nThis creates a PR and you accept the merge pull request.\nAfter that, you continue your work on preprocessing:\n$ cd ../mycomputer\n$ echo \"improve encoding\" &gt;&gt; preprocessing_test.py\n$ git commit -am \"improve encoding\"\n$ git push \nBut you do not have the last code of your colleague (you do not have pull the main branch) and you may think that you overwrite his/her file. This is wrong since you require Git to merge the “commits”: you have not modified/committed the file “first_model.py”. So you can create a PR and merge it. In this situation, no conflict exists. Let’s explore a conflictual situation where you and your colleague modify the same file.",
    "crumbs": [
      "Git",
      "Collaborative Git (PartII)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartII.html#real-conflict",
    "href": "Courses/Git/Collaborative_PartII.html#real-conflict",
    "title": "Collaborative Git (PartII)",
    "section": "",
    "text": "You come the morning to work and retrieve the last modifications on the main branch (as well as your colleague). Create a new branch and modify “first_model.py”. During this time, your colleague modify the file “first_model.py” (a different one): add, commit, push, PR. Validate the modifications of your colleague and finish your own (add, commit, push, PR). After Pull Request, the following message appears: “this branch has conflicts that must be resolved”.",
    "crumbs": [
      "Git",
      "Collaborative Git (PartII)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartII.html#handle-conflict",
    "href": "Courses/Git/Collaborative_PartII.html#handle-conflict",
    "title": "Collaborative Git (PartII)",
    "section": "",
    "text": "Click on resolve conflicts. You will see the modifications brought by your branch and the other changes. Once you have solved the conflicts, click on mark as resolved.\n\n\n\nWhen you merge the files, Git indicates the presence of a conflict. Three possibilities exist:\n\nopen the file locally and solve the conflict.\npreserve the local modifications with git checkout --ours file\noverwrite the local modifications and preserve the remote ones with git checkout --theirs file",
    "crumbs": [
      "Git",
      "Collaborative Git (PartII)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartII.html#modification-of-the-historic-with-git-rebase",
    "href": "Courses/Git/Collaborative_PartII.html#modification-of-the-historic-with-git-rebase",
    "title": "Collaborative Git (PartII)",
    "section": "",
    "text": "You have created a branch “Branch1” but during this time that main branch has been updated (let’s say the correction of a major bug). You have two possibilies:\n\non the branch “Branch1”, you merge the main branch. The drawback of this strategy is to create a complex historic.\nwith the command git rebase you overwrite the historic. Let’s illustrate this command.\n\nCreate a new branch “Branch1” and a new file “branch1_file.txt” and commit it. Return on the main branch, create a new file and commit it. Return on the branch “Branch1” and type:\n$ git rebase main\nThis indicates that the last commit of main becomes the parent of the branch “Branch1”.\nWarning: Do not rebase a public branch!",
    "crumbs": [
      "Git",
      "Collaborative Git (PartII)"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartII.html",
    "href": "Courses/Git/Basis_PartII.html",
    "title": "Basic commands: remote repo",
    "section": "",
    "text": "In this part, we will work on a real remote repository but without collaborators (next part). In this perspective, create an account on Github .\n\n\nClick on “new” or “create repository”. You need to fill in the following information:\n\nprovide a repository name and a description\nyou have to choose between public or private (choose private for now)\nchoose to add a “Readme” file (you see what it is)\nchoose to add a “gitignore”\n\n\n\n\nThe readme file is used for the documentation of your project. It is often written in markdown. You can find in this file:\n\na table of content\nthe goals of the project\ninstruction to install the project (clone is not sufficient and you need for example some python environment and packages)\nhow to use it\na documentation of the different functionalities of your code, …\n\n\n\n\nThere is nothing on your repository. You want to copy your repository on your computer, code and send your work inline. Remark: another possibility is to initiate a repository on your computer and send it to Github. But we will proceed more often like described previously.\nTo clone your repository, click on Code and then `SSH. Github tells you that you do not have a public key. Let’s create one.\n\n\nClick on the link in Code-&gt; SSH or [this one] (https://github.com/settings/ssh/new). We first generate a key using your Github email adress:\n$ ssh-keygen -t ed25519 -C \"your_email@example.com\"\nThe terminal suggests to enter the file name where you store the key. By default, it will be in the folder home, in a hidden folder “.ssh” and in the file “id_ed25519”. If you have already a ssh key, the terminal tells you so. Github asks you a passphrase: it is not mandatory.\nNow, you have generated public and private keys. You will add them to the ssh-agent: it is a program that keeps into memory your keys. You add the previous to this agent:\n$ eval \"$(ssh-agent -s)\"\n$ ssh-add ~/.ssh/id_ed25519 # le chemin vers votre clé privée générée plus haut\nTo know your ssh key, type:\n$ cat ~/.ssh/id_ed25519.pub\nCopy and paste this key to your github account: Settings -&gt; SSH and GPG keys -&gt; new SSH key.\nNow, you can finally clone your remote repository:\n$ git clone copied_adress\n\n\n\n\nWe will just repeat the same steps as in the offline part. Let’s create a new script:\n$ echo “recall: 60%” &gt;&gt; new_script.py\nAdd it to the index:\n$ git add new_script.py\nCommit the modifications:\n$ git commit -m \"message de commit\"\nPush everything on your remote repo:\n$ git push",
    "crumbs": [
      "Git",
      "Basic commands: remote repo"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartII.html#create-a-repository",
    "href": "Courses/Git/Basis_PartII.html#create-a-repository",
    "title": "Basic commands: remote repo",
    "section": "",
    "text": "Click on “new” or “create repository”. You need to fill in the following information:\n\nprovide a repository name and a description\nyou have to choose between public or private (choose private for now)\nchoose to add a “Readme” file (you see what it is)\nchoose to add a “gitignore”",
    "crumbs": [
      "Git",
      "Basic commands: remote repo"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartII.html#readme",
    "href": "Courses/Git/Basis_PartII.html#readme",
    "title": "Basic commands: remote repo",
    "section": "",
    "text": "The readme file is used for the documentation of your project. It is often written in markdown. You can find in this file:\n\na table of content\nthe goals of the project\ninstruction to install the project (clone is not sufficient and you need for example some python environment and packages)\nhow to use it\na documentation of the different functionalities of your code, …",
    "crumbs": [
      "Git",
      "Basic commands: remote repo"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartII.html#ssh-and-clone-of-your-repository-important",
    "href": "Courses/Git/Basis_PartII.html#ssh-and-clone-of-your-repository-important",
    "title": "Basic commands: remote repo",
    "section": "",
    "text": "There is nothing on your repository. You want to copy your repository on your computer, code and send your work inline. Remark: another possibility is to initiate a repository on your computer and send it to Github. But we will proceed more often like described previously.\nTo clone your repository, click on Code and then `SSH. Github tells you that you do not have a public key. Let’s create one.\n\n\nClick on the link in Code-&gt; SSH or [this one] (https://github.com/settings/ssh/new). We first generate a key using your Github email adress:\n$ ssh-keygen -t ed25519 -C \"your_email@example.com\"\nThe terminal suggests to enter the file name where you store the key. By default, it will be in the folder home, in a hidden folder “.ssh” and in the file “id_ed25519”. If you have already a ssh key, the terminal tells you so. Github asks you a passphrase: it is not mandatory.\nNow, you have generated public and private keys. You will add them to the ssh-agent: it is a program that keeps into memory your keys. You add the previous to this agent:\n$ eval \"$(ssh-agent -s)\"\n$ ssh-add ~/.ssh/id_ed25519 # le chemin vers votre clé privée générée plus haut\nTo know your ssh key, type:\n$ cat ~/.ssh/id_ed25519.pub\nCopy and paste this key to your github account: Settings -&gt; SSH and GPG keys -&gt; new SSH key.\nNow, you can finally clone your remote repository:\n$ git clone copied_adress",
    "crumbs": [
      "Git",
      "Basic commands: remote repo"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartII.html#commit-pull-push-on-the-remote-repository",
    "href": "Courses/Git/Basis_PartII.html#commit-pull-push-on-the-remote-repository",
    "title": "Basic commands: remote repo",
    "section": "",
    "text": "We will just repeat the same steps as in the offline part. Let’s create a new script:\n$ echo “recall: 60%” &gt;&gt; new_script.py\nAdd it to the index:\n$ git add new_script.py\nCommit the modifications:\n$ git commit -m \"message de commit\"\nPush everything on your remote repo:\n$ git push",
    "crumbs": [
      "Git",
      "Basic commands: remote repo"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html",
    "href": "Courses/Bash/streams_research.html",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "The role of grep is to look for a word in a text and to display the lines where it appears.\n\n\nIf you want to look for the word “alias” in the file “.bashrc”, type:\n$ grep alias .bashrc\nIf you want to ignore the difference between non-capital letters and capital letters, use the option “i”:\n$ grep -i alias .bashrc\nTo display the line numbers, use option “-n”:\n$ grep -n alias .bashrc\nIf you want to display all the lines where a word is not present, use ‘-v’:\n$ grep -v alias .bashrc\nFinally, to do a recursive research in a folder, use ‘-r’:\n$ grep -r alias folderName\n\n\n\nTo do some accurate researches, you need to use regex: it is a set of symbols that tell to the computer exactly what you look for. The following table gives the main symbols and their meaning:\n\n\n\nSymbols\nMeaning\n\n\n\n\n.\nAny character except \n\n\n^\nAt the beginning\n\n\n$\nAt the end\n\n\n[]\nOne character between the bracket\n\n\n[^]\nForbidden characters\n\n\n?\nOptional character (apply to the previoous one)\n\n\n*\nThe previous character may be present 0,1, or many times\n\n\n+\nThe previous character must be present 1 or many times\n\n\n|\nOr\n\n\n()\nGroup of expressions\n\n\n{n}\nThe previous character is present n times\n\n\n\nLet’s give some examples to illustrate this abstract table: first use option “-E” to indicate that you use regex.\n$ grep -E ^Alias .bashrc\nmeans that you look for lines that begin with ‘Alias’.\n$ grep -E [Aa]lias .bashrc\nmeans that you look for “Alias” or “alias”.\n$ grep -E [0-4] .bashrc\ngives all the lines that contain a number between 0 and 4.\nExercise (intermediary). Write the following regexs:\n\nlines that contain two or too\nlines that contain copyright or right\nlines that contain 3 vowels\n\nExercise (difficult). Write the following regexs:\n\nWrite a regexp that validates if an email adress is correct or not.\nWrite a regexp that captures phone numbers with the format (xxx) xxx-xxxx or xxx-xxx-xxxx.\n\n\n\n\n\nThe command sort sorts by alphabetical order:\n$ sort testFile\nThe result is only displayed on the terminal. To write the result on a file, use option “-o”:\n$ sort -o sortFile fileTest\nTo sort numbers use option ‘-n’.\n\n\n\nTo count the number of lines, use ‘l’:\n$ wc -l testFile\nFor the number of words, use ‘-w’:\n$ wc -w testFile\nFor the number of characters, use ‘-m’:\n$ wc -m testFile\n\n\n\nThe command cut enables to preserve only a part of each line. Let’s say that we have already a file marks.csv with a column of name, marks and appreciations. Basically, you can cut according to the number of characters. If you want to preserve only the characters from 2 to 4, type:\n$ cut -c 2-4 marks.csv\nBut if you want to extract only the names, you can not do this if all the names have not the same length. We use the fact that the file is a csv (Comma separated value) that is to say each column is separeted by a comma. The following command does exactly what you want:\n$ cut -d , -f 1 marks.csv\nLet’s detail the options:\n\n‘-d’ indicates the delimiter (here the comma)\n‘-f’ indicates the column to preservex\n\n\n\n\nFor each command we have seen earlier, the result is displayed on the terminal. But you can send the result to another output: in a file or as the input of another command (command pipeline). This is performed using special symbols like ‘&gt;’, ‘&gt;&gt;’ or ‘|’.\n\n\nThese symbols enable to write the result of a command in a file, instead of the terminal.\nLet’s first begin with &gt;. Let’s illustrate with the file “marks.csv” and we write the result of cut in another file.\n$ cut -d , -f 1 marks.csv &gt; names.txt\n&gt;&gt; redirects at the end of the file (so your file is not cleaned):\n$ cut -d , -f 1 marks.csv &gt;&gt; names.txt\nReally, each command produce two streams: the standard output (with everything except the error) and the error output. Imagine that you try to cut a file that does not exist:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt\nThe error message appears in the terminal. To redirect this message, use 2&gt;:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt 2&gt; errors.log\nIf you want to merge the two outputs, it is possible with:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt 2&gt;&1 errors.log\n\n\n\nLet’s chain up commands with the pipe | symbol. This means that the output of a previous command is the entrance of the new command:\n\n\n\nPipeline\n\n\nLet’s say that you want to sort the names of the file “marks.csv”, you can combine cut with sort and write the result in the file “sortNames.txt”:\n$ cut -d , -f 1 marks.csv | sort &gt; sortNames.txt\nIf you want to know all the folders sorted by their size and display only the most voluminuous:\n$ du | sort -nr | head\nExercise: display only the names of the file containing the word “log” in the folder “/var/log”, sort these names and eliminate the duplicates.",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html#filter-with-grep",
    "href": "Courses/Bash/streams_research.html#filter-with-grep",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "The role of grep is to look for a word in a text and to display the lines where it appears.\n\n\nIf you want to look for the word “alias” in the file “.bashrc”, type:\n$ grep alias .bashrc\nIf you want to ignore the difference between non-capital letters and capital letters, use the option “i”:\n$ grep -i alias .bashrc\nTo display the line numbers, use option “-n”:\n$ grep -n alias .bashrc\nIf you want to display all the lines where a word is not present, use ‘-v’:\n$ grep -v alias .bashrc\nFinally, to do a recursive research in a folder, use ‘-r’:\n$ grep -r alias folderName\n\n\n\nTo do some accurate researches, you need to use regex: it is a set of symbols that tell to the computer exactly what you look for. The following table gives the main symbols and their meaning:\n\n\n\nSymbols\nMeaning\n\n\n\n\n.\nAny character except \n\n\n^\nAt the beginning\n\n\n$\nAt the end\n\n\n[]\nOne character between the bracket\n\n\n[^]\nForbidden characters\n\n\n?\nOptional character (apply to the previoous one)\n\n\n*\nThe previous character may be present 0,1, or many times\n\n\n+\nThe previous character must be present 1 or many times\n\n\n|\nOr\n\n\n()\nGroup of expressions\n\n\n{n}\nThe previous character is present n times\n\n\n\nLet’s give some examples to illustrate this abstract table: first use option “-E” to indicate that you use regex.\n$ grep -E ^Alias .bashrc\nmeans that you look for lines that begin with ‘Alias’.\n$ grep -E [Aa]lias .bashrc\nmeans that you look for “Alias” or “alias”.\n$ grep -E [0-4] .bashrc\ngives all the lines that contain a number between 0 and 4.\nExercise (intermediary). Write the following regexs:\n\nlines that contain two or too\nlines that contain copyright or right\nlines that contain 3 vowels\n\nExercise (difficult). Write the following regexs:\n\nWrite a regexp that validates if an email adress is correct or not.\nWrite a regexp that captures phone numbers with the format (xxx) xxx-xxxx or xxx-xxx-xxxx.",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html#sort-the-files",
    "href": "Courses/Bash/streams_research.html#sort-the-files",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "The command sort sorts by alphabetical order:\n$ sort testFile\nThe result is only displayed on the terminal. To write the result on a file, use option “-o”:\n$ sort -o sortFile fileTest\nTo sort numbers use option ‘-n’.",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html#count-line-numbers-with-wc",
    "href": "Courses/Bash/streams_research.html#count-line-numbers-with-wc",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "To count the number of lines, use ‘l’:\n$ wc -l testFile\nFor the number of words, use ‘-w’:\n$ wc -w testFile\nFor the number of characters, use ‘-m’:\n$ wc -m testFile",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html#cut-a-part-of-a-file-cut",
    "href": "Courses/Bash/streams_research.html#cut-a-part-of-a-file-cut",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "The command cut enables to preserve only a part of each line. Let’s say that we have already a file marks.csv with a column of name, marks and appreciations. Basically, you can cut according to the number of characters. If you want to preserve only the characters from 2 to 4, type:\n$ cut -c 2-4 marks.csv\nBut if you want to extract only the names, you can not do this if all the names have not the same length. We use the fact that the file is a csv (Comma separated value) that is to say each column is separeted by a comma. The following command does exactly what you want:\n$ cut -d , -f 1 marks.csv\nLet’s detail the options:\n\n‘-d’ indicates the delimiter (here the comma)\n‘-f’ indicates the column to preservex",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/streams_research.html#streams-important",
    "href": "Courses/Bash/streams_research.html#streams-important",
    "title": "Extract, sort and filter data",
    "section": "",
    "text": "For each command we have seen earlier, the result is displayed on the terminal. But you can send the result to another output: in a file or as the input of another command (command pipeline). This is performed using special symbols like ‘&gt;’, ‘&gt;&gt;’ or ‘|’.\n\n\nThese symbols enable to write the result of a command in a file, instead of the terminal.\nLet’s first begin with &gt;. Let’s illustrate with the file “marks.csv” and we write the result of cut in another file.\n$ cut -d , -f 1 marks.csv &gt; names.txt\n&gt;&gt; redirects at the end of the file (so your file is not cleaned):\n$ cut -d , -f 1 marks.csv &gt;&gt; names.txt\nReally, each command produce two streams: the standard output (with everything except the error) and the error output. Imagine that you try to cut a file that does not exist:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt\nThe error message appears in the terminal. To redirect this message, use 2&gt;:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt 2&gt; errors.log\nIf you want to merge the two outputs, it is possible with:\n$ cut -d , -f 1 fileNotExist.csv &gt; names.txt 2&gt;&1 errors.log\n\n\n\nLet’s chain up commands with the pipe | symbol. This means that the output of a previous command is the entrance of the new command:\n\n\n\nPipeline\n\n\nLet’s say that you want to sort the names of the file “marks.csv”, you can combine cut with sort and write the result in the file “sortNames.txt”:\n$ cut -d , -f 1 marks.csv | sort &gt; sortNames.txt\nIf you want to know all the folders sorted by their size and display only the most voluminuous:\n$ du | sort -nr | head\nExercise: display only the names of the file containing the word “log” in the folder “/var/log”, sort these names and eliminate the duplicates.",
    "crumbs": [
      "Bash",
      "Extract, sort and filter data"
    ]
  },
  {
    "objectID": "Courses/Bash/first_step.html",
    "href": "Courses/Bash/first_step.html",
    "title": "Some basics on Bash",
    "section": "",
    "text": "Open a Linux terminal (or a Git Bash for Windows) and type:\n$ ls\nAs we can see, “ls” lists files and folders which are in the directory where we are when we execute the command.\nLet’s type mkdir to create a new folder (in the same directory):\n$ mkdir testfolder\nI want now to enter in this folder (change directory)\n$ cd testfolder\nWe can check the complete directory with ‘pwd’ (print working directory):\n$ pwd\nThis folder is for now empty. Let’s create a file:\n$ touch testfile\nThe file is empty. To write in this file, we can use the ‘echo’ command (we will task later about the operator &gt;):\n$ echo \"hello world\" &gt; testfile\nLet’s visualize the content of this file:\n$ cat testfile\nLet’s try to add another content:\n$ echo \"new line\" &gt; testfile\nThe previous content has been suppressed. To add a content to a file, use ‘&gt;&gt;’:\n$ echo \"new line\" &gt;&gt; testfile\n\n\n\nMost of the commands accept parameters to specify their behaviors. We will illustrate this on two commands: ‘ls’ and ‘du’ (size of file).\n\n\n\n“-a” displays all the hidden files and folders\n\n$ ls -a\n\n‘-l’ gives details about each file.\n\n$ ls -l\nThe following information are provided: 1. File permission (later subject) 2. Number of physical links 3. Owner of the file 4. Group owner 5. Size of the file (octets) 6. Last update 7. Name - ‘h’ enables to have readeable size (K,Mo,…)\n$ ls -lh\n\n‘t’ arranges the files according to the date of the last change:\n\n$ ls -lt\n\n\n\nTo return to the parent folder:\n$ cd ..\nIf you want to return to the home folder:\n$ cd ~\n\n\n\n$ du\ngives the size of all the folders in the current directory.\n$ du -h\ngives readeable size (K,Mo,…).\n$ du -a\ngives the size of files and folders since ‘du’ only displays size of folders by default.\n$ du -s\ndisplays the size of the whole folder without detailing each sub-folder.\n\n\n\n\nVim is one of the text editor available in the terminal. We will see how to use it basically. To open vim on “testFile”:\n$ vim testFile\n\n\nVim has three different modes:\n\ninterractive mode where you cannot write in your file, but you can move, do copy past, cancel out your actions.\ninsertion mode where you can write your text, code. To activate this mode, use i. To escape this mode, use Echap.\ncommand mode where you can execute some commands: register, quit, activate some options like the numbering. To activate this mode, use : from the interractive mode.\n\n\n\n\nWe sum up some commands to move efficiently in the file and register your modifications:\n\n0 and $ enable to move at the beginning or the end of the file (interractive model).\n:w nameFile to save your file (insertion mode).\n:q to quit the file (insertion mode).\n:wq to save and quit the file (insertion mode).\n x to suppress a letter. (number) x to suppress a certain number of letters.\n(number)dd to suppress/cut a certain number of lines.\ndw to suppress a word.\nd0 or d$ to suppress the beginning or the end of the line.\nyy to copy a line.\n(number)p to paste.\nu to cancel the modifications.\n(n)G to move to the line n.\n:set number to activate the numbering.",
    "crumbs": [
      "Bash",
      "Some basics on Bash"
    ]
  },
  {
    "objectID": "Courses/Bash/first_step.html#survival-kit",
    "href": "Courses/Bash/first_step.html#survival-kit",
    "title": "Some basics on Bash",
    "section": "",
    "text": "Open a Linux terminal (or a Git Bash for Windows) and type:\n$ ls\nAs we can see, “ls” lists files and folders which are in the directory where we are when we execute the command.\nLet’s type mkdir to create a new folder (in the same directory):\n$ mkdir testfolder\nI want now to enter in this folder (change directory)\n$ cd testfolder\nWe can check the complete directory with ‘pwd’ (print working directory):\n$ pwd\nThis folder is for now empty. Let’s create a file:\n$ touch testfile\nThe file is empty. To write in this file, we can use the ‘echo’ command (we will task later about the operator &gt;):\n$ echo \"hello world\" &gt; testfile\nLet’s visualize the content of this file:\n$ cat testfile\nLet’s try to add another content:\n$ echo \"new line\" &gt; testfile\nThe previous content has been suppressed. To add a content to a file, use ‘&gt;&gt;’:\n$ echo \"new line\" &gt;&gt; testfile",
    "crumbs": [
      "Bash",
      "Some basics on Bash"
    ]
  },
  {
    "objectID": "Courses/Bash/first_step.html#parameters-for-commands",
    "href": "Courses/Bash/first_step.html#parameters-for-commands",
    "title": "Some basics on Bash",
    "section": "",
    "text": "Most of the commands accept parameters to specify their behaviors. We will illustrate this on two commands: ‘ls’ and ‘du’ (size of file).\n\n\n\n“-a” displays all the hidden files and folders\n\n$ ls -a\n\n‘-l’ gives details about each file.\n\n$ ls -l\nThe following information are provided: 1. File permission (later subject) 2. Number of physical links 3. Owner of the file 4. Group owner 5. Size of the file (octets) 6. Last update 7. Name - ‘h’ enables to have readeable size (K,Mo,…)\n$ ls -lh\n\n‘t’ arranges the files according to the date of the last change:\n\n$ ls -lt\n\n\n\nTo return to the parent folder:\n$ cd ..\nIf you want to return to the home folder:\n$ cd ~\n\n\n\n$ du\ngives the size of all the folders in the current directory.\n$ du -h\ngives readeable size (K,Mo,…).\n$ du -a\ngives the size of files and folders since ‘du’ only displays size of folders by default.\n$ du -s\ndisplays the size of the whole folder without detailing each sub-folder.",
    "crumbs": [
      "Bash",
      "Some basics on Bash"
    ]
  },
  {
    "objectID": "Courses/Bash/first_step.html#introduction-to-vim",
    "href": "Courses/Bash/first_step.html#introduction-to-vim",
    "title": "Some basics on Bash",
    "section": "",
    "text": "Vim is one of the text editor available in the terminal. We will see how to use it basically. To open vim on “testFile”:\n$ vim testFile\n\n\nVim has three different modes:\n\ninterractive mode where you cannot write in your file, but you can move, do copy past, cancel out your actions.\ninsertion mode where you can write your text, code. To activate this mode, use i. To escape this mode, use Echap.\ncommand mode where you can execute some commands: register, quit, activate some options like the numbering. To activate this mode, use : from the interractive mode.\n\n\n\n\nWe sum up some commands to move efficiently in the file and register your modifications:\n\n0 and $ enable to move at the beginning or the end of the file (interractive model).\n:w nameFile to save your file (insertion mode).\n:q to quit the file (insertion mode).\n:wq to save and quit the file (insertion mode).\n x to suppress a letter. (number) x to suppress a certain number of letters.\n(number)dd to suppress/cut a certain number of lines.\ndw to suppress a word.\nd0 or d$ to suppress the beginning or the end of the line.\nyy to copy a line.\n(number)p to paste.\nu to cancel the modifications.\n(n)G to move to the line n.\n:set number to activate the numbering.",
    "crumbs": [
      "Bash",
      "Some basics on Bash"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Software Development for Data Science",
    "section": "",
    "text": "In this website, you can find ressources about:\n\nBash commands\na Git tutorial for colloborative code\nmethodology of Unit Test and Continuous Integration\nOriented Object Programmation in Python\nData Science usual libraries in Python (Numpy, Pandas, Scipy)\n\nThe grading project is also described with the evaluation criteria.\n\n\n\n Back to top"
  },
  {
    "objectID": "Courses/Bash/handled_files.html",
    "href": "Courses/Bash/handled_files.html",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "Two commands make it possible to do this:\n\ncat already seen (display the whole file). ‘-n’ for the line number.\nless display the file page by page. Here are some useful shortcuts:\n\nSpace displays the next part of the file.\nEntry displays the next line.\nb goes back to the last part.\nq stops the reading.\n\n\n\n\n\nTwo commands make it possible to do this:\n\nhead retrieves the first lines. Option ‘n’ indicates the number of lines we want to display.\ntail for the end of a file. Same option ‘n’. Option ‘f’ to follow the evolution of a file.\n\n\n\n\n\ncp copies a file. If you want to do a copy of “testfile”:\n\n$ cp testfile testcopie\nThe option ‘-R’ enables to copy a whole folder. Let’s see a powerful application using the wildcard (‘joker’) *:\n$ cp jeu* jeuFolder\nmakes it possible to copy all the files beginning by jeu to the folder “jeuFolder”.\nMini Exercise (easy): copy all the jpeg images to another folder.\n\n`` mv ``` is useful to move a file or rename it.\n\n$ mv file Folder\nMini Exercise (easy): move all the csv files to another folder.\nYou can use ‘mv’ to rename a file:\n$ mv firstName secondName\n\n\n\nWe use the command rm.\n$ rm firstFile secondFile\nThe option ‘-i’ enables to ask confirmation for each file. The option ‘-r’ can supress all a folder:\n$ rm -r folder\n\n\n\nThere are two kinds of links (shortcuts) in Linux created with ln:\n\nPhysical links\nSymbolic ones\n\nTo understand their differences, we have to present some elements on the way Linux handles files.\n\n\nOn the hard drive, each file is split into two parts:\n\nthe name\nthe permissions\nthe content\n\nThe names and the contents are not stored at the same place! Each file has an identification number called the inode: file-&gt; inode -&gt; content.\n\n\n\nA physical link makes it possible to have two names that refer to the same content (inode).\n$ touch file1\n$ ln file1 file2\n$ ls -l\nIf you change ‘file1’ or ‘file2’, you change the same content.\n\n\n\nLet’s type:\n$ ln -s file1 file2\n$ ls -l\n“file2” points towards the name of ‘file1’ and not to its content.\n\n\n\n\nIn Linux, several users can be connected at the same time. Each user has its own account and its own persimissions (rights to do something).\n\n\nAmong the users, there is the root (the super-user) with all the rights. Users can be gather into groups.\n\n\n\nUsers and groups\n\n\n\n\nThis require your password in order to execute the command with the root permission:\n$ sudo commande\n\n\n\n$ sudo su \nTo quit this mode, type exit or Ctrl+D.\n\n\n\n\nIf you want to add the user ‘yanis’:\n$ sudo adduser yanis \nIf you want to change his password lately, you can use:\n$ sudo passwd yanis  \nTo supress its account, use:\n$ sudo deluser yanis  \nTo also suppress his home, add the following option:\n$ sudo deluser --remove-home yanis\n\n\n\nTo create a new group, use:\n$ sudo addgroup family\nBut anyone is present in this group.\nTo edit an user, we use usermod. If you want to add ‘yanis’ to the group ‘family’:\n$ sudo usermod family yanis\nIf you want to suppress the group:\n$ sudo delgroup family\n\n\n\nOnly the root can modify the owner of a file. If you want to modify the owner of a file (make yanis the owner of ‘filtest’):\n$ sudo chown yanis testFile\nYou can use chgrp to do the same thing with a group:\n$ sudo chgrp family testFile\nThe same thing could be done with chown by separating the new user and group by a ::\n$ sudo chown yanis:family testFile\nOne can use the option ‘-R’ to do this recursively:\n$ sudo chown -R yanis:family Folder\n\n\n\n\n\nWhen you type:\n$ ls -l\nThe first column indicates five different permissions:\n\nd indicates if the element is a folder\nl indicates if the element is a link\nr indicates if you can read the element\nw indicates if you can write/modify the element\nx for a file indicates that you can run it. For a folder, you can see the different sub-folders.\n\n\n\n\nPermissions/Rights\n\n\n\n\n\nHere we will use chmod (sudo is not necessary: we just need to be the owner of the file). We can give permissions with two methods: using numbers or letters.\n\n\n\nchmod numbers\n\n\nTo combine the permissions, we have to add up these numbers. If you want to give reading and writing rights, this corresponds to \\(4+2=6\\):\n$ chmod 640 fileTest\nThe previous command gives:\n\nreading and writing rights for the owner\nreading right for the group\nnothing for the others.\n\nAnother way exists with letter. We need to know that:\n\nu=user\ng=grou\no=other\n‘+’ means adding the permission\n‘-’ means removing the permission\n‘=’ to affect permissions\n\nIf you want to affect all the rights to the user and the reading permission to the group:\n$ chmod u=rwx, g+r fileTest\n\n\n\n\n\nIn this part, we explain how to install a sofware on Debian (this depends on the Linux distributions). In Linux, we have packages: a zip folder that contains all the files of the program: it is a file .deb which contains all the instructions to install a program. We have to focus on two specific points of Linux:\n\nadministration of the dependences: very often, a sofware depends on others softwares called the dependencies.\nAll the packages are gathered at the same place, called the repository.\n\nTo download the last list of packages, use:\n$ sudo apt-get update\nTo dowload a package, use (install openarena for instance):\n$ sudo apt-get install nomPackage \nTo supress a package and all its useless dependencies, use:\n$ sudo apt-get autoremove nomPackage\nTo update all the packages:\n$ sudo apt-get upgrade \n\n\n\nIn this short part, we explain how to use the manual to have all the ways to use a command.\nIf you want to know everything on a command (say mkdir), use:\n$ sudo man mkdir\nTo move on this page:\n\nEspace to go to the next page.\n/ to research a word in this documentation\nQ to quit the page.\n\nEach man usage displays the following sections:\n\nNAME and a short description\nSYNOPSIS lists all the ways to use the command.\nDESCRIPTION is a deeper description of the commands with all the options.\nAUTHOR of the program.\nREPORTING BUGS the people to contact for a bug.\nCOPYRIGHT user license.\nSEE ALSO others related commands.\n\nman is useful if you know the name of the command. Otherwhise, use apropos. For instance, if you look for a command related to the sound of your machine (gives all the command which talks about sound in their description):\n$ apropos sound\n\n\n\nThere is a lot of commands in this direction. The most popular and powerful command is find. It goes all over your hard drive. The command can use three kinds of arguments:\n\nwhere: it is the folder where we look for.\nwhat: it is the file we look for (by name, size, date,…).\naction: automatic actions on the selected files.\n\n\n\nFind a file by name in the current directory:\n$ find -name \"research.png\"\nIf you want to find the file called ‘research.png’ in the directory ‘folder’:\n$ find folder -name \"research.png\"\nResearch with the size:\n$ find ~ -size +10M\nResearch with the date of the last access (less than 7 days):\n$ find ~ -name \".odt\" -atime 6 \n\n\n\nYou can add an action to the result of the search. For instance, if you want to delete the results:\n$ find ~ -name \".odt\" -delete \nWith exec you can execute a command for all the files giving by find. Let’s say that you want to change the permissions of all the ‘.odt’ files:\n$ find ~ -name \".odt\" -exec chmod 640 {} \\; \nHow does it work? For each file found, the command after exec is executed. The curly brackets ‘{}’ will be replaced by the name of this file. The command must end with ‘;’.\nExercise: gather all your .jpeg files into a folder ‘images’.",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#display-content-file",
    "href": "Courses/Bash/handled_files.html#display-content-file",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "Two commands make it possible to do this:\n\ncat already seen (display the whole file). ‘-n’ for the line number.\nless display the file page by page. Here are some useful shortcuts:\n\nSpace displays the next part of the file.\nEntry displays the next line.\nb goes back to the last part.\nq stops the reading.",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#display-the-beginning-and-the-end-of-a-file",
    "href": "Courses/Bash/handled_files.html#display-the-beginning-and-the-end-of-a-file",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "Two commands make it possible to do this:\n\nhead retrieves the first lines. Option ‘n’ indicates the number of lines we want to display.\ntail for the end of a file. Same option ‘n’. Option ‘f’ to follow the evolution of a file.",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#copy-and-displacement",
    "href": "Courses/Bash/handled_files.html#copy-and-displacement",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "cp copies a file. If you want to do a copy of “testfile”:\n\n$ cp testfile testcopie\nThe option ‘-R’ enables to copy a whole folder. Let’s see a powerful application using the wildcard (‘joker’) *:\n$ cp jeu* jeuFolder\nmakes it possible to copy all the files beginning by jeu to the folder “jeuFolder”.\nMini Exercise (easy): copy all the jpeg images to another folder.\n\n`` mv ``` is useful to move a file or rename it.\n\n$ mv file Folder\nMini Exercise (easy): move all the csv files to another folder.\nYou can use ‘mv’ to rename a file:\n$ mv firstName secondName",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#supress-files",
    "href": "Courses/Bash/handled_files.html#supress-files",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "We use the command rm.\n$ rm firstFile secondFile\nThe option ‘-i’ enables to ask confirmation for each file. The option ‘-r’ can supress all a folder:\n$ rm -r folder",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#create-links-between-files",
    "href": "Courses/Bash/handled_files.html#create-links-between-files",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "There are two kinds of links (shortcuts) in Linux created with ln:\n\nPhysical links\nSymbolic ones\n\nTo understand their differences, we have to present some elements on the way Linux handles files.\n\n\nOn the hard drive, each file is split into two parts:\n\nthe name\nthe permissions\nthe content\n\nThe names and the contents are not stored at the same place! Each file has an identification number called the inode: file-&gt; inode -&gt; content.\n\n\n\nA physical link makes it possible to have two names that refer to the same content (inode).\n$ touch file1\n$ ln file1 file2\n$ ls -l\nIf you change ‘file1’ or ‘file2’, you change the same content.\n\n\n\nLet’s type:\n$ ln -s file1 file2\n$ ls -l\n“file2” points towards the name of ‘file1’ and not to its content.",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#the-users-and-their-permissions",
    "href": "Courses/Bash/handled_files.html#the-users-and-their-permissions",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "In Linux, several users can be connected at the same time. Each user has its own account and its own persimissions (rights to do something).\n\n\nAmong the users, there is the root (the super-user) with all the rights. Users can be gather into groups.\n\n\n\nUsers and groups\n\n\n\n\nThis require your password in order to execute the command with the root permission:\n$ sudo commande\n\n\n\n$ sudo su \nTo quit this mode, type exit or Ctrl+D.\n\n\n\n\nIf you want to add the user ‘yanis’:\n$ sudo adduser yanis \nIf you want to change his password lately, you can use:\n$ sudo passwd yanis  \nTo supress its account, use:\n$ sudo deluser yanis  \nTo also suppress his home, add the following option:\n$ sudo deluser --remove-home yanis\n\n\n\nTo create a new group, use:\n$ sudo addgroup family\nBut anyone is present in this group.\nTo edit an user, we use usermod. If you want to add ‘yanis’ to the group ‘family’:\n$ sudo usermod family yanis\nIf you want to suppress the group:\n$ sudo delgroup family\n\n\n\nOnly the root can modify the owner of a file. If you want to modify the owner of a file (make yanis the owner of ‘filtest’):\n$ sudo chown yanis testFile\nYou can use chgrp to do the same thing with a group:\n$ sudo chgrp family testFile\nThe same thing could be done with chown by separating the new user and group by a ::\n$ sudo chown yanis:family testFile\nOne can use the option ‘-R’ to do this recursively:\n$ sudo chown -R yanis:family Folder\n\n\n\n\n\nWhen you type:\n$ ls -l\nThe first column indicates five different permissions:\n\nd indicates if the element is a folder\nl indicates if the element is a link\nr indicates if you can read the element\nw indicates if you can write/modify the element\nx for a file indicates that you can run it. For a folder, you can see the different sub-folders.\n\n\n\n\nPermissions/Rights\n\n\n\n\n\nHere we will use chmod (sudo is not necessary: we just need to be the owner of the file). We can give permissions with two methods: using numbers or letters.\n\n\n\nchmod numbers\n\n\nTo combine the permissions, we have to add up these numbers. If you want to give reading and writing rights, this corresponds to \\(4+2=6\\):\n$ chmod 640 fileTest\nThe previous command gives:\n\nreading and writing rights for the owner\nreading right for the group\nnothing for the others.\n\nAnother way exists with letter. We need to know that:\n\nu=user\ng=grou\no=other\n‘+’ means adding the permission\n‘-’ means removing the permission\n‘=’ to affect permissions\n\nIf you want to affect all the rights to the user and the reading permission to the group:\n$ chmod u=rwx, g+r fileTest",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#software-installation",
    "href": "Courses/Bash/handled_files.html#software-installation",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "In this part, we explain how to install a sofware on Debian (this depends on the Linux distributions). In Linux, we have packages: a zip folder that contains all the files of the program: it is a file .deb which contains all the instructions to install a program. We have to focus on two specific points of Linux:\n\nadministration of the dependences: very often, a sofware depends on others softwares called the dependencies.\nAll the packages are gathered at the same place, called the repository.\n\nTo download the last list of packages, use:\n$ sudo apt-get update\nTo dowload a package, use (install openarena for instance):\n$ sudo apt-get install nomPackage \nTo supress a package and all its useless dependencies, use:\n$ sudo apt-get autoremove nomPackage\nTo update all the packages:\n$ sudo apt-get upgrade",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#commands-documentation-important-for-your-autonomy",
    "href": "Courses/Bash/handled_files.html#commands-documentation-important-for-your-autonomy",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "In this short part, we explain how to use the manual to have all the ways to use a command.\nIf you want to know everything on a command (say mkdir), use:\n$ sudo man mkdir\nTo move on this page:\n\nEspace to go to the next page.\n/ to research a word in this documentation\nQ to quit the page.\n\nEach man usage displays the following sections:\n\nNAME and a short description\nSYNOPSIS lists all the ways to use the command.\nDESCRIPTION is a deeper description of the commands with all the options.\nAUTHOR of the program.\nREPORTING BUGS the people to contact for a bug.\nCOPYRIGHT user license.\nSEE ALSO others related commands.\n\nman is useful if you know the name of the command. Otherwhise, use apropos. For instance, if you look for a command related to the sound of your machine (gives all the command which talks about sound in their description):\n$ apropos sound",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Bash/handled_files.html#looking-for-files",
    "href": "Courses/Bash/handled_files.html#looking-for-files",
    "title": "Manipulate files and permissions",
    "section": "",
    "text": "There is a lot of commands in this direction. The most popular and powerful command is find. It goes all over your hard drive. The command can use three kinds of arguments:\n\nwhere: it is the folder where we look for.\nwhat: it is the file we look for (by name, size, date,…).\naction: automatic actions on the selected files.\n\n\n\nFind a file by name in the current directory:\n$ find -name \"research.png\"\nIf you want to find the file called ‘research.png’ in the directory ‘folder’:\n$ find folder -name \"research.png\"\nResearch with the size:\n$ find ~ -size +10M\nResearch with the date of the last access (less than 7 days):\n$ find ~ -name \".odt\" -atime 6 \n\n\n\nYou can add an action to the result of the search. For instance, if you want to delete the results:\n$ find ~ -name \".odt\" -delete \nWith exec you can execute a command for all the files giving by find. Let’s say that you want to change the permissions of all the ‘.odt’ files:\n$ find ~ -name \".odt\" -exec chmod 640 {} \\; \nHow does it work? For each file found, the command after exec is executed. The curly brackets ‘{}’ will be replaced by the name of this file. The command must end with ‘;’.\nExercise: gather all your .jpeg files into a folder ‘images’.",
    "crumbs": [
      "Bash",
      "Manipulate files and permissions"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html",
    "href": "Courses/Git/Basis_PartI.html",
    "title": "Basic commands: locally",
    "section": "",
    "text": "A repository is a place where you store the code.\n$ git init --bare ~/reposTest/firstProject.git\nWe initialized this repository as we have created a repository on Internet. Later, we will create the repository in Github. But for now, imagine that it is the case.\n\n\n\nThe repository is for now empty. Let’s clone an empty project. It is useless but this enables to learn the syntax to do it: in practice, we clone from an Internet adress. The syntax is:\n$ git clone repository_adress(remote) localFolder\nIn our case:\n$ git clone reposTest/firstProject.git firstProject\nLet’s enter in this folder. Let’s type git status and to check if the repertory has been cloned. If you type ls -a you can see a folder “.git”. It’s thanks to it that your folder is a repository and that git follows the modifications.\n\n\n\nLet’s first create a file in this folder:\n$ echo \"Accuracy: 60%\" &gt;&gt; basic_model.py\nWith git status, we see that git has detected the new file.\nNow, we want to follow the future modifications of this file (future snapshots=commits).\n$ git add basic_model.py\nYou can check that the file is followed by git using git status. The command means that you add the file to the git index located in the folder “.git”.\n\n\n\nIn this part, imagine that we have data file (csv) and a script to process it.\n$ mkdir data\n$ echo data &gt;&gt; data/housing.csv\n$ touch \"pre_process.py\"\nYou indicate that you track all the files:\n$ git add .\nBut you do not want to track the csv files (you usually do not follow data files), what can you do to suppress the csv file from the index:\n$ git rm --cached data/housing.csv\nIn fact, you will never track the csv file, so how to tell to git to ignore the folder data forever. To do this you need to create a file .gitignore and write all the files that you do not want to track in this file.\n$ echo \"data/\" &gt;&gt; .gitignore\nYou will want to add this file to your remote repository. But you do not do this immediately since it is preferable to do a commit for the code and another for gitignore.\n\n\n\nIn this part, you will do your first commit. You can see a commit as a snapshot of your code. Let’s type git commit with the “-m” option to add a message:\n$ git commit -m \"first ML model\"\nExercise (easy): Add “.gitignore” to the index and commit with an adapted message.\nThe commits are useful to:\n\nknow the previous states of the code\nget back to a previous state if one functionality does not work as expected. If your commit message is explicit, it is easier to find a previous commit.\n\n\n\n\nTo display the history of your commit, use:\n$ git log \nThe list of commits appears from the newest to oldest. To each commit a hash code (an ID) is associated to precisely identify it.\n\n\n\nIn this part, you have initialized a repository (imagine that it is in Internet) and clone it on your folder. Let’s create another folder to pretend to have another computer and clone the faking remote repository:\n$ mkdir ../another_computer\n$ cd ../another_computer\n$ git clone ../../reposTest/firstProject.git firstProject\nNow, let’s send the last modifications to the faking remote repository with git push (pousser):\n$ cd ../firstProject\n$ git push \nReturn now to your faking computer to recover the last modifications with git pull (tirer):\n$ cd ../another_computer/firstProject\n$ git pull\nIn the next part, we will work with a real remote repository on Github but without collaborators.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#initialization",
    "href": "Courses/Git/Basis_PartI.html#initialization",
    "title": "Basic commands: locally",
    "section": "",
    "text": "A repository is a place where you store the code.\n$ git init --bare ~/reposTest/firstProject.git\nWe initialized this repository as we have created a repository on Internet. Later, we will create the repository in Github. But for now, imagine that it is the case.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#clone-a-project",
    "href": "Courses/Git/Basis_PartI.html#clone-a-project",
    "title": "Basic commands: locally",
    "section": "",
    "text": "The repository is for now empty. Let’s clone an empty project. It is useless but this enables to learn the syntax to do it: in practice, we clone from an Internet adress. The syntax is:\n$ git clone repository_adress(remote) localFolder\nIn our case:\n$ git clone reposTest/firstProject.git firstProject\nLet’s enter in this folder. Let’s type git status and to check if the repertory has been cloned. If you type ls -a you can see a folder “.git”. It’s thanks to it that your folder is a repository and that git follows the modifications.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#how-to-follow-the-different-versions-of-my-files",
    "href": "Courses/Git/Basis_PartI.html#how-to-follow-the-different-versions-of-my-files",
    "title": "Basic commands: locally",
    "section": "",
    "text": "Let’s first create a file in this folder:\n$ echo \"Accuracy: 60%\" &gt;&gt; basic_model.py\nWith git status, we see that git has detected the new file.\nNow, we want to follow the future modifications of this file (future snapshots=commits).\n$ git add basic_model.py\nYou can check that the file is followed by git using git status. The command means that you add the file to the git index located in the folder “.git”.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#how-to-untrack-your-file",
    "href": "Courses/Git/Basis_PartI.html#how-to-untrack-your-file",
    "title": "Basic commands: locally",
    "section": "",
    "text": "In this part, imagine that we have data file (csv) and a script to process it.\n$ mkdir data\n$ echo data &gt;&gt; data/housing.csv\n$ touch \"pre_process.py\"\nYou indicate that you track all the files:\n$ git add .\nBut you do not want to track the csv files (you usually do not follow data files), what can you do to suppress the csv file from the index:\n$ git rm --cached data/housing.csv\nIn fact, you will never track the csv file, so how to tell to git to ignore the folder data forever. To do this you need to create a file .gitignore and write all the files that you do not want to track in this file.\n$ echo \"data/\" &gt;&gt; .gitignore\nYou will want to add this file to your remote repository. But you do not do this immediately since it is preferable to do a commit for the code and another for gitignore.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#first-commit",
    "href": "Courses/Git/Basis_PartI.html#first-commit",
    "title": "Basic commands: locally",
    "section": "",
    "text": "In this part, you will do your first commit. You can see a commit as a snapshot of your code. Let’s type git commit with the “-m” option to add a message:\n$ git commit -m \"first ML model\"\nExercise (easy): Add “.gitignore” to the index and commit with an adapted message.\nThe commits are useful to:\n\nknow the previous states of the code\nget back to a previous state if one functionality does not work as expected. If your commit message is explicit, it is easier to find a previous commit.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#display-the-recent-history-of-your-repo",
    "href": "Courses/Git/Basis_PartI.html#display-the-recent-history-of-your-repo",
    "title": "Basic commands: locally",
    "section": "",
    "text": "To display the history of your commit, use:\n$ git log \nThe list of commits appears from the newest to oldest. To each commit a hash code (an ID) is associated to precisely identify it.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Basis_PartI.html#send-your-code-on-a-remote-repository-and-recover-it",
    "href": "Courses/Git/Basis_PartI.html#send-your-code-on-a-remote-repository-and-recover-it",
    "title": "Basic commands: locally",
    "section": "",
    "text": "In this part, you have initialized a repository (imagine that it is in Internet) and clone it on your folder. Let’s create another folder to pretend to have another computer and clone the faking remote repository:\n$ mkdir ../another_computer\n$ cd ../another_computer\n$ git clone ../../reposTest/firstProject.git firstProject\nNow, let’s send the last modifications to the faking remote repository with git push (pousser):\n$ cd ../firstProject\n$ git push \nReturn now to your faking computer to recover the last modifications with git pull (tirer):\n$ cd ../another_computer/firstProject\n$ git pull\nIn the next part, we will work with a real remote repository on Github but without collaborators.",
    "crumbs": [
      "Git",
      "Basic commands: locally"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartI.html",
    "href": "Courses/Git/Collaborative_PartI.html",
    "title": "Collaborative Git (PartI)",
    "section": "",
    "text": "In this part, we will imagine that you work with a colleague.\n\n\nLet’s copy the repository in two folders:\n$ git clone git_adress mycomputer\n$ git clone git_adress colleague_computer\nLet’s create a new file in the folder “mycomputer”:\n$ echo \"first_model: 60% Accuracy\" &gt;&gt; first_model.py\nThen, add this file to the index, commit the modification and send them to the repo. Next step: return to the colleague’s computer and retrieve the new file. Your colleague also modify “first_model”,commit and send it to the repo:\n$ echo \"improvement model: 80% Accuracy\" &gt;&gt; first_model.py\nLet us indicate a small trick. It is possible to combine add and commit (if the file has been already added to the index):\n$ git commit -am message\n\n\n\nLet’s imagine that we want to experiment some pre-processing, a new model, to debug some specific parts. If you do that directly on the branch “main” and send it to the repo, it will suppress what your colleague have done before (its new work). Therefore, we create a (parallel) branch:\n$ git branch  preprocess_test\nTo move on this branch:\n$ git checkout preprocess_test\nIt is possible to do the two things at the same time:\n$ git checkout -b preprocess_test\nLet’s add a new file to this branch:\n$ echo \"label encoder\" &gt;&gt; preprocessing_test.py\nLet’s add it to the index and commit:\n$ git add preprocessing_test.py\n$ git commit -m \"experimental step of preprocessing\"\nYou can see that the file appears on the branch “preprocessing_test” but not on main. This is due to the fact, you have commited the new file on the “preprocessing_test” branch. Then try to push the modification: it does not work. Let’s explain why. Briefly, the “main” branch has the “origin/main” has a remote associated branch but not the branch “preprocess_test”. So you have to write:\n$ git push --set-upstream origin preprocess_test\nYou can configure the repo to associate a similar remote branch to each branch by typing:\ngit config --add --bool push.autoSetupRemote true\n\n\n\nLet’s say you are satisfied from your work and you want to integrate your new functionality. But you need a feedback/checking of one of your colleague. This is the object of a pull request (PR).\nOn Github, click on Pull Request -&gt; New pull request. After comparing the new branch the main one, create pull request. On this page, you find:\n\na box to enter a comment on your PR: say why you have written this code.\na list of commits\nthe number of files that have been added, modified or deleted.\n\nAt this stage, one of your colleague has to checked your work and merge it, that is to say, integrate the last changes to the main branch. How to do this:\n\nonline by clicking on “Merge pull request”\nlocally once main is updated:\n\ngit merge preprocess_test\nIn practice, git merge creates a commit whose the parents are the last commit of “main” and “preprocess_test”.\nIf you return to the main page on Github, you can see that:\n\nthe last commit is the merge one\nthe file “preprocessing_test.py” is now on the main branch.\n\nFinally, you can retrieve the last changes as well as your colleague with git pull.\nRessource: Tutorial on branch\n\n\n\nImagine you want to integrate a new file of data to your project or a model that takes different kind of inputs. This may impact the exploration of your data, the clearning, the development of the statistical algorithms, etc.\nOne may do all these changes and commit, push the code. But your poor colleague will receive a lot of lines of code and try to understand and check them. The modifications that he/she will see do not appear clearly in a logical order:\n\n\n\nBad Pull Request\n\n\nInstead of this, prefer to commit each functionality. The central question is: does each of your commit have one special purpose?\nUntil now, we have presented a simple scenario where you are the only collaborator to do modifications. Let’s present the real scenario where one collaborator has also modified the code, in the next part.",
    "crumbs": [
      "Git",
      "Collaborative Git (PartI)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartI.html#share-code-with-my-colleague-small-exercise",
    "href": "Courses/Git/Collaborative_PartI.html#share-code-with-my-colleague-small-exercise",
    "title": "Collaborative Git (PartI)",
    "section": "",
    "text": "Let’s copy the repository in two folders:\n$ git clone git_adress mycomputer\n$ git clone git_adress colleague_computer\nLet’s create a new file in the folder “mycomputer”:\n$ echo \"first_model: 60% Accuracy\" &gt;&gt; first_model.py\nThen, add this file to the index, commit the modification and send them to the repo. Next step: return to the colleague’s computer and retrieve the new file. Your colleague also modify “first_model”,commit and send it to the repo:\n$ echo \"improvement model: 80% Accuracy\" &gt;&gt; first_model.py\nLet us indicate a small trick. It is possible to combine add and commit (if the file has been already added to the index):\n$ git commit -am message",
    "crumbs": [
      "Git",
      "Collaborative Git (PartI)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartI.html#create-branch-for-experiment-or-debugging",
    "href": "Courses/Git/Collaborative_PartI.html#create-branch-for-experiment-or-debugging",
    "title": "Collaborative Git (PartI)",
    "section": "",
    "text": "Let’s imagine that we want to experiment some pre-processing, a new model, to debug some specific parts. If you do that directly on the branch “main” and send it to the repo, it will suppress what your colleague have done before (its new work). Therefore, we create a (parallel) branch:\n$ git branch  preprocess_test\nTo move on this branch:\n$ git checkout preprocess_test\nIt is possible to do the two things at the same time:\n$ git checkout -b preprocess_test\nLet’s add a new file to this branch:\n$ echo \"label encoder\" &gt;&gt; preprocessing_test.py\nLet’s add it to the index and commit:\n$ git add preprocessing_test.py\n$ git commit -m \"experimental step of preprocessing\"\nYou can see that the file appears on the branch “preprocessing_test” but not on main. This is due to the fact, you have commited the new file on the “preprocessing_test” branch. Then try to push the modification: it does not work. Let’s explain why. Briefly, the “main” branch has the “origin/main” has a remote associated branch but not the branch “preprocess_test”. So you have to write:\n$ git push --set-upstream origin preprocess_test\nYou can configure the repo to associate a similar remote branch to each branch by typing:\ngit config --add --bool push.autoSetupRemote true",
    "crumbs": [
      "Git",
      "Collaborative Git (PartI)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartI.html#pull-request",
    "href": "Courses/Git/Collaborative_PartI.html#pull-request",
    "title": "Collaborative Git (PartI)",
    "section": "",
    "text": "Let’s say you are satisfied from your work and you want to integrate your new functionality. But you need a feedback/checking of one of your colleague. This is the object of a pull request (PR).\nOn Github, click on Pull Request -&gt; New pull request. After comparing the new branch the main one, create pull request. On this page, you find:\n\na box to enter a comment on your PR: say why you have written this code.\na list of commits\nthe number of files that have been added, modified or deleted.\n\nAt this stage, one of your colleague has to checked your work and merge it, that is to say, integrate the last changes to the main branch. How to do this:\n\nonline by clicking on “Merge pull request”\nlocally once main is updated:\n\ngit merge preprocess_test\nIn practice, git merge creates a commit whose the parents are the last commit of “main” and “preprocess_test”.\nIf you return to the main page on Github, you can see that:\n\nthe last commit is the merge one\nthe file “preprocessing_test.py” is now on the main branch.\n\nFinally, you can retrieve the last changes as well as your colleague with git pull.\nRessource: Tutorial on branch",
    "crumbs": [
      "Git",
      "Collaborative Git (PartI)"
    ]
  },
  {
    "objectID": "Courses/Git/Collaborative_PartI.html#philosophy-of-commit-commit-early-commit-often",
    "href": "Courses/Git/Collaborative_PartI.html#philosophy-of-commit-commit-early-commit-often",
    "title": "Collaborative Git (PartI)",
    "section": "",
    "text": "Imagine you want to integrate a new file of data to your project or a model that takes different kind of inputs. This may impact the exploration of your data, the clearning, the development of the statistical algorithms, etc.\nOne may do all these changes and commit, push the code. But your poor colleague will receive a lot of lines of code and try to understand and check them. The modifications that he/she will see do not appear clearly in a logical order:\n\n\n\nBad Pull Request\n\n\nInstead of this, prefer to commit each functionality. The central question is: does each of your commit have one special purpose?\nUntil now, we have presented a simple scenario where you are the only collaborator to do modifications. Let’s present the real scenario where one collaborator has also modified the code, in the next part.",
    "crumbs": [
      "Git",
      "Collaborative Git (PartI)"
    ]
  },
  {
    "objectID": "Courses/Git/Errors.html",
    "href": "Courses/Git/Errors.html",
    "title": "Handle errors in Git",
    "section": "",
    "text": "In this part, we will learn how to correct some small errors in Git: create a branch that you do not want, modify the main branch, forget files in the commit, etc.\n\n\nLet’s create a folder “Test” and initiate a repo.\n\n\nCreate a file and commit it. If you main branch is named “master”, type:\n$ git branch -M main\nThen create a new branch “branchTest”. But you did not want to create this branch now but after adding some files. To delete it:\n$ git branch -d branchTest\nIf you have previously done modifications on this branch, use:\n$ git branch -D branchTest\n\n\n\nIt is possible to modify a main branch inadvertently. There are two scenarios.\n\n\nIn this case, you have modified the main before creating your branch and you have not committed anything. You need to create a “stash”: you put your modifications aside and then apply them to your new branch.\nModify a file on the main branch. Create a stash (kind of buffer):\n$ git stash\nIf you type git status, you see that your main branch is clean. Create a new branch “branchBeforeCommit”. Then apply the stash:\n$ git stash apply\nIf you have created many stashes, you can access their ID via:\n$ git stash list\nThen you can use git stash apply followed by the ID.\n\n\n\nThe case where you commit the modifications is more complex. Let’s modify the files and commit. With git log, you can access the ID/hash of your previous commits. Let’s suppress the last commit with:\n$ git reset --hard HEAD^\nCreate your new branch and use git reset --hard followed by the hash to apply the commit to your new branch.\n\n\n\n\nThe processing is very simple, just type:\n$ git commit --amend -m \"new commit message\"\n\n\n\nIn the case where you forget to add a file to the last commit, use again git commit --amend --no-edit after git add.\n\n\n\n\nIn this situation, you have pushed wrong files. We will present two important commands: git reset (already glimpsed) and git revert.\n\n\nLet’s imagine a client requires some functionaly but changes his mind the next day. You will go backward with git reset: this command has a impact on the historic. It can be used with three parameters: --hard, --medium and --soft. It is a command often use locally.\nLet’s begin with --hard:\n$ git reset --hard target_commit\nThis command enables to go backward to any target commit but you absolutely forget everything that occurs after!.\nThe option --medium enables to go backward just after the last commit or a specific one, without suppressing the last modifications. It is very useful to desindex a file that has not been committed.\n$ git reset --mixed HEAD~\nThe last option --soft makes it possible to move on a specific commit to see a snapshot or to create a new branch from a previous commit: it does not suppress files or commits.\n\n\n\nLet’s imagine you have added the wrong file to your commit. Instead of suppressing the commit from the historic, git revert creates a new commit with the last content: you do not loose the historic of your project. In the our scenario, you do your commit, you “cancel” it with:\n$ git revert HEAD\nThen you can remove the wrong file and recommit.\n\n\n\n\n\n\nThe goal of Git is to record the changes applied to your code:\n\nwho has contributed ?\nWhere bugs have been introduced ?\nCancel problematic changes.\n\nWe have seen that git log lists all the commits from the most recent to the oldest with their hash code. In order to have more details of your historic, use git reflog lists the commits but all the other actions: message modifications, merge, reset, etc. To move on a previous action, use git checkout hash.\n\n\n\nIf you discover a bug in your project, you want to identify its origin and to track every line of code. git blame file displays for each line:\n\nits ID\nits author\nthe date of the modification\nthe line number\nthe content\n\n\n\n\nSometimes, you do not want to merge a whole branch into another and you only need some specific commits. Imagine you work on “Branch1” and you do many commits but your colleague does not need all the modifications but only some. Type:\n$ git cherry-pick hashs\nto duplicate the commits and add them to your main branch.\n\n\n\n\nLet’s imagine you develop a new functionality and during this time you note a bug and need to correct it. The method is the following:\n\nkeep into memory what you currently make using git stash\nfind the bug using git log and git bisect\nrestore the problematic file with git revert hash\nrestore what you have done using git stash apply.\n\nThe new command here is git bisect with different options:\n\nstart to begin the research of the bug\nbad HEAD to indicate that the bug is present in the last commit\ngood hash to indicate the most recent commit that does not have the bug From now, the command launches a dichomotic algorithm. For each commit where the command moves on indicate git bisect good if the bug is not present and git bisect bad if not. At the end of the process, you have finally found the commit where the bug appears for the first time, then use git revert hash.",
    "crumbs": [
      "Git",
      "Handle errors in Git"
    ]
  },
  {
    "objectID": "Courses/Git/Errors.html#errors-in-local-repository",
    "href": "Courses/Git/Errors.html#errors-in-local-repository",
    "title": "Handle errors in Git",
    "section": "",
    "text": "Let’s create a folder “Test” and initiate a repo.\n\n\nCreate a file and commit it. If you main branch is named “master”, type:\n$ git branch -M main\nThen create a new branch “branchTest”. But you did not want to create this branch now but after adding some files. To delete it:\n$ git branch -d branchTest\nIf you have previously done modifications on this branch, use:\n$ git branch -D branchTest\n\n\n\nIt is possible to modify a main branch inadvertently. There are two scenarios.\n\n\nIn this case, you have modified the main before creating your branch and you have not committed anything. You need to create a “stash”: you put your modifications aside and then apply them to your new branch.\nModify a file on the main branch. Create a stash (kind of buffer):\n$ git stash\nIf you type git status, you see that your main branch is clean. Create a new branch “branchBeforeCommit”. Then apply the stash:\n$ git stash apply\nIf you have created many stashes, you can access their ID via:\n$ git stash list\nThen you can use git stash apply followed by the ID.\n\n\n\nThe case where you commit the modifications is more complex. Let’s modify the files and commit. With git log, you can access the ID/hash of your previous commits. Let’s suppress the last commit with:\n$ git reset --hard HEAD^\nCreate your new branch and use git reset --hard followed by the hash to apply the commit to your new branch.\n\n\n\n\nThe processing is very simple, just type:\n$ git commit --amend -m \"new commit message\"\n\n\n\nIn the case where you forget to add a file to the last commit, use again git commit --amend --no-edit after git add.",
    "crumbs": [
      "Git",
      "Handle errors in Git"
    ]
  },
  {
    "objectID": "Courses/Git/Errors.html#errors-in-the-remote-repository",
    "href": "Courses/Git/Errors.html#errors-in-the-remote-repository",
    "title": "Handle errors in Git",
    "section": "",
    "text": "In this situation, you have pushed wrong files. We will present two important commands: git reset (already glimpsed) and git revert.\n\n\nLet’s imagine a client requires some functionaly but changes his mind the next day. You will go backward with git reset: this command has a impact on the historic. It can be used with three parameters: --hard, --medium and --soft. It is a command often use locally.\nLet’s begin with --hard:\n$ git reset --hard target_commit\nThis command enables to go backward to any target commit but you absolutely forget everything that occurs after!.\nThe option --medium enables to go backward just after the last commit or a specific one, without suppressing the last modifications. It is very useful to desindex a file that has not been committed.\n$ git reset --mixed HEAD~\nThe last option --soft makes it possible to move on a specific commit to see a snapshot or to create a new branch from a previous commit: it does not suppress files or commits.\n\n\n\nLet’s imagine you have added the wrong file to your commit. Instead of suppressing the commit from the historic, git revert creates a new commit with the last content: you do not loose the historic of your project. In the our scenario, you do your commit, you “cancel” it with:\n$ git revert HEAD\nThen you can remove the wrong file and recommit.",
    "crumbs": [
      "Git",
      "Handle errors in Git"
    ]
  },
  {
    "objectID": "Courses/Git/Errors.html#go-backward-in-your-project",
    "href": "Courses/Git/Errors.html#go-backward-in-your-project",
    "title": "Handle errors in Git",
    "section": "",
    "text": "The goal of Git is to record the changes applied to your code:\n\nwho has contributed ?\nWhere bugs have been introduced ?\nCancel problematic changes.\n\nWe have seen that git log lists all the commits from the most recent to the oldest with their hash code. In order to have more details of your historic, use git reflog lists the commits but all the other actions: message modifications, merge, reset, etc. To move on a previous action, use git checkout hash.\n\n\n\nIf you discover a bug in your project, you want to identify its origin and to track every line of code. git blame file displays for each line:\n\nits ID\nits author\nthe date of the modification\nthe line number\nthe content\n\n\n\n\nSometimes, you do not want to merge a whole branch into another and you only need some specific commits. Imagine you work on “Branch1” and you do many commits but your colleague does not need all the modifications but only some. Type:\n$ git cherry-pick hashs\nto duplicate the commits and add them to your main branch.",
    "crumbs": [
      "Git",
      "Handle errors in Git"
    ]
  },
  {
    "objectID": "Courses/Git/Errors.html#correct-a-bug",
    "href": "Courses/Git/Errors.html#correct-a-bug",
    "title": "Handle errors in Git",
    "section": "",
    "text": "Let’s imagine you develop a new functionality and during this time you note a bug and need to correct it. The method is the following:\n\nkeep into memory what you currently make using git stash\nfind the bug using git log and git bisect\nrestore the problematic file with git revert hash\nrestore what you have done using git stash apply.\n\nThe new command here is git bisect with different options:\n\nstart to begin the research of the bug\nbad HEAD to indicate that the bug is present in the last commit\ngood hash to indicate the most recent commit that does not have the bug From now, the command launches a dichomotic algorithm. For each commit where the command moves on indicate git bisect good if the bug is not present and git bisect bad if not. At the end of the process, you have finally found the commit where the bug appears for the first time, then use git revert hash.",
    "crumbs": [
      "Git",
      "Handle errors in Git"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Unit_Test.html",
    "href": "Courses/Test_Python/Unit_Test.html",
    "title": "Unit Tests",
    "section": "",
    "text": "Disclaimer: this course is adapted from the following sources:",
    "crumbs": [
      "Testing Tools",
      "Unit Tests"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Unit_Test.html#tests",
    "href": "Courses/Test_Python/Unit_Test.html#tests",
    "title": "Unit Tests",
    "section": "Tests",
    "text": "Tests\nTests are small pieces of code ensuring that a part of a program is working as expected.\n\nWhy tests are useful\nThis is why we place the uttermost importance on implementing tests along the development steps. It will help you to ensure:\n\nthat code works correctly.\nthat changes do not break anything.\nthat bugs are not reintroduced.\nrobustness to user errors.\ncode is reachable (i.e., it will actually be executed)\netc.\n\n\n\nTypes of tests\nThere are different kinds of tests, with the more important ones being:\n\nUnit tests: test whether a simple unit element (function, class, etc.) does the right thing.\nIntegration tests: test whether the different parts used by your software work well together.\nNon-regression tests: test whether a new or modified functionality works correctly and that previous functionalities were not affected (e.g., removing a bug does not alter the rest of the code).\n\n\n\nHow to test?\nMany coding languages come with their own test framework. In python, we will focus on pytest. It is simple though powerful. pytest searches for all test*.py files and runs all test* methods found. It outputs a nice error report.\n\n\n\n\n\n\nEXERCISE: pytest\n\n\n\n\nInstall pytest with pip using the user scheme (--user option)\nTest if the command pytest is in your PATH (depending on your configuration you will have to add ~/.local/bin in PATH)\n\n\n\nGet the path to pytest binary as follows with python:\n\nimport pytest\npytest.__path__\n\nThis outputs a directory containing the pytest binary, say /path/to/pytest. Then, to add the path containing pytest in your (Linux) system, you have to type the following command in your terminal:\n$ export PATH=$PATH:/path/to/pytest\n$ pytest --help\n\n\nExample\nLet us assume we have a file inc.py containing\ndef inc1(x):\n    return x + 1\n\ndef inc2(x):\n    return x + 2\nThence, the content of test_inc.py is\nfrom inc import inc1, inc2\n\n# This test will work\ndef test_inc1():\n    assert inc1(3) == 4\n\n# This test will fail\ndef test_inc2():\n    assert inc2(-1) == 4\nTo run these tests:\n$ pytest test_inc.py\n\n\n\n\n\n\nEXERCISE: documentation\n\n\n\n\nCorrect the test_inc2 test.\nDetermine the syntax to run any test in a directory.\nDetermine the syntax to run only the test called test_inc1.",
    "crumbs": [
      "Testing Tools",
      "Unit Tests"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Unit_Test.html#code-coverage",
    "href": "Courses/Test_Python/Unit_Test.html#code-coverage",
    "title": "Unit Tests",
    "section": "Code coverage",
    "text": "Code coverage\npytest comes with some useful plugins. In particular, we will use the coverage report plugin.\nA test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing: this suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage.\nTo install the coverage plugin simply run the pip command:\n$ pip install pytest-cov\nAssuming the inc_cov.py contains:\ndef inc(x):\n    if x &lt; 0:\n        return 0\n    return x + 1\n\ndef dec(x):\n     return x - 1\nand a single test is performed through the file test_inc_cov.py\nfrom inc_cov import inc\n\ndef test_inc():\n     assert inc(3) == 4\nthen\npytest test_inc_cov.py --cov\n============================= test session starts =============================\nplatform linux -- Python 3.10.10, pytest-7.4.2, pluggy-1.0.0\nrootdir: /home/jsalmon/Documents/Mes_cours/Montpellier/HAX712X/Courses/Test\nplugins: dash-2.9.3, cov-4.1.0, anyio-3.6.2\ncollected 1 item\n\ntest_inc_cov.py                                                          [100%]\n\n---------- coverage: platform linux, python 3.10.10-final-0 ----------\nName              Stmts   Miss  Cover\n-------------------------------------\ninc_cov.py            6      2    67%\ntest_inc_cov.py       3      0   100%\n-------------------------------------\nTOTAL                 9      2    78%\n\n\n===============================================================================\n1 passed in 0.02s\n===============================================================================\nTwo lines in inc_cov module were not used. See\npytest --cov --cov-report=html test_inc_cov.py\n\n============================= test session starts =============================\nplatform linux -- Python 3.10.10, pytest-7.4.2, pluggy-1.0.0\nrootdir: /home/jsalmon/Documents/Mes_cours/Montpellier/HAX712X/Courses/Test\nplugins: dash-2.9.3, cov-4.1.0, anyio-3.6.2\ncollected 1 item\n\ntest_inc_cov.py                                                          [100%]\n\n---------- coverage: platform linux, python 3.10.10-final-0 ----------\nCoverage HTML written to dir htmlcov\nfor details.\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nInstall the pytest’s coverage plugin.\nLoad the biketrauma package you can download at https://github.com/HMMA238-2020/biketrauma/\nAdd some unit tests to biketrauma in a new sub-directory ./biketrauma/tests/:\n\nCreate a first test_df() that test if the Côtes-d’or département has 152 accidents. Add a second test_df_log() testing that the log of the number of accidents in the département 92 is close to 7.161622002.\nCreate a test_dl() function that tests the md5sum hash of the downloaded file (a.k.a. bicycle_db.csv). You may use the pooch package or you can use this piece of code to compute the md5sum:\n\n\n\n\nimport hashlib\ndef md5(fname):\n    hash_md5 = hashlib.md5()\n    with open(fname, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\nBy running the following line in the biketrauma/biketrauma location:\npytest --cov-report=html --cov=biketrauma tests/\nyou should reach 79% of code coverage, and the report should look like this (see the htmlcov/index.html file for an interactive report):\n---------- coverage: platform linux, python 3.10.10-final-0 ----------\nName                                    Stmts   Miss  Cover\n----------------------------------------------------------------------\nbiketrauma/__init__.py                      4      0   100%\nbiketrauma/io/Load_db.py                   22      5    77%\nbiketrauma/io/__init__.py                   3      0   100%\nbiketrauma/preprocess/__init__.py           0      0   100%\nbiketrauma/preprocess/get_accident.py       7      0   100%\nbiketrauma/vis/__init__.py                  0      0   100%\nbiketrauma/vis/plot_location.py             6      4    33%\n----------------------------------------------------------------------\nTOTAL                                      42      9    79%",
    "crumbs": [
      "Testing Tools",
      "Unit Tests"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Unit_Test.html#references",
    "href": "Courses/Test_Python/Unit_Test.html#references",
    "title": "Unit Tests",
    "section": "References",
    "text": "References\n\nThe pytest documentation\nWikipedia: Code coverage",
    "crumbs": [
      "Testing Tools",
      "Unit Tests"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html",
    "href": "Courses/Test_Python/Profiling_Debugging.html",
    "title": "Debugging & Profiling",
    "section": "",
    "text": "References:\n\nBuilt-in magic commands\nautoreload\n\nmagic commands are IPython commands such as: %timeit, %matplotlib, %autoreload. They work only in interactive cases (Ipython, Jupyter Notebook, Jupyter lab, etc.).\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 1000\nval = 5.4\n\nLet us compare the time to create a vector of size n and fill it with the value val with various methods, using the `\n\nprint('Using empty and fill:')\n%timeit a = np.empty(n); a.fill(val)\n# Alternative: uncomment below\n# from IPython import get_ipython\n# get_ipython().run_line_magic('timeit', 'a = np.empty(n); a.fill(val)')\n\nUsing empty and fill:\n1.47 µs ± 8.69 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\nprint('Using empty and list syntax:')\n%timeit a = np.empty(n); a[:] = val\n\nUsing empty and list syntax:\n2.06 µs ± 13.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using full:')\n%timeit a = np.full((n,), val)\n\nUsing full:\n3.33 µs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using ones:')\n%timeit a = np.ones(n) * val\n\nUsing ones:\n6.64 µs ± 27.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using repeat:')\n%timeit a = np.repeat(val, n)\n\nUsing repeat:\n5.95 µs ± 8.86 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\nAnother classical way to time a chunk of code is to use the time module, as follows:\n\nimport time\nstart = time.time()\na = np.ones(n) * val\nend = time.time()\nprint(f\"Execution time: {end - start:.5f} s.\")\n\nExecution time: 0.00028 s.",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html#time-efficiency",
    "href": "Courses/Test_Python/Profiling_Debugging.html#time-efficiency",
    "title": "Debugging & Profiling",
    "section": "",
    "text": "References:\n\nBuilt-in magic commands\nautoreload\n\nmagic commands are IPython commands such as: %timeit, %matplotlib, %autoreload. They work only in interactive cases (Ipython, Jupyter Notebook, Jupyter lab, etc.).\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 1000\nval = 5.4\n\nLet us compare the time to create a vector of size n and fill it with the value val with various methods, using the `\n\nprint('Using empty and fill:')\n%timeit a = np.empty(n); a.fill(val)\n# Alternative: uncomment below\n# from IPython import get_ipython\n# get_ipython().run_line_magic('timeit', 'a = np.empty(n); a.fill(val)')\n\nUsing empty and fill:\n1.47 µs ± 8.69 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\nprint('Using empty and list syntax:')\n%timeit a = np.empty(n); a[:] = val\n\nUsing empty and list syntax:\n2.06 µs ± 13.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using full:')\n%timeit a = np.full((n,), val)\n\nUsing full:\n3.33 µs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using ones:')\n%timeit a = np.ones(n) * val\n\nUsing ones:\n6.64 µs ± 27.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nprint('Using repeat:')\n%timeit a = np.repeat(val, n)\n\nUsing repeat:\n5.95 µs ± 8.86 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\nAnother classical way to time a chunk of code is to use the time module, as follows:\n\nimport time\nstart = time.time()\na = np.ones(n) * val\nend = time.time()\nprint(f\"Execution time: {end - start:.5f} s.\")\n\nExecution time: 0.00028 s.",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html#memory-efficiency",
    "href": "Courses/Test_Python/Profiling_Debugging.html#memory-efficiency",
    "title": "Debugging & Profiling",
    "section": "Memory efficiency",
    "text": "Memory efficiency\n\nmemory_profiler\nThis module is deprecated and do not run with Python 3.12\nYou can run a memory profiler with the following magic command (other alternatives are available but require more coding and exporting the reports):\n%load_ext memory_profiler\nFor illustration we compare a way to perform a distance matrix computation between two vectors, using a double loop or a vectorized approach.\n\nn1 = 100\nn2 = 100\nx = np.random.randn(n1)\ny = np.random.randn(n2)\n\n\n%%file mprun_demo.py\nimport numpy as np\n\n\ndef inv_distance(x, y):\n    n1 = len(x)\n    n2 = len(y)\n    dist = np.empty((n1, n2))\n    for i in range(n1):\n        for j in range(n2):\n            dist[i, j] = 1.0 / np.sqrt((x[i] - y[j]) ** 2)\n    return dist\n\n\ndef inv_distance_vect(x, y):\n    n1 = len(x)\n    n2 = len(y)\n    return 1.0 / np.sqrt((x.reshape((n1, 1)) - y.reshape((1, n2))) ** 2)\n\nOverwriting mprun_demo.py\n\n\nWe can time the execution of the two functions:\n\nfrom mprun_demo import inv_distance, inv_distance_vect\n%timeit inv_distance(x, y)\n%timeit inv_distance_vect(x, y)\n\n25.9 ms ± 172 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n77.9 µs ± 122 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nThe conclusion is simple: try to avoid loops in Python when performing matrix computation, and use vectorization and broadcasting as much as possible (or use numba or cython).\nSome memory profiling can be performed with the memory_profiler package to investigate any difference in the memory footprint:\n%mprun -f inv_distance inv_distance(x, y)\n%mprun -T mprun0 -f inv_distance_vect inv_distance_vect(x, y)\nprint(open('mprun0', 'r').read())\nIn this case, the difference is almost negligible concerning memory, only the time computation was significantly different between the two approaches.",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html#profiling",
    "href": "Courses/Test_Python/Profiling_Debugging.html#profiling",
    "title": "Debugging & Profiling",
    "section": "Profiling",
    "text": "Profiling\nA profile is a set of statistics that describes the time taken by various parts of a program.\n\ncProfile\nThe cProfile module provides deterministic profiling of Python programs. A profile is a set of statistics describing how often and for how long various parts of the program run.\nLet us illustrate the profiling for some of the previous examples.\n\nimport cProfile\ncProfile.run('a = np.empty(n); a.fill(val)','fill.prof')\n\n\ncProfile.run('a = np.ones(n) * val','ones.prof')\n\n\ncProfile.run('inv_distance_vect(x, y)', 'inv_distance_vect.prof')\n\n\n\nline_profiler\nYou can use inline magic commands to profile a function line by line.\n%load_ext line_profiler\n%lprun -f inv_distance_vect inv_distance_vect(x, y)\nor\n%lprun -T lprof0 -f inv_distance inv_distance(x, y)\nprint(open('lprof0', 'r').read())\nAn alternative is to use the kernprof package and @profile decorator, see examples here.\n\n\nVisualization of the profiling results\n\nsnakeviz: a browser-based graphical viewer for the output of Python’s cProfile. Here is a video tutorial. Launch the command snakeviz program.prof\n\n\n%load_ext snakeviz\n%snakeviz -t inv_distance(x, y)\n\n \n*** Profile stats marshalled to file '/tmp/tmpnji_oy9d'.\nOpening SnakeViz in a new tab...\n\n\nThe output generated is an interactive file that can be investigated in a browser, looking like:\n\n\n\nsnakeviz\n\n\n\ngprof2dot a Python script to convert the output from many profilers (including cProfile’s output) into a dot graph, with export in .svg. You can run it in your terminal with the following commands: default     python -m cProfile -o output.pstats inv_distances.py     gprof2dot.py -f pstats output.pstats | dot -Tsvg -o output.svg You can then display the .svg file:  \n`viztracer: a nice solution that provides html output with interactive visualization. This can lead to very large traces for non-trivial programs. In this case it is recommended to trim the shortest function calls from the trace file before attempting to load it in the visualizer (See filter options here).\npy-spy: this package can be combined with speedscope for nice visualization.\ngeneral help on Python profiling",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html#debugging-with-pdb",
    "href": "Courses/Test_Python/Profiling_Debugging.html#debugging-with-pdb",
    "title": "Debugging & Profiling",
    "section": "Debugging with pdb",
    "text": "Debugging with pdb\nLet us use import pdb; pdb.set_trace() to enter a code and inspect it. Push the key c and then enter to go next.\nA first recommendation is to use the python debugger in your IDE.\nPure python or IPython can use the pdb package and the command pdb.set_trace(). A command prompt launches when an error is met, and you can check the current status of the environment. Useful shortcuts are available (e.g., the c key or the j key, etc.); a full list is available here. For instance, you can quit the debugger with the command q or quit\ndef function_debile(x):\n    answer = 42\n    answer += x\n    return answer\nfunction_debile(12)\ndef illustrate_pdb(x):\n    answer = 42\n    for i in range(10):\n        import pdb; pdb.set_trace()\n        answer += i\n    answer += x\n    return answer\nillustrate_pdb(12)\nA terminal is launched when a problem occurs, and one can then take over and see what’s going on.\nget_ipython().run_line_magic('pdb', '')\n# %pdb\ndef blobl_func(x):\n    answer = 0\n    for i in range(x, -1, -1):\n        print(i)\n        answer += 1 / i\n\n    return answer\n\nblobl_func(4)",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/Test_Python/Profiling_Debugging.html#debugging-with-vscode",
    "href": "Courses/Test_Python/Profiling_Debugging.html#debugging-with-vscode",
    "title": "Debugging & Profiling",
    "section": "Debugging with VSCode",
    "text": "Debugging with VSCode\nYou can use the debugger in VSCode to debug your code. Here is a video by Arjan Code on the topic.\nThe main point is that you can set breakpoints in your code and then run the debugger to inspect the variables and the flow of the code. All that is done in the IDE, which is very convenient.\nFor that just click on the left of the line number to set a breakpoint, and then run the debugger with the green arrow.\nBelow is a simple example of a code that can be debugged in VSCode:\n\n# %%\ndef sieve_of_eratosthenes(limit):\n    # Create a boolean array \"prime[0..limit]\" and initialize\n    # all entries it as false. A value in prime[i] will\n    # finally be false if i is Not a prime, true if i is a prime.\n    prime = [True for _ in range(limit + 1)]\n    p = 2\n    while p * p &lt; limit:\n        # If prime[p] is not changed, then it is a prime\n        if prime[p] == True:\n            # Update all multiples of p\n            for i in range(p * p, limit + 1, p):\n                prime[i] = False\n        p += 1\n\n    # Collect all prime numbers\n    prime_numbers = [p for p in range(2, limit) if prime[p]]\n    return prime_numbers\n\n\nprint(sieve_of_eratosthenes(100))\nprint(sieve_of_eratosthenes(101))\nClick on  and let the debugger run. You can then inspect the variables and the flow of the code to understand what it is doing.\nReferences:\n\nDebugging Jupyter notebooks by David Hamann\nAdvanced Python Debugging with pdb by Steven Kryskalla\nDebug Python with VSCode",
    "crumbs": [
      "Testing Tools",
      "Debugging & Profiling"
    ]
  },
  {
    "objectID": "Courses/OPP/modules.html",
    "href": "Courses/OPP/modules.html",
    "title": "Creating a Python module",
    "section": "",
    "text": "You already know it: this is a set of python functions and statements, and this is what you import at the beginning of your python functions.\n\n\nIndeed, a module can simply be a single file:\n\nimport fibo\n\nThis does not enter the names of the functions defined in fibo directly in the current symbol table though; it only enters the module name fibo there. Using the module name you can access the functions:\n\nfibo.fib_print(1000)\n\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 \n\n\n\nfibo.fib_list(100)\n\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n\n\nWhen importing a module, several methods are (automatically) defined. Their names are usually prefixed and suffixed by the symbol __, e.g.,\n\nfibo.__name__\n\n'fibo'\n\n\n\nfibo.__file__\n\n'/home/bbensaid/Documents/Cours_MCF1/Dev_Logiciel_website/Courses/OPP/fibo.py'\n\n\n\n\n\nYou can also import a full directory (containing many python files stored in a sub-folder). python looks for a folder located in sys.path list. You have already imported the numpy module, for numerical analysis with python:\n\nimport numpy as np\nprint(np.array([0, 1, 2, 3]).reshape(2, 2))\nprint(np.array([0, 1, 2, 3]).mean())\n\n[[0 1]\n [2 3]]\n1.5\n\n\nIn fact, you have imported the following folder:\n\nnp.__path__\n\n['/home/bbensaid/.local/lib/python3.10/site-packages/numpy']\n\n\nDepending on your installation you might obtain ['/usr/lib/python3.9/site-packages/numpy'] or ['/home/username/anaconda3/lib/python3.7/site-packages/numpy'] if you installed with Anaconda.\nMore precisely you will get either\n&gt;&gt;&gt; np.__file__\n'/usr/lib/python3.9/site-packages/numpy/__init__.py'\nor\n&gt;&gt;&gt; np.__file__\n'/home/username/anaconda3/lib/python3.7/site-packages/numpy/__init__.py'\nAny (sub-)directory of your python module should contain an __init__.py file!\n\n\n\n\n\n\nUseful tips\n\n\n\n\nThe __init__.py file can contain a list of functions to be loaded when the module is imported. It allows to expose functions to users in a concise way.\nYou can also import modules with relative paths, using ., .., ..., etc.\n\n\n\nReference: Absolute vs Relative Imports in Python by Mbithe Nzomo.\n\n\n\nThe built-in function dir() is used to find out which names a module defines. It returns a sorted list of strings:\n\nimport fibo, numpy\ndir(fibo)\n\n['__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'fib_list',\n 'fib_print']\n\n\n\ndir(numpy)\n\n\n\n\n['ALLOW_THREADS',\n 'BUFSIZE',\n 'CLIP',\n 'DataSource',\n 'ERR_CALL',\n 'ERR_DEFAULT',\n 'ERR_IGNORE',\n 'ERR_LOG',\n 'ERR_PRINT',\n 'ERR_RAISE',\n 'ERR_WARN',\n 'FLOATING_POINT_SUPPORT',\n 'FPE_DIVIDEBYZERO',\n 'FPE_INVALID',\n 'FPE_OVERFLOW',\n 'FPE_UNDERFLOW',\n 'False_',\n 'Inf',\n 'Infinity',\n 'MAXDIMS',\n 'MAY_SHARE_BOUNDS',\n 'MAY_SHARE_EXACT',\n 'NAN',\n 'NINF',\n 'NZERO',\n 'NaN',\n 'PINF',\n 'PZERO',\n 'RAISE',\n 'RankWarning',\n 'SHIFT_DIVIDEBYZERO',\n 'SHIFT_INVALID',\n 'SHIFT_OVERFLOW',\n 'SHIFT_UNDERFLOW',\n 'ScalarType',\n 'True_',\n 'UFUNC_BUFSIZE_DEFAULT',\n 'UFUNC_PYVALS_NAME',\n 'WRAP',\n '_CopyMode',\n '_NoValue',\n '_UFUNC_API',\n '__NUMPY_SETUP__',\n '__all__',\n '__builtins__',\n '__cached__',\n '__config__',\n '__deprecated_attrs__',\n '__dir__',\n '__doc__',\n '__expired_functions__',\n '__file__',\n '__former_attrs__',\n '__future_scalars__',\n '__getattr__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '__version__',\n '_add_newdoc_ufunc',\n '_builtins',\n '_core',\n '_distributor_init',\n '_financial_names',\n '_get_promotion_state',\n '_globals',\n '_int_extended_msg',\n '_mat',\n '_no_nep50_warning',\n '_pyinstaller_hooks_dir',\n '_pytesttester',\n '_set_promotion_state',\n '_specific_msg',\n '_typing',\n '_using_numpy2_behavior',\n '_utils',\n 'abs',\n 'absolute',\n 'add',\n 'add_docstring',\n 'add_newdoc',\n 'add_newdoc_ufunc',\n 'all',\n 'allclose',\n 'alltrue',\n 'amax',\n 'amin',\n 'angle',\n 'any',\n 'append',\n 'apply_along_axis',\n 'apply_over_axes',\n 'arange',\n 'arccos',\n 'arccosh',\n 'arcsin',\n 'arcsinh',\n 'arctan',\n 'arctan2',\n 'arctanh',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'argwhere',\n 'around',\n 'array',\n 'array2string',\n 'array_equal',\n 'array_equiv',\n 'array_repr',\n 'array_split',\n 'array_str',\n 'asanyarray',\n 'asarray',\n 'asarray_chkfinite',\n 'ascontiguousarray',\n 'asfarray',\n 'asfortranarray',\n 'asmatrix',\n 'atleast_1d',\n 'atleast_2d',\n 'atleast_3d',\n 'average',\n 'bartlett',\n 'base_repr',\n 'binary_repr',\n 'bincount',\n 'bitwise_and',\n 'bitwise_not',\n 'bitwise_or',\n 'bitwise_xor',\n 'blackman',\n 'block',\n 'bmat',\n 'bool_',\n 'broadcast',\n 'broadcast_arrays',\n 'broadcast_shapes',\n 'broadcast_to',\n 'busday_count',\n 'busday_offset',\n 'busdaycalendar',\n 'byte',\n 'byte_bounds',\n 'bytes_',\n 'c_',\n 'can_cast',\n 'cast',\n 'cbrt',\n 'cdouble',\n 'ceil',\n 'cfloat',\n 'char',\n 'character',\n 'chararray',\n 'choose',\n 'clip',\n 'clongdouble',\n 'clongfloat',\n 'column_stack',\n 'common_type',\n 'compare_chararrays',\n 'compat',\n 'complex128',\n 'complex256',\n 'complex64',\n 'complex_',\n 'complexfloating',\n 'compress',\n 'concatenate',\n 'conj',\n 'conjugate',\n 'convolve',\n 'copy',\n 'copysign',\n 'copyto',\n 'corrcoef',\n 'correlate',\n 'cos',\n 'cosh',\n 'count_nonzero',\n 'cov',\n 'cross',\n 'csingle',\n 'ctypeslib',\n 'cumprod',\n 'cumproduct',\n 'cumsum',\n 'datetime64',\n 'datetime_as_string',\n 'datetime_data',\n 'deg2rad',\n 'degrees',\n 'delete',\n 'deprecate',\n 'deprecate_with_doc',\n 'diag',\n 'diag_indices',\n 'diag_indices_from',\n 'diagflat',\n 'diagonal',\n 'diff',\n 'digitize',\n 'disp',\n 'divide',\n 'divmod',\n 'dot',\n 'double',\n 'dsplit',\n 'dstack',\n 'dtype',\n 'dtypes',\n 'e',\n 'ediff1d',\n 'einsum',\n 'einsum_path',\n 'emath',\n 'empty',\n 'empty_like',\n 'equal',\n 'errstate',\n 'euler_gamma',\n 'exceptions',\n 'exp',\n 'exp2',\n 'expand_dims',\n 'expm1',\n 'expm1x',\n 'extract',\n 'eye',\n 'fabs',\n 'fastCopyAndTranspose',\n 'fft',\n 'fill_diagonal',\n 'find_common_type',\n 'finfo',\n 'fix',\n 'flatiter',\n 'flatnonzero',\n 'flexible',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float128',\n 'float16',\n 'float32',\n 'float64',\n 'float_',\n 'float_power',\n 'floating',\n 'floor',\n 'floor_divide',\n 'fmax',\n 'fmin',\n 'fmod',\n 'format_float_positional',\n 'format_float_scientific',\n 'format_parser',\n 'frexp',\n 'from_dlpack',\n 'frombuffer',\n 'fromfile',\n 'fromfunction',\n 'fromiter',\n 'frompyfunc',\n 'fromregex',\n 'fromstring',\n 'full',\n 'full_like',\n 'gcd',\n 'generic',\n 'genfromtxt',\n 'geomspace',\n 'get_array_wrap',\n 'get_include',\n 'get_printoptions',\n 'getbufsize',\n 'geterr',\n 'geterrcall',\n 'geterrobj',\n 'gradient',\n 'greater',\n 'greater_equal',\n 'half',\n 'hamming',\n 'hanning',\n 'heaviside',\n 'histogram',\n 'histogram2d',\n 'histogram_bin_edges',\n 'histogramdd',\n 'hsplit',\n 'hstack',\n 'hypot',\n 'i0',\n 'identity',\n 'iinfo',\n 'imag',\n 'in1d',\n 'index_exp',\n 'indices',\n 'inexact',\n 'inf',\n 'info',\n 'infty',\n 'inner',\n 'insert',\n 'int16',\n 'int32',\n 'int64',\n 'int8',\n 'int_',\n 'intc',\n 'integer',\n 'interp',\n 'intersect1d',\n 'intp',\n 'invert',\n 'is_busday',\n 'isclose',\n 'iscomplex',\n 'iscomplexobj',\n 'isfinite',\n 'isfortran',\n 'isin',\n 'isinf',\n 'isnan',\n 'isnat',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'isrealobj',\n 'isscalar',\n 'issctype',\n 'issubclass_',\n 'issubdtype',\n 'issubsctype',\n 'iterable',\n 'ix_',\n 'kaiser',\n 'kernel_version',\n 'kron',\n 'lcm',\n 'ldexp',\n 'left_shift',\n 'less',\n 'less_equal',\n 'lexsort',\n 'lib',\n 'linalg',\n 'linspace',\n 'little_endian',\n 'load',\n 'loadtxt',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'logaddexp',\n 'logaddexp2',\n 'logical_and',\n 'logical_not',\n 'logical_or',\n 'logical_xor',\n 'logspace',\n 'longcomplex',\n 'longdouble',\n 'longfloat',\n 'longlong',\n 'lookfor',\n 'ma',\n 'mask_indices',\n 'mat',\n 'matmul',\n 'matrix',\n 'max',\n 'maximum',\n 'maximum_sctype',\n 'may_share_memory',\n 'mean',\n 'median',\n 'memmap',\n 'meshgrid',\n 'mgrid',\n 'min',\n 'min_scalar_type',\n 'minimum',\n 'mintypecode',\n 'mod',\n 'modf',\n 'moveaxis',\n 'msort',\n 'multiply',\n 'nan',\n 'nan_to_num',\n 'nanargmax',\n 'nanargmin',\n 'nancumprod',\n 'nancumsum',\n 'nanmax',\n 'nanmean',\n 'nanmedian',\n 'nanmin',\n 'nanpercentile',\n 'nanprod',\n 'nanquantile',\n 'nanstd',\n 'nansum',\n 'nanvar',\n 'nbytes',\n 'ndarray',\n 'ndenumerate',\n 'ndim',\n 'ndindex',\n 'nditer',\n 'negative',\n 'nested_iters',\n 'newaxis',\n 'nextafter',\n 'nonzero',\n 'not_equal',\n 'numarray',\n 'number',\n 'obj2sctype',\n 'object_',\n 'ogrid',\n 'oldnumeric',\n 'ones',\n 'ones_like',\n 'outer',\n 'packbits',\n 'pad',\n 'partition',\n 'percentile',\n 'pi',\n 'piecewise',\n 'place',\n 'poly',\n 'poly1d',\n 'polyadd',\n 'polyder',\n 'polydiv',\n 'polyfit',\n 'polyint',\n 'polymul',\n 'polynomial',\n 'polysub',\n 'polyval',\n 'positive',\n 'power',\n 'printoptions',\n 'prod',\n 'product',\n 'promote_types',\n 'ptp',\n 'put',\n 'put_along_axis',\n 'putmask',\n 'quantile',\n 'r_',\n 'rad2deg',\n 'radians',\n 'random',\n 'ravel',\n 'ravel_multi_index',\n 'real',\n 'real_if_close',\n 'rec',\n 'recarray',\n 'recfromcsv',\n 'recfromtxt',\n 'reciprocal',\n 'record',\n 'remainder',\n 'repeat',\n 'require',\n 'reshape',\n 'resize',\n 'result_type',\n 'right_shift',\n 'rint',\n 'roll',\n 'rollaxis',\n 'roots',\n 'rot90',\n 'round',\n 'round_',\n 'row_stack',\n 's_',\n 'safe_eval',\n 'save',\n 'savetxt',\n 'savez',\n 'savez_compressed',\n 'sctype2char',\n 'sctypeDict',\n 'sctypes',\n 'searchsorted',\n 'select',\n 'set_numeric_ops',\n 'set_printoptions',\n 'set_string_function',\n 'setbufsize',\n 'setdiff1d',\n 'seterr',\n 'seterrcall',\n 'seterrobj',\n 'setxor1d',\n 'shape',\n 'shares_memory',\n 'short',\n 'show_config',\n 'show_runtime',\n 'sign',\n 'signbit',\n 'signedinteger',\n 'sin',\n 'sinc',\n 'single',\n 'singlecomplex',\n 'sinh',\n 'size',\n 'sometrue',\n 'sort',\n 'sort_complex',\n 'source',\n 'spacing',\n 'split',\n 'sqrt',\n 'square',\n 'squeeze',\n 'stack',\n 'std',\n 'str_',\n 'string_',\n 'subtract',\n 'sum',\n 'swapaxes',\n 'take',\n 'take_along_axis',\n 'tan',\n 'tanh',\n 'tensordot',\n 'test',\n 'testing',\n 'tile',\n 'timedelta64',\n 'trace',\n 'tracemalloc_domain',\n 'transpose',\n 'trapz',\n 'tri',\n 'tril',\n 'tril_indices',\n 'tril_indices_from',\n 'trim_zeros',\n 'triu',\n 'triu_indices',\n 'triu_indices_from',\n 'true_divide',\n 'trunc',\n 'typecodes',\n 'typename',\n 'ubyte',\n 'ufunc',\n 'uint',\n 'uint16',\n 'uint32',\n 'uint64',\n 'uint8',\n 'uintc',\n 'uintp',\n 'ulonglong',\n 'unicode_',\n 'union1d',\n 'unique',\n 'unpackbits',\n 'unravel_index',\n 'unsignedinteger',\n 'unwrap',\n 'ushort',\n 'vander',\n 'var',\n 'vdot',\n 'vectorize',\n 'version',\n 'void',\n 'vsplit',\n 'vstack',\n 'where',\n 'who',\n 'zeros',\n 'zeros_like']\n\n\n\n\n To list every element in your symbol table simply call dir().\nReference: Python doc of the dir function\n\n\n\nA namespace is a set of names (functions, variables, etc.). Different namespaces can co-exist at a given time but are completely isolated. In this way, you can control which function you are using.\nA namespace containing all the built-in names is created when we start the python interpreter and exists as long we don’t exit.\n\ncos(3)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In [11], line 1\n----&gt; 1 cos(3)\n\nNameError: name 'cos' is not defined\n\n\n\nYou need to import a package for mathematical functions:\n\nimport math, numpy as np\nprint(math.cos(3), np.cos(3))\n\n-0.9899924966004454 -0.9899924966004454\n\n\nReference: Python Namespace and Scope tutorial\n\n\n\nWhen a module named spam is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named spam.py in a list of directories given by the variable sys.path. The variable sys.path is initialized from these locations:\n\nThe directory containing the input script (or the current directory when no file is specified).\nThe environment variable PYTHONPATH (a list of directory names, with the same syntax as the shell variable PATH).\n\nReference: Python documentation, The Module Search Path\n\n\n\nFind the loader for a module, optionally within the specified path.\n\nimport importlib\nspam_spec = importlib.util.find_spec(\"spam\")\nfound = spam_spec is not None\nprint(found)\n\nFalse\n\n\nNow,\n\nimport numpy\nnumpy_spec = importlib.util.find_spec(\"numpy\")\nprint(numpy_spec)\n\nModuleSpec(name='numpy', loader=&lt;_frozen_importlib_external.SourceFileLoader object at 0x7ba454140a90&gt;, origin='/home/bbensaid/.local/lib/python3.10/site-packages/numpy/__init__.py', submodule_search_locations=['/home/bbensaid/.local/lib/python3.10/site-packages/numpy'])\n\n\nshould return more information and where the loader is.\nReferences:\n\nPython doc on find_loader\nHow to check if a Python module exists without importing it on Stackoverflow\n\n\n\n\nA module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed only the first time the module name is encountered in an import statement.\nTo force a module to be reloaded, you can use importlib.reload().\nRemark: when using ipython (interactive python, an ancestor of the jupyter notebook), one can use the “magic” command %autoreload 2\nReferences: - Python doc on reload - IPython autoreload\n\n\n\nTo speed up loading modules, python caches the compiled version of each module in the __pycache__ directory under the name module.version.pyc, where the version encodes the format of the compiled file; it generally contains the python version number.\nFor example, in CPython release 3.3 the compiled version of spam.py would be cached as __pycache__/spam.cpython-33.pyc. This naming convention allows compiled modules from different releases and different versions of python to coexist.\n\n\n\n\n\n\nUseful git tip\n\n\n\nYou should add __pycache__ entry in your .gitignore file to avoid adding a compiled python file to your project.",
    "crumbs": [
      "OOP",
      "Creating a Python module"
    ]
  },
  {
    "objectID": "Courses/OPP/modules.html#what-is-a-python-module",
    "href": "Courses/OPP/modules.html#what-is-a-python-module",
    "title": "Creating a Python module",
    "section": "",
    "text": "You already know it: this is a set of python functions and statements, and this is what you import at the beginning of your python functions.\n\n\nIndeed, a module can simply be a single file:\n\nimport fibo\n\nThis does not enter the names of the functions defined in fibo directly in the current symbol table though; it only enters the module name fibo there. Using the module name you can access the functions:\n\nfibo.fib_print(1000)\n\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 \n\n\n\nfibo.fib_list(100)\n\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n\n\nWhen importing a module, several methods are (automatically) defined. Their names are usually prefixed and suffixed by the symbol __, e.g.,\n\nfibo.__name__\n\n'fibo'\n\n\n\nfibo.__file__\n\n'/home/bbensaid/Documents/Cours_MCF1/Dev_Logiciel_website/Courses/OPP/fibo.py'\n\n\n\n\n\nYou can also import a full directory (containing many python files stored in a sub-folder). python looks for a folder located in sys.path list. You have already imported the numpy module, for numerical analysis with python:\n\nimport numpy as np\nprint(np.array([0, 1, 2, 3]).reshape(2, 2))\nprint(np.array([0, 1, 2, 3]).mean())\n\n[[0 1]\n [2 3]]\n1.5\n\n\nIn fact, you have imported the following folder:\n\nnp.__path__\n\n['/home/bbensaid/.local/lib/python3.10/site-packages/numpy']\n\n\nDepending on your installation you might obtain ['/usr/lib/python3.9/site-packages/numpy'] or ['/home/username/anaconda3/lib/python3.7/site-packages/numpy'] if you installed with Anaconda.\nMore precisely you will get either\n&gt;&gt;&gt; np.__file__\n'/usr/lib/python3.9/site-packages/numpy/__init__.py'\nor\n&gt;&gt;&gt; np.__file__\n'/home/username/anaconda3/lib/python3.7/site-packages/numpy/__init__.py'\nAny (sub-)directory of your python module should contain an __init__.py file!\n\n\n\n\n\n\nUseful tips\n\n\n\n\nThe __init__.py file can contain a list of functions to be loaded when the module is imported. It allows to expose functions to users in a concise way.\nYou can also import modules with relative paths, using ., .., ..., etc.\n\n\n\nReference: Absolute vs Relative Imports in Python by Mbithe Nzomo.\n\n\n\nThe built-in function dir() is used to find out which names a module defines. It returns a sorted list of strings:\n\nimport fibo, numpy\ndir(fibo)\n\n['__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'fib_list',\n 'fib_print']\n\n\n\ndir(numpy)\n\n\n\n\n['ALLOW_THREADS',\n 'BUFSIZE',\n 'CLIP',\n 'DataSource',\n 'ERR_CALL',\n 'ERR_DEFAULT',\n 'ERR_IGNORE',\n 'ERR_LOG',\n 'ERR_PRINT',\n 'ERR_RAISE',\n 'ERR_WARN',\n 'FLOATING_POINT_SUPPORT',\n 'FPE_DIVIDEBYZERO',\n 'FPE_INVALID',\n 'FPE_OVERFLOW',\n 'FPE_UNDERFLOW',\n 'False_',\n 'Inf',\n 'Infinity',\n 'MAXDIMS',\n 'MAY_SHARE_BOUNDS',\n 'MAY_SHARE_EXACT',\n 'NAN',\n 'NINF',\n 'NZERO',\n 'NaN',\n 'PINF',\n 'PZERO',\n 'RAISE',\n 'RankWarning',\n 'SHIFT_DIVIDEBYZERO',\n 'SHIFT_INVALID',\n 'SHIFT_OVERFLOW',\n 'SHIFT_UNDERFLOW',\n 'ScalarType',\n 'True_',\n 'UFUNC_BUFSIZE_DEFAULT',\n 'UFUNC_PYVALS_NAME',\n 'WRAP',\n '_CopyMode',\n '_NoValue',\n '_UFUNC_API',\n '__NUMPY_SETUP__',\n '__all__',\n '__builtins__',\n '__cached__',\n '__config__',\n '__deprecated_attrs__',\n '__dir__',\n '__doc__',\n '__expired_functions__',\n '__file__',\n '__former_attrs__',\n '__future_scalars__',\n '__getattr__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '__version__',\n '_add_newdoc_ufunc',\n '_builtins',\n '_core',\n '_distributor_init',\n '_financial_names',\n '_get_promotion_state',\n '_globals',\n '_int_extended_msg',\n '_mat',\n '_no_nep50_warning',\n '_pyinstaller_hooks_dir',\n '_pytesttester',\n '_set_promotion_state',\n '_specific_msg',\n '_typing',\n '_using_numpy2_behavior',\n '_utils',\n 'abs',\n 'absolute',\n 'add',\n 'add_docstring',\n 'add_newdoc',\n 'add_newdoc_ufunc',\n 'all',\n 'allclose',\n 'alltrue',\n 'amax',\n 'amin',\n 'angle',\n 'any',\n 'append',\n 'apply_along_axis',\n 'apply_over_axes',\n 'arange',\n 'arccos',\n 'arccosh',\n 'arcsin',\n 'arcsinh',\n 'arctan',\n 'arctan2',\n 'arctanh',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'argwhere',\n 'around',\n 'array',\n 'array2string',\n 'array_equal',\n 'array_equiv',\n 'array_repr',\n 'array_split',\n 'array_str',\n 'asanyarray',\n 'asarray',\n 'asarray_chkfinite',\n 'ascontiguousarray',\n 'asfarray',\n 'asfortranarray',\n 'asmatrix',\n 'atleast_1d',\n 'atleast_2d',\n 'atleast_3d',\n 'average',\n 'bartlett',\n 'base_repr',\n 'binary_repr',\n 'bincount',\n 'bitwise_and',\n 'bitwise_not',\n 'bitwise_or',\n 'bitwise_xor',\n 'blackman',\n 'block',\n 'bmat',\n 'bool_',\n 'broadcast',\n 'broadcast_arrays',\n 'broadcast_shapes',\n 'broadcast_to',\n 'busday_count',\n 'busday_offset',\n 'busdaycalendar',\n 'byte',\n 'byte_bounds',\n 'bytes_',\n 'c_',\n 'can_cast',\n 'cast',\n 'cbrt',\n 'cdouble',\n 'ceil',\n 'cfloat',\n 'char',\n 'character',\n 'chararray',\n 'choose',\n 'clip',\n 'clongdouble',\n 'clongfloat',\n 'column_stack',\n 'common_type',\n 'compare_chararrays',\n 'compat',\n 'complex128',\n 'complex256',\n 'complex64',\n 'complex_',\n 'complexfloating',\n 'compress',\n 'concatenate',\n 'conj',\n 'conjugate',\n 'convolve',\n 'copy',\n 'copysign',\n 'copyto',\n 'corrcoef',\n 'correlate',\n 'cos',\n 'cosh',\n 'count_nonzero',\n 'cov',\n 'cross',\n 'csingle',\n 'ctypeslib',\n 'cumprod',\n 'cumproduct',\n 'cumsum',\n 'datetime64',\n 'datetime_as_string',\n 'datetime_data',\n 'deg2rad',\n 'degrees',\n 'delete',\n 'deprecate',\n 'deprecate_with_doc',\n 'diag',\n 'diag_indices',\n 'diag_indices_from',\n 'diagflat',\n 'diagonal',\n 'diff',\n 'digitize',\n 'disp',\n 'divide',\n 'divmod',\n 'dot',\n 'double',\n 'dsplit',\n 'dstack',\n 'dtype',\n 'dtypes',\n 'e',\n 'ediff1d',\n 'einsum',\n 'einsum_path',\n 'emath',\n 'empty',\n 'empty_like',\n 'equal',\n 'errstate',\n 'euler_gamma',\n 'exceptions',\n 'exp',\n 'exp2',\n 'expand_dims',\n 'expm1',\n 'expm1x',\n 'extract',\n 'eye',\n 'fabs',\n 'fastCopyAndTranspose',\n 'fft',\n 'fill_diagonal',\n 'find_common_type',\n 'finfo',\n 'fix',\n 'flatiter',\n 'flatnonzero',\n 'flexible',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float128',\n 'float16',\n 'float32',\n 'float64',\n 'float_',\n 'float_power',\n 'floating',\n 'floor',\n 'floor_divide',\n 'fmax',\n 'fmin',\n 'fmod',\n 'format_float_positional',\n 'format_float_scientific',\n 'format_parser',\n 'frexp',\n 'from_dlpack',\n 'frombuffer',\n 'fromfile',\n 'fromfunction',\n 'fromiter',\n 'frompyfunc',\n 'fromregex',\n 'fromstring',\n 'full',\n 'full_like',\n 'gcd',\n 'generic',\n 'genfromtxt',\n 'geomspace',\n 'get_array_wrap',\n 'get_include',\n 'get_printoptions',\n 'getbufsize',\n 'geterr',\n 'geterrcall',\n 'geterrobj',\n 'gradient',\n 'greater',\n 'greater_equal',\n 'half',\n 'hamming',\n 'hanning',\n 'heaviside',\n 'histogram',\n 'histogram2d',\n 'histogram_bin_edges',\n 'histogramdd',\n 'hsplit',\n 'hstack',\n 'hypot',\n 'i0',\n 'identity',\n 'iinfo',\n 'imag',\n 'in1d',\n 'index_exp',\n 'indices',\n 'inexact',\n 'inf',\n 'info',\n 'infty',\n 'inner',\n 'insert',\n 'int16',\n 'int32',\n 'int64',\n 'int8',\n 'int_',\n 'intc',\n 'integer',\n 'interp',\n 'intersect1d',\n 'intp',\n 'invert',\n 'is_busday',\n 'isclose',\n 'iscomplex',\n 'iscomplexobj',\n 'isfinite',\n 'isfortran',\n 'isin',\n 'isinf',\n 'isnan',\n 'isnat',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'isrealobj',\n 'isscalar',\n 'issctype',\n 'issubclass_',\n 'issubdtype',\n 'issubsctype',\n 'iterable',\n 'ix_',\n 'kaiser',\n 'kernel_version',\n 'kron',\n 'lcm',\n 'ldexp',\n 'left_shift',\n 'less',\n 'less_equal',\n 'lexsort',\n 'lib',\n 'linalg',\n 'linspace',\n 'little_endian',\n 'load',\n 'loadtxt',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'logaddexp',\n 'logaddexp2',\n 'logical_and',\n 'logical_not',\n 'logical_or',\n 'logical_xor',\n 'logspace',\n 'longcomplex',\n 'longdouble',\n 'longfloat',\n 'longlong',\n 'lookfor',\n 'ma',\n 'mask_indices',\n 'mat',\n 'matmul',\n 'matrix',\n 'max',\n 'maximum',\n 'maximum_sctype',\n 'may_share_memory',\n 'mean',\n 'median',\n 'memmap',\n 'meshgrid',\n 'mgrid',\n 'min',\n 'min_scalar_type',\n 'minimum',\n 'mintypecode',\n 'mod',\n 'modf',\n 'moveaxis',\n 'msort',\n 'multiply',\n 'nan',\n 'nan_to_num',\n 'nanargmax',\n 'nanargmin',\n 'nancumprod',\n 'nancumsum',\n 'nanmax',\n 'nanmean',\n 'nanmedian',\n 'nanmin',\n 'nanpercentile',\n 'nanprod',\n 'nanquantile',\n 'nanstd',\n 'nansum',\n 'nanvar',\n 'nbytes',\n 'ndarray',\n 'ndenumerate',\n 'ndim',\n 'ndindex',\n 'nditer',\n 'negative',\n 'nested_iters',\n 'newaxis',\n 'nextafter',\n 'nonzero',\n 'not_equal',\n 'numarray',\n 'number',\n 'obj2sctype',\n 'object_',\n 'ogrid',\n 'oldnumeric',\n 'ones',\n 'ones_like',\n 'outer',\n 'packbits',\n 'pad',\n 'partition',\n 'percentile',\n 'pi',\n 'piecewise',\n 'place',\n 'poly',\n 'poly1d',\n 'polyadd',\n 'polyder',\n 'polydiv',\n 'polyfit',\n 'polyint',\n 'polymul',\n 'polynomial',\n 'polysub',\n 'polyval',\n 'positive',\n 'power',\n 'printoptions',\n 'prod',\n 'product',\n 'promote_types',\n 'ptp',\n 'put',\n 'put_along_axis',\n 'putmask',\n 'quantile',\n 'r_',\n 'rad2deg',\n 'radians',\n 'random',\n 'ravel',\n 'ravel_multi_index',\n 'real',\n 'real_if_close',\n 'rec',\n 'recarray',\n 'recfromcsv',\n 'recfromtxt',\n 'reciprocal',\n 'record',\n 'remainder',\n 'repeat',\n 'require',\n 'reshape',\n 'resize',\n 'result_type',\n 'right_shift',\n 'rint',\n 'roll',\n 'rollaxis',\n 'roots',\n 'rot90',\n 'round',\n 'round_',\n 'row_stack',\n 's_',\n 'safe_eval',\n 'save',\n 'savetxt',\n 'savez',\n 'savez_compressed',\n 'sctype2char',\n 'sctypeDict',\n 'sctypes',\n 'searchsorted',\n 'select',\n 'set_numeric_ops',\n 'set_printoptions',\n 'set_string_function',\n 'setbufsize',\n 'setdiff1d',\n 'seterr',\n 'seterrcall',\n 'seterrobj',\n 'setxor1d',\n 'shape',\n 'shares_memory',\n 'short',\n 'show_config',\n 'show_runtime',\n 'sign',\n 'signbit',\n 'signedinteger',\n 'sin',\n 'sinc',\n 'single',\n 'singlecomplex',\n 'sinh',\n 'size',\n 'sometrue',\n 'sort',\n 'sort_complex',\n 'source',\n 'spacing',\n 'split',\n 'sqrt',\n 'square',\n 'squeeze',\n 'stack',\n 'std',\n 'str_',\n 'string_',\n 'subtract',\n 'sum',\n 'swapaxes',\n 'take',\n 'take_along_axis',\n 'tan',\n 'tanh',\n 'tensordot',\n 'test',\n 'testing',\n 'tile',\n 'timedelta64',\n 'trace',\n 'tracemalloc_domain',\n 'transpose',\n 'trapz',\n 'tri',\n 'tril',\n 'tril_indices',\n 'tril_indices_from',\n 'trim_zeros',\n 'triu',\n 'triu_indices',\n 'triu_indices_from',\n 'true_divide',\n 'trunc',\n 'typecodes',\n 'typename',\n 'ubyte',\n 'ufunc',\n 'uint',\n 'uint16',\n 'uint32',\n 'uint64',\n 'uint8',\n 'uintc',\n 'uintp',\n 'ulonglong',\n 'unicode_',\n 'union1d',\n 'unique',\n 'unpackbits',\n 'unravel_index',\n 'unsignedinteger',\n 'unwrap',\n 'ushort',\n 'vander',\n 'var',\n 'vdot',\n 'vectorize',\n 'version',\n 'void',\n 'vsplit',\n 'vstack',\n 'where',\n 'who',\n 'zeros',\n 'zeros_like']\n\n\n\n\n To list every element in your symbol table simply call dir().\nReference: Python doc of the dir function\n\n\n\nA namespace is a set of names (functions, variables, etc.). Different namespaces can co-exist at a given time but are completely isolated. In this way, you can control which function you are using.\nA namespace containing all the built-in names is created when we start the python interpreter and exists as long we don’t exit.\n\ncos(3)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In [11], line 1\n----&gt; 1 cos(3)\n\nNameError: name 'cos' is not defined\n\n\n\nYou need to import a package for mathematical functions:\n\nimport math, numpy as np\nprint(math.cos(3), np.cos(3))\n\n-0.9899924966004454 -0.9899924966004454\n\n\nReference: Python Namespace and Scope tutorial\n\n\n\nWhen a module named spam is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named spam.py in a list of directories given by the variable sys.path. The variable sys.path is initialized from these locations:\n\nThe directory containing the input script (or the current directory when no file is specified).\nThe environment variable PYTHONPATH (a list of directory names, with the same syntax as the shell variable PATH).\n\nReference: Python documentation, The Module Search Path\n\n\n\nFind the loader for a module, optionally within the specified path.\n\nimport importlib\nspam_spec = importlib.util.find_spec(\"spam\")\nfound = spam_spec is not None\nprint(found)\n\nFalse\n\n\nNow,\n\nimport numpy\nnumpy_spec = importlib.util.find_spec(\"numpy\")\nprint(numpy_spec)\n\nModuleSpec(name='numpy', loader=&lt;_frozen_importlib_external.SourceFileLoader object at 0x7ba454140a90&gt;, origin='/home/bbensaid/.local/lib/python3.10/site-packages/numpy/__init__.py', submodule_search_locations=['/home/bbensaid/.local/lib/python3.10/site-packages/numpy'])\n\n\nshould return more information and where the loader is.\nReferences:\n\nPython doc on find_loader\nHow to check if a Python module exists without importing it on Stackoverflow\n\n\n\n\nA module can contain executable statements as well as function definitions. These statements are intended to initialize the module. They are executed only the first time the module name is encountered in an import statement.\nTo force a module to be reloaded, you can use importlib.reload().\nRemark: when using ipython (interactive python, an ancestor of the jupyter notebook), one can use the “magic” command %autoreload 2\nReferences: - Python doc on reload - IPython autoreload\n\n\n\nTo speed up loading modules, python caches the compiled version of each module in the __pycache__ directory under the name module.version.pyc, where the version encodes the format of the compiled file; it generally contains the python version number.\nFor example, in CPython release 3.3 the compiled version of spam.py would be cached as __pycache__/spam.cpython-33.pyc. This naming convention allows compiled modules from different releases and different versions of python to coexist.\n\n\n\n\n\n\nUseful git tip\n\n\n\nYou should add __pycache__ entry in your .gitignore file to avoid adding a compiled python file to your project.",
    "crumbs": [
      "OOP",
      "Creating a Python module"
    ]
  },
  {
    "objectID": "Courses/OPP/modules.html#the-python-package-index-pypi-repository",
    "href": "Courses/OPP/modules.html#the-python-package-index-pypi-repository",
    "title": "Creating a Python module",
    "section": "The python Package Index (Pypi) repository",
    "text": "The python Package Index (Pypi) repository\nThe python Package Index, abbreviated as PyPI, is the official third-party software repository for python. PyPI primarily hosts python packages in the form of archives called sdists (source distributions) or pre-compiled “wheels”.\n\n\n\n\n\n\nEXERCISE: pypi\n\n\n\n\nGo to https://test.pypi.org/ and describe the aim of this repository.\n\n\n\n\nPip\npip is a de facto standard package-management system used to install and manage software packages from PyPi.\n$ pip install some-package-name\n$ pip uninstall some-package-name\n$ pip search some-package-name\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nInstall the modules pooch, setuptools, pandas, pygal and pygal_maps_fr. Beware, you should use the option --user to force the installation in your home directory.\nList all the package in your venv using pip.\n\n\n\nIt is possible to install a local module with pip\n$ pip install /path/to/my/local/module\nwhere /path/to/my/local/module is the path to the module. But if some changes occur in the /path/to/my/local/module folder, the module will not be reloaded. This might be annoying during the development stage. To force python to reload the module at each change call, consider the -e option:\n$ pip install -e /path/to/my/local/module",
    "crumbs": [
      "OOP",
      "Creating a Python module"
    ]
  },
  {
    "objectID": "Courses/OPP/modules.html#creating-a-python-module",
    "href": "Courses/OPP/modules.html#creating-a-python-module",
    "title": "Creating a Python module",
    "section": "Creating a python module",
    "text": "Creating a python module\nWe are going to create a simple package. The structure is always similar, and an example can be found for instance here.\nReference: How To Package Your Python Code\n\nPicking a name\npython module/package names should generally follow the following constraints:\n\nAll lowercase (🇫🇷: en minuscule)\nUnique on PyPI, even if you do not want to make your package publicly available (you might want to specify it privately as a dependency later)\nUnderscore-separated or no word separators at all, and do not use hyphens (i.e., use _ not -).\n\nWe are going to create a module called biketrauma to visualize the bicycle_db (Source: here, adapted for the original version from there) used in the some of these lectures.\n\n\nModule structure\nThe initial directory structure for biketrauma should look like this:\npackaging_tutorial/\n    ├── biketrauma/\n    │     ├── __init__.py\n    │     └── data/\n    ├── setup.py\n    └── .gitignore\nThe top-level directory is the root of our Version Control System (e.g. git) repository packaging_tutorial. The sub-directory, biketrauma, is the actual python module.\n\n\n\n\n\n\nEXERCISE: packaging\n\n\n\nWe are going to create a new python module that can be used to visualize the bike dataset.\n\nCreate a new folder ~/packaging_tutorial/ and initialize a git in it.\nCreate a .gitignore file to ignore __pycache__, .vscode directories and files containing the string egg-info or dist in their name as well.\nPush your work into a new repository on your github.\nCreate a sub-folder ~/packaging_tutorial/biketrauma. This is where our python module will be stored.\nCreate a ~/packaging_tutorial/biketrauma/__init__.py file where a string __version__ defined at 0.0.1.\nCreate an empty sub-folder ~/packaging_tutorial/biketrauma/data locally on your computer/session. How to add it to git? (Hint: .gitkeep)\nCreate an empty ~/packaging_tutorial/setup.py file.\nCommit and push into your repository.\n\nReference: Single-sourcing the package version.\n\n\n\n\nSub-modules\nThe final directory structure of our module will look like:\npackaging_tutorial/\n    ├── biketrauma/\n    │     ├── __init__.py\n    │     ├── io/\n    │     │     ├─ __init__.py\n    │     │     └─ Load_db.py\n    │     ├── preprocess/\n    │     │     ├─ __init__.py\n    │     │     └─ get_accident.py\n    │     └── vis/\n    │     │     ├─ __init__.py\n    │     │     └─ plot_location.py\n    │     └── data/\n    │     │     └─ .gitkeep\n    ├── setup.py\n    ├── script.py\n    └── .gitignore\n\n\n\n\n\n\nEXERCISE: modules\n\n\n\nSee the git repo of the course: the subfolder Courses/Python-modules/modules_files: contains some files you should include into your package tree:\n\nCopy the script.py into the project root folder.\nAdd some sub-folders to biketrauma directory called io (for input/output), preprocess, vis (for visualization). Add an empty __init__.py file into the module and sub-module folders.\nPopulate the preprocess sub-module with the get_accident.py file.\nPopulate the vis sub-module with the plot_location.py file.\nPopulate the io sub-module with the file Load_db.py (it downloads the bike dataset). At the loading step of the io sub-module, it should create the following variables (Hint: this is done in the __init__.py file)\nurl_db = \"https://github.com/josephsalmon/HAX712X/raw/main/Data/accidents-velos_2022.csv.xz\"\npath_target = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), \"..\", \"data\", \"bicycle_db.csv.xz\"\n)\nCommit your changes.\n\n\n\n\n\nFix the import\nIn order to load the functions in the io, preprocess and vis sub-modules, you can add the following lines to the ~/packaging_tutorial/biketrauma/__init__.py:\nfrom .io.Load_db import Load_db\nfrom .vis.plot_location import plot_location\nfrom .preprocess.get_accident import get_accident\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nBe sure to have the following packages installed (Hint: this list could be saved in a requirements.txt file in the project root folder.):\ntqdm\npygal_maps_fr\npooch\npandas\nnumpy\npygal\nsetuptools\nlxml\nCheck that your module does work by launching the script.\n$ cd ~/packaging_tutorial\n$ python script.py\nYou can then open the file created file biketrauma_map.svg in a navigator: {=html}  &lt;center&gt;   &lt;iframe width= 100% height=500 src=\"biketrauma_map.svg\"&gt;&lt;/iframe&gt;  &lt;/center&gt; \nCommit and push your changes.\n\n\n\n\n\nPackage the module with setuptools\nThe main setup configuration file, setup.py, should contain a single call to setuptools.setup(), like so:\nfrom setuptools import setup\nfrom biketrauma import __version__ as current_version\n\nsetup(\n  name='biketrauma',\n  version=current_version,\n  description='Visualization of a bicycle accident db',\n  url='http://github.com/xxxxxxxxxxx.git',\n  author='xxxxxxxxxxx',\n  author_email='xxxxxxxxxx@xxxxxxxxxxxxx.xxx',\n  license='MIT',\n  packages=['biketrauma','biketrauma.io', 'biketrauma.preprocess', 'biketrauma.vis'],\n  zip_safe=False\n)\nTo create a sdist package (a source distribution):\n$ cd ~/packaging_tutorial/\n$ python setup.py sdist\nThis will create dist/biketrauma-0.0.1.tar.gz inside the top-level directory. You can now install it with\n$ pip install ~/packaging_tutorial/dist/biketrauma-0.0.1.tar.gz\nReferences:\n\nBuilding and Distributing Packages with Setuptools\nPackaging Python Projects\n\n\n\nAdd requirement file\nTo get a list of the installed packages in your current Venv, you can use the following command:\n$ pip freeze &gt; requirements.txt\nUnfortunately, it may generate a huge collection of package dependencies. To get a sparser list, you can use pipreqs.\n\n\n\n\n\n\nEXERCISE: requirements\n\n\n\nCreate a minimal requirements.txt file with pipreqs. Add it to the biketrauma module.\n\n\n\n\nUpload on PyPI\ntwine is a utility for publishing python packages on PyPI. We are going to use the test repository https://test.pypi.org/.\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nCreate an account on the PyPI test repository.\nThis is quite easy to upload a python module on PyPI:\nCreate some distributions in the normal way:\n$ python setup.py sdist bdist_wheel\nUpload with twine to Test PyPI and verify things look right. Twine will automatically prompt for your username and password:\n$ twine upload --repository-url https://test.pypi.org/legacy/ dist/*\nusername: ...\npassword: ...\nUpload to PyPI:\n$ twine upload dist/*\n\n\n\n\n\n\n\n\n\nAbout the data folder\n\n\n\nWe have included the data folder in the sub-module tree which is not a good practice: permission may not be granted in the destination dir of the module… A better idea could be to create a data folder in a cache or temp directory.",
    "crumbs": [
      "OOP",
      "Creating a Python module"
    ]
  },
  {
    "objectID": "Courses/OPP/modules.html#references",
    "href": "Courses/OPP/modules.html#references",
    "title": "Creating a Python module",
    "section": "References",
    "text": "References\n\nPython Packaging User Guide\nTwine, uploads of source, provides additional documentation on using twine to upload packages to PyPI.",
    "crumbs": [
      "OOP",
      "Creating a Python module"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html",
    "href": "Courses/OPP/Docs.html",
    "title": "Markup languages / Documentation",
    "section": "",
    "text": "Before starting a description of Sphinx, we first start by introducing, markdown, quarto and reStructuredText files.\nDisclaimer: this course is mainly adapted from the Quarto and Sphinx documentation.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#markdown",
    "href": "Courses/OPP/Docs.html#markdown",
    "title": "Markup languages / Documentation",
    "section": "Markdown",
    "text": "Markdown\nMarkdown is a lightweight markup language developed by John Gruber and Aaron Swartz1 .\nHence Markdown is an easy-to-read markup language, popular for simple text formatting, creating documentation for software projects, but also for writing nice emails (for Thunderbird or any browser, etc.).\nThe extension for a Mardkown file is .md. To render such a file in VSCode or Codium, and possibly export a .pdf or .html file, you need to install a package for that (for instance the Markdown All in One could be installed with Ctrl+Shift+p, and looking for the right name in Extensions: Install Extensions).\n\n\nLeft: Markdown code\n\n# Title level 1\n\n## Title level 2\n\n\n# Another title level 1\n\n## Another title level 2\n\n\nThis would be a paragraph. You can use **bold font** but also *italic* and or  ~~strikethrough~~.\n\nOther elements are listed below :\n\n- item 1\n- item 2\n  - sub-item 1\n  - sub-item 2\n\n\n\nRight: rendered\n\nTitle level 1\n\nTitle level 2\n\n\n\nTitle level 1\n\nTitle level 2\nThis would be a paragraph. You can use bold font but also italic or strikethrough.\nOther elements are listed here :\n\nitem 1\nitem 2\n\nsub-item 1\nsub-item 2\n\n\n\n\n\n\nA full description can be found here: https://www.markdownguide.org/basic-syntax/; a nice cheatsheet is also available here: https://www.markdownguide.org/cheat-sheet/.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#title-level-2",
    "href": "Courses/OPP/Docs.html#title-level-2",
    "title": "Markup languages / Documentation",
    "section": "Title level 2",
    "text": "Title level 2",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#title-level-2-1",
    "href": "Courses/OPP/Docs.html#title-level-2-1",
    "title": "Markup languages / Documentation",
    "section": "Title level 2",
    "text": "Title level 2\nThis would be a paragraph. You can use bold font but also italic or strikethrough.\nOther elements are listed here :\n\nitem 1\nitem 2\n\nsub-item 1\nsub-item 2",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#quarto",
    "href": "Courses/OPP/Docs.html#quarto",
    "title": "Markup languages / Documentation",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. For instance, we have used it for creating the website you are reading right now. We describe here its main usage. Quarto allows rendering markdown elements and creating a website easily, that can run some code (R, Python, etc.) and display the results (tables, figures, etc.), possibly in an interactive way. The extension is .qmd for Quarto files.\nAs an example, you can see below how to create a code snippet in a Quarto, and how it is rendered, along with the code output.\n\n\nLeft: Quarto code\n```{python}\nimport numpy as np\nprint(np.sum(np.ones(12)))\n```\n\n\n\nRight: Rendered\n\nimport numpy as np\nprint(np.sum(np.ones(12)))\n\n12.0\n\n\n\n\nDocumentation for using python in Quarto: https://quarto.org/docs/computations/python.html",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#restructuredtext",
    "href": "Courses/OPP/Docs.html#restructuredtext",
    "title": "Markup languages / Documentation",
    "section": "reStructuredText",
    "text": "reStructuredText\nreStructuredText (.RST, .ReST, or .reST) is a file format for textual data used primarily in the Python programming language community for technical documentation and is similar to the Markdown format.\nIt is part of the Docutils project of the Python Doc-SIG (Documentation Special Interest Group), aimed at creating a set of tools for Python similar to Javadoc for Java or Plain Old Documentation (pod) for Perl or vignette for R.\nDocutils can extract comments and information from Python programs, and format them into various forms of program documentation.\nIn this sense, reStructuredText is a lightweight markup language designed to be both:\n\nProcessable by documentation-processing software such as Docutils.\nEasily readable by human programmers who are reading and writing Python source code.\n\nReferences:\n\nWikipedia on ReStructuredText\nMarkup languages\nDocumentation generators\n\n\nSyntax\nA ReST file is a plain text file with a .rst extension. Like Markdown, it allows you to easily write formatted text.\n\nHeaders\nSection Header\n==============\n\nSubsection Header\n-----------------\n\n\nLists\n- A bullet list item\n- Second item\n\n  - A sub-item (indentation matters!)\n\n- Spacing between items creates separate lists\n\n- Third item\n\n1) An enumerated list item\n\n2) Second item\n\n   a) Sub-item that goes on at length and thus needs\n      to be wrapped. Note the indentation that must\n      match the beginning of the text, not the\n      enumerator.\n\n      i) List items can even include\n\n         paragraph breaks.\n\n3) Third item\n\n#) Another enumerated list item\n\n#) Second item\n\n\nImages\n\n.. image:: /path/to/image.jpg\n   :height: 100\n   :width: 200\n   :scale: 50\n   :align: center\n   :alt: ordinateur\n\n   Caption text rendered below the image...\n\n\nNamed links and anonymous links\nA sentence with links to Wikipedia and the Linux kernel archive.\n.. Wikipedia: https://www.wikipedia.org/\n.. Linux kernel archive: https://www.kernel.org/\nAnother sentence with an anonymous link to the Python website.\n__ https://www.python.org/\n\n\n\n\n\n\nNote\n\n\n\nNamed links and anonymous links are enclosed in grave accents (`), and not in apostrophes (’).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to create references to labels linked to an image, a section, in the .rst file, etc.\n\n\n\n\nLiteral blocks\n::\n  some literal text\n\nThis may also be used inline at the end of a paragraph, like so::\nSome more literal text:\n.. code:: python\n\n   print(\"A literal block directive explicitly marked as python code\")",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#sphinx-a-documentation-generator",
    "href": "Courses/OPP/Docs.html#sphinx-a-documentation-generator",
    "title": "Markup languages / Documentation",
    "section": "Sphinx: a documentation generator",
    "text": "Sphinx: a documentation generator\nSphinx is an extension of reStructuredText.\nThe documentation of a python package is usually located in a docs or doc folder located at the root of a project. For instance, in the biketrauma module we have considered in the courses Creating a python module, the structure could be as follows:\npackaging_tutorial/\n    ├── biketrauma/\n    │     ├── __init__.py\n    │     ├── io/\n    │     ├── preprocess/\n    │     └── vis/\n    │     └── data/\n    ├── doc/\n    ├── setup.py\n    ├── script.py\n    └── .gitignore\nIn the Sphinx terminology, this doc folder is called the source directory. It contains:\n\nA configuration file conf.py with all the information needed to read the sources and build the doc. By building, it is meant the process of generating the doc (usually in html, pdf, etc.) from the ReST files.\nA directory structure containing .md or .rst files with the doc.\n\nTo help you, Sphinx comes with a script called sphinx-quickstart that sets up a source directory and creates a default conf.py with the most useful configuration values from a few questions it asks you. To use this, run:\n$ sphinx-quickstart\nAnswer each question asked. Be sure to say yes to the autodoc extension, as we will use this later. There is also an automatic API documentation (API: Application Programming Interface) generator called sphinx-apidoc; see sphinx-apidoc for details.\n\n\n\n\n\n\nEXERCISE: Setting up your documentation\n\n\n\nSet up the documentation for the biketrauma Python module.\n\nInstall the sphinx package with pip\nCreate a doc folder and cd into it\nLaunch sphinx-quickstart --sep.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#defining-documentation-structure",
    "href": "Courses/OPP/Docs.html#defining-documentation-structure",
    "title": "Markup languages / Documentation",
    "section": "Defining documentation structure",
    "text": "Defining documentation structure\nLet us assume you have run sphinx-quickstart. It has created a source directory with conf.py and a master document, index.rst (if you accepted the default parameters).\nThe main function of the master document is to serve as a welcome page and to contain the root of the “table of contents tree” (or toctree). This is one of the main things that Sphinx adds to reStructuredText, a way to connect multiple files to a single hierarchy of documents.\nThe toctree directive initially is empty and looks like so:\n.. toctree::\n   :maxdepth: 2\nYou add documents listing them in the content of the directive:\n.. toctree::\n   :maxdepth: 2\n\n   usage/installation\n   usage/quickstart\n   ...\nThis is exactly how the toctree for this documentation looks. The documents to include are given as document names, which in short means that you leave off the file name extension and use forward slashes (/) as directory separators.\n\n\n\n\n\n\nEXERCISE: installing a documentation\n\n\n\n\nUpdate the index.rst: by adding an image located here just below the title of the page\nInstall the read_the_doc theme following details given here. Additional themes are available on sphinx-doc themes.\nCreate the corresponding directory and files to add:\n\nAn Installation section with a few sentences and code snippets that explain how to install biketrauma\nA Documentation section with subsections io and visu each one containing a title and a few lines of text.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#building-the-doc",
    "href": "Courses/OPP/Docs.html#building-the-doc",
    "title": "Markup languages / Documentation",
    "section": "Building the doc",
    "text": "Building the doc\nDuring the configuration of Sphinx, a text file called MakeFile was created: In software development, Make is a build automation tool that automatically builds executable programs and libraries from source code by reading files called Makefiles which specify how to derive the target program.\nReferences:\n\nWikipedia on Make Software\n\n$ make html\nThen to access the web pages created:\n$ firefox _build/html/index.html\n\n\n\n\n\n\nNote\n\n\n\nThere is also a sphinx-build tool that can help you to build without make.\n\n\n\n\n\n\n\n\nEXERCISE: Makefiles\n\n\n\n\nList all the target defined in the Makefiles.\nBuild your doc and visualize it with a web navigator.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#api-doc-autodoc",
    "href": "Courses/OPP/Docs.html#api-doc-autodoc",
    "title": "Markup languages / Documentation",
    "section": "API doc (autodoc)",
    "text": "API doc (autodoc)\nWhen documenting Python code, it is common to put a lot of documentation in the source files, in documentation strings. Sphinx supports the inclusion of docstrings from your modules with an extension (an extension is a Python module that provides additional features for Sphinx projects) called autodoc.\nIn order to use autodoc, you need to activate it in conf.py by putting the string 'sphinx.ext.autodoc' into the list assigned to the extensions config value. Then, you have a few additional directives at your disposal.\nFor example, to document the function io.open(), reading its signature and docstring from the source file, you’d write this:\n.. autofunction:: io.open\nYou can also document whole classes or even modules automatically, using member options for the auto directives, like:\n.. automodule:: io\n   :members:\nautodoc needs to import your modules to extract the docstrings. Therefore, you must add the appropriate path to sys.path in your conf.py.\n\n\n\n\n\n\nEXERCISE: docstring\n\n\n\n\nWrite a docstring for the class biketrauma.io.Load_db and the function plot_location.\nIntegrate this documentation in a section called API in the Sphinx toctree.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#sphinx-gallery",
    "href": "Courses/OPP/Docs.html#sphinx-gallery",
    "title": "Markup languages / Documentation",
    "section": "Sphinx-Gallery",
    "text": "Sphinx-Gallery\nSphinx-Gallery is an extension able to create galleries of examples in the html documentation directly from the script files of your project.\nReferences: Sphinx Gallery\n\nConfiguration\nConfiguration and customization of sphinx-gallery are done primarily with a dictionary specified in your conf.py file. A typical sample is:\nfrom sphinx_gallery.sorting import FileNameSortKey\nsphinx_gallery_conf = {\n     # path to your examples scripts\n    'examples_dirs': ['../script',],\n     # path where to save gallery-generated examples\n    'gallery_dirs': ['_auto_scripts'],\n    # Order of the Gallery\n    'within_subsection_order': FileNameSortKey,\n}\nA list of the possible keys can be found on Sphinx Galleries.\n\n\n\n\n\n\nEXERCISE: Sphinx gallery\n\n\n\n\nInstall the sphinx-gallery extension with pip.\nUpdate the conf.py of the biketrauma package with the dictionary containing the configuration of the sphinx-gallery.\n\n\n\n\n\nStructure your example files\nSphinx-Gallery parses the folder listed in the key examples_dirs. It expects each Python file to have two things:\n\nA docstring, written in rST, that defines the header for the example. It must begin by defining a .rST title. The title may contain any punctuation mark but cannot start with the same punctuation mark repeated more than 3 times. For example:\n   \"\"\"\n   \"This\" is my example-script\n   ===========================\n\n   This example doesn't do much, it just makes a simple plot\n   \"\"\"\npython code. This can be any valid python code that you wish. Any matplotlib images that are generated will be saved to disk, and the ‘.rST’ generated will display these images with the built examples. By default, only images generated by matplotlib, or packages based on matplotlib (e.g., seaborn or yellowbrick) are saved and displayed. However, you can change this to include other packages, see for instance Image scrapers.\n\nWarning: With default options, Sphinx-Gallery only executes the script files with a filename starting with plot_.\nWarning: Sphinx-Gallery expects to find a README.txt (or README.rst) file in every folder containing examples.\n\n\nInclude examples in your toc-tree\nFor instance, you can add those lines in the index.rst:\n.. toctree::\n   :maxdepth: 2\n   :caption: Previsions:\n\n   _auto_scripts/index\nto add a section containing all the examples.\n\n\n\n\n\n\nEXERCISE: auto-build\n\n\n\n\nTransform the script.py examples into an auto-build example.",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/OPP/Docs.html#footnotes",
    "href": "Courses/OPP/Docs.html#footnotes",
    "title": "Markup languages / Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAaron Swartz: hacktivist (1986-2013) who also helped develop Creative Commons licenses, RSS, Reddit, etc. For more on his epic life, see Brian Knappenberger’ documentary The Internet’s Own Boy: The Story of Aaron Swartz (2014) ↩︎",
    "crumbs": [
      "OOP",
      "Markup languages / Documentation"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Venv.html",
    "href": "Courses/Scientific_Data_Libraries/Venv.html",
    "title": "Virtual Python Environment",
    "section": "",
    "text": "A venv is an isolated standalone python distribution with a specific version of modules. This is useful when one needs to run different python versions in a single system. Various commands can create a venv: venv, virtualenv, conda… We are going to use Anaconda to set up various python virtual environments on our system.\nReferences: - Virtualenv - Python documentation on venv",
    "crumbs": [
      "Generality",
      "Virtual Python Environment"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Venv.html#preamble",
    "href": "Courses/Scientific_Data_Libraries/Venv.html#preamble",
    "title": "Virtual Python Environment",
    "section": "",
    "text": "A venv is an isolated standalone python distribution with a specific version of modules. This is useful when one needs to run different python versions in a single system. Various commands can create a venv: venv, virtualenv, conda… We are going to use Anaconda to set up various python virtual environments on our system.\nReferences: - Virtualenv - Python documentation on venv",
    "crumbs": [
      "Generality",
      "Virtual Python Environment"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Venv.html#python-package-system",
    "href": "Courses/Scientific_Data_Libraries/Venv.html#python-package-system",
    "title": "Virtual Python Environment",
    "section": "Python package system",
    "text": "Python package system\nThe Python Package Index (PyPI) is a repository of software for the python programming language. PyPI helps you find and install software developed and shared by the python community.\nThe pip program allows you to install most standard packages:\n$ pip --version\n$ pip install numpy\nAlso it is possible to install a package from a file:\n$ pip install -r requirements.txt\nwhere requirements.txt is a file containing the list of packages to install.\nIn particular such a file can be generated by the following command:\n$ pip freeze &gt; requirements.txt\nor to output only the packages installed into your virtual env:\n$ pip freeze --all &gt; requirements.txt\nAn altarnative could be using the pipreqs package:\n$ pipreqs /path/to/project\nReferences: - Pypi - Installing Packages\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nDetermine which python version there is on your system using locate and which\nDetermine which version of python is used by the pip command\nList all the python modules installed with the pip command",
    "crumbs": [
      "Generality",
      "Virtual Python Environment"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Venv.html#anaconda",
    "href": "Courses/Scientific_Data_Libraries/Venv.html#anaconda",
    "title": "Virtual Python Environment",
    "section": "Anaconda",
    "text": "Anaconda\nAnaconda is a package manager, an environment manager coming with a python/R data science distribution, and a large collection of open-source packages. It is cross-platform and is a very popular choice in the data scientist community. Nevertheless, it suffers from a main drawback: it is heavy. Moreover, it comes with its own package manager conda which allows you to install a python module (like pip) and other programs.\nOn the Linux box provided by the FdS, there is a terminal with the $PATH environment variable already configured (/net/apps/bin/init_anaconda3). You may launch it via the Graphical User Interface.\nYou can see also the mamba project https://github.com/mamba-org/mamba.\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nDisplay the $PATH variable in the Anaconda_init terminal\nType conda deactivate and (re)-display the $PATH variable\n\n\n\n\nCreating an environment\nUse the terminal or an Anaconda Prompt for the following steps:\n\nTo create an environment:\n$ conda create --name myenv\nReplace myenv with the environment name.\nWhen conda asks you to proceed, type y:\n  proceed ([y]/n)?\nBy default, environments are installed into the envs sub-directory in your conda directory. See conda create --help for information on specifying a different path. This creates the myenv environment in envs/. This environment uses the same version of python that you are currently using because you did not specify a version.\nTo create an environment with a specific version of python:\n$ conda create -n myenv python=3.9\nTo create an environment with a specific package:\n$ conda create -n myenv scipy\nor:\n$ conda create -n myenv python\n$ conda install -n myenv scipy\nTo create an environment with a specific version of a package:\n$ conda create -n myenv scipy=0.15.0\nor\n$ conda create -n myenv python\n$ conda install -n myenv scipy=0.15.0\nTo create an environment with a specific version of python and multiple packages:\n$ conda create -n myenv python=3.6 scipy=0.15.0 astroid babel\n\nSee: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nCreate a new environment called toto with python3.5 and pandas version 0.23\nCreate another environment called tata with python3.7 and pandas version 1.0\n\n\n\n\n\nSwitch environment\nTo switch to an environment, it must be “activated” (in git we would have said “to checkout”). Activation entails two primary functions: adding entries to PATH for the environment and running any activation scripts that the environment may contain. These activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. You can also use the config API to set environment variables. To activate an environment:\n$ conda activate myenv\nChange myenv with the name of your environment.\nSee: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nActivate the toto environment. Launch python and check the version of pandas\nActivate the tata environment. Launch python and check the version of pandas\nList all the available environments (look in the documentation by yourself)\nCome back to the base environment\n\n\n\n\n\nSave and export an environment\nReferences: Building identical conda environments\n\n\n\n\n\n\nEXERCISE:\n\n\n\nImagine that you are coding a python module and some users are not able to run your code due to some missing dependencies. How can you help them to set up the python venv?\n\n\n\n\nRemoving an environment and cleaning\nAnaconda is particularly greedy in terms of disk usage. It can be a good practice to remove an unused environment\n$ conda env remove -n myenv\nTo remove all cache and package run\n$ conda clean --all\n\n\n\n\n\n\nEXERCISE:\n\n\n\n\nRemove all the environments created during this session\nCreate an environment called hax712_env with matplotlib (this venv will be used in the next courses)\nClean the conda caches to free disk space.",
    "crumbs": [
      "Generality",
      "Virtual Python Environment"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html",
    "title": "SciPy",
    "section": "",
    "text": "Disclaimer: this course is adapted from the notebooks by",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#introduction",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#introduction",
    "title": "SciPy",
    "section": "Introduction",
    "text": "Introduction\nSciPy is a scientific library that builds upon NumPy. Among others, SciPy deals with:\n\nIntegration (scipy.integrate)\nOptimization (scipy.optimize)\nInterpolation (scipy.interpolate)\nFourier Transform (scipy.fftpack)\nSignal Processing (scipy.signal)\nLinear Algebra (scipy.linalg)\nSparse matrices (scipy.sparse)\nStatistics (scipy.stats)\nImage processing (scipy.ndimage)\nIO (input/output) (scipy.io)\n\n\n%matplotlib inline\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nReferences:\n\nMatplotlib Animations / JavaScript Widgets by Louis Tiao",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#linear-algebra",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#linear-algebra",
    "title": "SciPy",
    "section": "Linear algebra",
    "text": "Linear algebra\nscipy for linear algebra : use linalg. It includes functions for solving linear systems, eigenvalues decomposition, SVD, Gaussian elimination (LU, Cholesky), etc.\nReferences:\n\nScipy documentation\n\n\nSolving linear systems:\nFind \\(x\\) such that: \\(A x = b\\) for specified matrix \\(A\\) and vector \\(b\\).\n\nA = np.array([[1, 0, 3], [4, 5, 12], [7, 8, 9]], dtype=float)\nb = np.array([[1, 2, 3]], dtype=np.float64).T\n\nprint(A, b)\n\nx = linalg.solve(A, b)\nprint(x, x.shape, b.shape)\n\n[[ 1.  0.  3.]\n [ 4.  5. 12.]\n [ 7.  8.  9.]] [[1.]\n [2.]\n [3.]]\n[[ 0.8       ]\n [-0.4       ]\n [ 0.06666667]] (3, 1) (3, 1)\n\n\nCheck the result at a given precision (different from ==)\n\nnp.allclose(A @ x, b, atol=1e-14, rtol=1e-15)\n\nTrue\n\n\nRemark: NEVER (or you should really know why) invert a matrix. ALWAYS solve linear systems instead!\n\nEigenvalues/ Eigenvectors\n\\(A v_n = \\lambda_n v_n\\) with \\(v_n\\) the \\(n\\)-th eigen vector and \\(\\lambda_n\\) the \\(n\\)-th eigen value. The associated python functions are eigvals and eig:\n\nA = np.random.randn(3, 3)\nA = A + A.T\nevals, evecs = linalg.eig(A)\nprint(evals, \"\\n ------\\n\", evecs)\n\nnp.allclose(A, evecs @ np.diag(evals) @ evecs.T)\n\n[-2.6946067 +0.j  3.59976358+0.j  0.99797869+0.j] \n ------\n [[ 0.48627146  0.73935221 -0.4657235 ]\n [ 0.35180651 -0.65352962 -0.67017253]\n [ 0.79985764 -0.16204121  0.57790172]]\n\n\nTrue\n\n\n\n\n\n\n\n\nEXERCISE: Eigenvalues/Eigenvectors\n\n\n\nVerify numerically that the outputs from linalg.eig are indeed approximately eigenvalues and eigenvectors of matrix A above.\nHint: use Scipy documentation on allclose\n\n\n\n\nSymmetric matrices\nIf A is symmetric you should use eigvalsh (H for Hermitian) instead: This is more robust and leverages the structures (you know they are real!)\n\n\n\nMatrix operations\n\nlinalg.trace(A) # trace\nlinalg.det(A) # determinant\nlinalg.inv(A) # Inverse, consider NEVER using it though :)\n\n\n\nNorms\n\nprint(linalg.norm(A, ord=\"fro\"))  # fro for Frobenius\nprint((np.sum(A ** 2)) ** 0.5)\nprint(linalg.norm(A, ord=2))\nprint((linalg.eigvalsh(A.T @ A) ** 0.5))\nprint(linalg.norm(A, ord=np.inf))\n\n4.605992247720379\n4.605992247720379\n3.5997635840966864\n[0.99797869 2.6946067  3.59976358]\n5.183862068176654\n\n\n\n\n\n\n\n\nEXERCISE: Norms computation\n\n\n\nCheck numerically what the instruction linalg.norm(A, ord=np.inf) is computing. Double check with the help and a numerical test.\n\n\n\nA = np.random.randn(3, 3)\nprint(linalg.norm(A, ord=np.inf))\n\n4.861906741828836",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#random-generation-distributions-etc.",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#random-generation-distributions-etc.",
    "title": "SciPy",
    "section": "Random generation, distributions, etc.",
    "text": "Random generation, distributions, etc.\nReferences:\n\nGood practices with numpy random number generators by Albert Thomas\nNumpy documentation on RandomState\nRandom Widgets, by Joseph Salmon: Visualization of various popular distributions.\n\n\nseed = 12345\nrng = np.random.default_rng(seed)  # can be called without a seed\nrng.random()\n\n0.22733602246716966",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#optimization",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#optimization",
    "title": "SciPy",
    "section": "Optimization",
    "text": "Optimization\nGoal: find functions minima or maxima\nReferences:\n\nScipy Lectures on mathematical optimization.\n\n\nfrom scipy import optimize\n\n\nFinding (local!) minima\ndef f(x):\n    return 4 * x ** 3 + (x - 2) ** 2 + x ** 4\n\n\ndef mf(x):\n    return -(4 * x ** 3 + (x - 2) ** 2 + x ** 4)\n\n\nxs = np.linspace(-5, 3, 100)\nplt.figure()\nplt.plot(xs, f(xs))\nplt.show()\n\n\n\n\n\n\n\nDefault solver for minimization/maximization: fmin_bfgs (see Wikipedia on BFGS)\nx_min = optimize.fmin_bfgs(f, x0=-4)\nx_max = optimize.fmin_bfgs(mf, x0=-2)\nx_min2 = optimize.fmin_bfgs(f, x0=2)\n\n\nplt.figure()\nplt.plot(xs, f(xs))\nplt.plot(x_min, f(x_min), \"o\", markersize=10, color=\"orange\")\nplt.plot(x_min2, f(x_min2), \"o\", markersize=10, color=\"red\")\nplt.plot(x_max, f(x_max), \"|\", markersize=20)\nplt.show()\n\n\n\nOptimization terminated successfully.\n         Current function value: -3.506641\n         Iterations: 7\n         Function evaluations: 16\n         Gradient evaluations: 8\nOptimization terminated successfully.\n         Current function value: -6.201654\n         Iterations: 5\n         Function evaluations: 12\n         Gradient evaluations: 6\nOptimization terminated successfully.\n         Current function value: 2.804988\n         Iterations: 7\n         Function evaluations: 16\n         Gradient evaluations: 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE: Basin of attraction\n\n\n\nDraw the points on the curves with two different colors :\n\norange: for the points leading to find the left local minima\nred: for the points leading to the right local minima.\n\n\n\n\n\nFind the zeros of a function\nFind \\(x\\) such that \\(f(x) = 0\\), with fsolve.\nomega_c = 3.0\n\ndef f(omega):\n    return np.tan(2 * np.pi * omega) - omega_c / omega\n\n\nx = np.linspace(1e-8, 3.2, 1000)\ny = f(x)\n\n# Remove vertical lines when the function flips signs\nmask = np.where(np.abs(y) &gt; 50)\nx[mask] = y[mask] = np.nan\nplt.plot(x, y)\nplt.plot([0, 3.3], [0, 0], \"k\")\nplt.ylim(-5, 5)\n\noptimize.fsolve(f, 0.72)\noptimize.fsolve(f, 1.1)\noptimize.fsolve(f, np.linspace(0.001, 3, 20))\nnp.unique(np.round(optimize.fsolve(f, np.linspace(0.2, 3, 20)), 3))\n\nmy_zeros = (\n    np.unique((optimize.fsolve(f, np.linspace(0.2, 3, 20)) * 1000).astype(int)) / 1000.0\n)\nplt.figure()\nplt.plot(x, y, label=\"$f$\")\nplt.plot([0, 3.3], [0, 0], \"k\")\nplt.plot(my_zeros, np.zeros(my_zeros.shape), \"o\", label=\"$x : f(x)=0$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters estimation\nfrom scipy.optimize import curve_fit\n\n\ndef f(x, a, b, c):\n    \"\"\"f(x) = a exp(-bx) + c.\"\"\"\n    return a * np.exp(-b * x) + c\n\n\nx = np.linspace(0, 4, 50)\ny = f(x, 2.5, 1.3, 0.5)  # true signal\nyn = y + 0.2 * np.random.randn(len(x))  # noisy added\n\nplt.figure()\nplt.plot(x, yn, \".\")\nplt.plot(x, y, \"k\", label=\"$f$\")\nplt.legend()\nplt.show()\n\n(a, b, c), _ = curve_fit(f, x, yn)\nprint(a,\"\\n\", b,\"\\n\", c)\n\n\n\n\n\n\n\n\n2.3831560530490443 \n 1.2447044111099794 \n 0.5742731679926126\n\n\n\nDisplaying\nplt.figure()\nplt.plot(x, yn, \".\", label=\"data\")\nplt.plot(x, y, \"k\", label=\"True $f$\")\nplt.plot(x, f(x, a, b, c), \"--k\", label=\"Estimated $\\hat{f}$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFor polynomial fitting, one can directly use numpy functionsL\nx = np.linspace(0, 1, 10)\ny = np.sin(x * np.pi / 2.0)\nline = np.polyfit(x, y, deg=10)\nplt.figure()\nplt.plot(x, y, \".\", label=\"data\")\nplt.plot(x, np.polyval(line, x), \"k--\", label=\"polynomial approximation\")\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_17657/4158621946.py:3: RankWarning:\n\nPolyfit may be poorly conditioned",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#interpolation",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#interpolation",
    "title": "SciPy",
    "section": "Interpolation",
    "text": "Interpolation\nfrom scipy.interpolate import interp1d, CubicSpline\n\n\ndef f(x):\n    return np.sin(x)\n\n\nn = np.arange(0, 10)\nx = np.linspace(0, 9, 100)\n\ny_meas = f(n) + 0.1 * np.random.randn(len(n))  # add noise\ny_real = f(x)\n\nlinear_interpolation = interp1d(n, y_meas)\ny_interp1 = linear_interpolation(x)\n\ncubic_interpolation = CubicSpline(n, y_meas)\ny_interp2 = cubic_interpolation(x)\n\n\nplt.figure()\nplt.plot(n, y_meas, \"bs\", label=\"noisy data\")\nplt.plot(x, y_real, \"k\", lw=2, label=\"true function\")\nplt.plot(x, y_interp1, \"r\", label=\"linear interp\")\nplt.plot(x, y_interp2, \"g\", label=\"CubicSpline interp\")\nplt.legend(loc=3)\nplt.show()",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#images",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#images",
    "title": "SciPy",
    "section": "Images",
    "text": "Images\n\nRGB decomposition\nFirst, discuss the color decomposition in RGB. The RGB color model is an additive color model[1] in which the red, green and blue primary colors of light are added together in various ways to reproduce a broad array of colors. Hence, each channel (R, G or B) represents a grayscale image, usually coded on [0,1] or [0,255].\nfrom scipy import ndimage, datasets\n\nimg = datasets.face()\nprint(type(img), img.dtype, img.ndim, img.shape)\n\nprint(2 ** 8)  # uint8-&gt; code sur 256 niveau.\n\nn_1, n_2, n_3 = img.shape\nprint(n_1, n_2, n_3)\n\n# True image\nplt.figure()\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n\n\n\n&lt;class 'numpy.ndarray'&gt; uint8 3 (768, 1024, 3)\n256\n768 1024 3\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(3, 2)\nfig.set_size_inches(7, 4.5)\nn_1, n_2, n_3 = img.shape\n\n# add subplot titles\nax[0, 0].set_title(\"Red channel\")\nax[0, 0].imshow(img[:, :, 0], cmap=plt.cm.Reds)\nax[0, 1].set_title(\"Pixel values histogram (red channel)\")\nax[0, 1].hist(img[:, :, 0].reshape(n_1 * n_2), np.arange(0, 256))\n\nax[1, 0].set_title(\"Green channel\")\nax[1, 0].imshow(img[:, :, 1], cmap=plt.cm.Greens)\nax[1, 1].set_title(\"Pixel values histogram (green channel)\")\nax[1, 1].hist(img[:, :, 1].reshape(n_1 * n_2), np.arange(0, 256))\n\nax[2, 0].set_title(\"Blue channel\")\nax[2, 0].imshow(img[:, :, 2], cmap=plt.cm.Blues)\nax[2, 1].set_title(\"Pixel values histogram (blue channel)\")\nax[2, 1].hist(img[:, :, 2].reshape(n_1 * n_2), np.arange(0, 256))\n\nplt.tight_layout()\n\n\n\n\n\n\n\nprint(img.flags)  # cannot edit...\nimg_compressed = img.copy()\nimg_compressed.setflags(write=1)\nprint(img_compressed.flags)  # can edit now\n\n\nimg_compressed[img_compressed &lt; 70] = 50\nimg_compressed[(img_compressed &gt;= 70) & (img_compressed &lt; 110)] = 100\nimg_compressed[(img_compressed &gt;= 110) & (img_compressed &lt; 180)] = 150\nimg_compressed[(img_compressed &gt;= 180)] = 200\nplt.figure()\nplt.imshow(img_compressed, cmap=plt.cm.gray)\nplt.axis(\"off\")\nplt.show()\n\n\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : False\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n\n\n\n\n\n\n\n\n\n\nConvert a color image in grayscale\nplt.figure()\nplt.imshow(np.mean(img, axis=2), cmap=plt.cm.gray)\nplt.show()\n\n\n\n\n\n\n\n\n\nChanging colors in an image\nimport pooch\nimport requests\nimport os\n\nurl = \"https://upload.wikimedia.org/wikipedia/en/thumb/0/05/Flag_of_Brazil.svg/486px-Flag_of_Brazil.svg.png\"\n\nname_img = pooch.retrieve(url, known_hash=\"f0e55d6c9384907aa0629e66dd2133473b9d7467af0f817670e358bded62487f\")\n\nimg = (255 * plt.imread(name_img)).astype(int)\nimg = img.copy()\nplt.figure()\nplt.imshow(img)\n\nfig, ax = plt.subplots(3, 2)\nfig.set_size_inches(7, 4.5)\nn_1, n_2, n_3 = img.shape\n\nax[0, 0].imshow(img[:, :, 0], cmap=plt.cm.Reds)\nax[0, 0].set_title(\"Red channel\")\nax[0, 1].hist(img[:, :, 0].reshape(n_1 * n_2), np.arange(0, 256), density=True)\nax[0, 1].set_title(\"Pixel values histogram (red channel)\")\n\nax[1, 0].imshow(img[:, :, 1], cmap=plt.cm.Greens)\nax[1, 0].set_title(\"Green channel\")\nax[1, 1].hist(img[:, :, 1].reshape(n_1 * n_2), np.arange(0, 256), density=True)\nax[1, 1].set_title(\"Pixel values histogram (green channel)\")\n\nax[2, 0].imshow(img[:, :, 2], cmap=plt.cm.Blues)\nax[2, 0].set_title(\"Blue channel\")\nax[2, 1].hist(img[:, :, 2].reshape(n_1 * n_2), np.arange(0, 256), density=True)\nax[2, 1].set_title(\"Pixel values histogram (Blue channel)\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE: Make the Brazilian italianer\n\n\n\nCreate a version of the Brazilian flag as follows:\n\n\n\n\n\n\n\n\n\n\n\nRGBA\nRGBA stands for red (R), green (G), blue (B) and alpha (A). Alpha indicates how the transparency allows an image to be combined over others using alpha compositing, with transparent areas.\n\n\nHexadecimal decomposition\nOften, colors are represented not with an RGB triplet, say (255, 0, 0), but with a hexadecimal code (say #FF0000). To get a hexadecimal decomposition, transform each 8-bit RGB channel (i.e., \\(2^8=256\\)) into a 2-digit hexadecimal number (i.e., \\(16^2=256\\)). This requires letters for representing \\(10: A, 11: B,\\dots, 15: FF\\) (see https://www.rgbtohex.net/ for an online converter)\n\n\nCMYK decomposition\nThis is rather a subtractive color model, where the primary colors are cyan (C), magenta (M), yellow (Y), and black (B). For a good source to go from RGB to CMYK (and back), see https://fr.wikipedia.org/wiki/Quadrichromie.\n\n\nHSL (hue, saturation, lightness)\nXXX TODO.\nHere, you can find a simple online converter for all popular color models: https://www.myfixguide.com/color-converter/.",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/Scipy.html#image-files-formats",
    "href": "Courses/Scientific_Data_Libraries/Scipy.html#image-files-formats",
    "title": "SciPy",
    "section": "Image files formats",
    "text": "Image files formats\nBitmap formats: - PNG (raw, uncompressed format, opens with Gimp) - JPG (compressed format) - GIF (compressed, animated format)\nVector formats: - PDF (recommended for your documents) - SVG (easily modifiable with Inkscape) - EPS - etc.\nx1 = np.linspace(0.0, 5.0, num=50)\nx2 = np.linspace(0.0, 2.0, num=50)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\ny2 = np.cos(2 * np.pi * x2)\n\nfig1 = plt.figure(figsize=(5, 4))\nplt.plot(x1, y1)\nplt.xlim(0, 6)\nplt.ylim(-1, 1)\n\n\n\n\n\n\n\nThen, we can save the figure in various formats:\n\nfig1.savefig(\"ma_figure_pas_belle.png\", format='png', dpi=90)\nfig1.savefig(\"ma_figure_plus_belle.svg\",format='svg', dpi=90)\n\nNow that the images have been saved, we can visualize the difference between the PNG and SVG formats.\nPNG (zoom on hover):\n\n    \n\nSVG (zoom on hover):\n\n    \n\n\n\n\n\n\n\nNote\n\n\n\nSome additional effects to produce the above zoom-on-hover effect can be found here: https://www.notuxedo.com/effet-de-zoom-image-css/\n\n\nReferences:\n\nIntroduction to geopandas\nCourse on images/slides by Joseph Salmon\nOfficial SciPy web page\nSciPy User Guide\nScipy lectures\nThe SciPy source code",
    "crumbs": [
      "Scientific Libraries",
      "SciPy"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html",
    "title": "Titanic dataset",
    "section": "",
    "text": "Disclaimer: this course is adapted from the work Pandas tutorial by Joris Van den Bossche. R users might also want to read Pandas: Comparison with R / R libraries for a smooth start in Pandas.\nWe start by importing the necessary libraries:\n%matplotlib inline\nimport os\nimport numpy as np\nimport calendar\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nimport pooch  # download data / avoid re-downloading\nfrom IPython import get_ipython\n\n\nsns.set_palette(\"colorblind\")\npalette = sns.color_palette(\"twilight\", n_colors=12)\npd.options.display.max_rows = 8",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-preparation",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-preparation",
    "title": "Titanic dataset",
    "section": "Data preparation",
    "text": "Data preparation\n\nAutomatic data download\nFirst, it is important to download automatically remote files for reproducibility (and avoid typing names manually). Let us apply this to the Titanic dataset:\n\n'''\n# works locally but not on the website\nurl = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/\"\npath_target = \"./titanic.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)  # if needed `pip install pooch`\n'''\n\n'\\n# works locally but not on the website\\nurl = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/\"\\npath_target = \"./titanic.csv\"\\npath, fname = os.path.split(path_target)\\npooch.retrieve(url, path=path, fname=fname, known_hash=None)  # if needed `pip install pooch`\\n'\n\n\nReading the file as a pandas dataframe with read_csv:\n\ndf_titanic_raw = pd.read_csv(\"titanic.csv\")\n\nVisualize the end of the dataset:\n\ndf_titanic_raw.tail(n=3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.45\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.00\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.75\nNaN\nQ\n\n\n\n\n\n\n\nVisualize the beginning of the dataset:\n\ndf_titanic_raw.head(n=5)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\nMissing values\nIt is common to encounter features/covariates with missing values. In pandas they were mostly handled as np.nan (not a number) in the past. With versions &gt;=1.0 missing values are now treated as NA (Not Available), in a similar way as in R; see the Pandas documentation on missing data for standard behavior and details.\nNote that the main difference between pd.NA and np.nan is that pd.NA propagates even for comparisons:\n\npd.NA == 1\n\n&lt;NA&gt;\n\n\nwhereas\n\nnp.nan == 1\n\nFalse\n\n\nTesting the presence of missing values\n\nprint(pd.isna(pd.NA), pd.isna(np.nan))\n\nTrue True\n\n\nThe simplest strategy (when you can / when you have enough samples) consists of removing all nans/NAs, and can be performed with dropna:\n\ndf_titanic = df_titanic_raw.dropna()\ndf_titanic.tail(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n\n\n\nYou can see for instance the passenger with PassengerId 888 has been removed by the dropna call.\n\n# Useful info on the dataset (especially missing values!)\ndf_titanic.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 183 entries, 1 to 889\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  183 non-null    int64  \n 1   Survived     183 non-null    int64  \n 2   Pclass       183 non-null    int64  \n 3   Name         183 non-null    object \n 4   Sex          183 non-null    object \n 5   Age          183 non-null    float64\n 6   SibSp        183 non-null    int64  \n 7   Parch        183 non-null    int64  \n 8   Ticket       183 non-null    object \n 9   Fare         183 non-null    float64\n 10  Cabin        183 non-null    object \n 11  Embarked     183 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 18.6+ KB\n\n\n\n# Check that the `Cabin` information is mostly missing; the same holds for `Age`\ndf_titanic_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nOther strategies are possible to handle missing values but they are out of scope for this preliminary course on pandas, see the documentation for more details.",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-description",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-description",
    "title": "Titanic dataset",
    "section": "Data description",
    "text": "Data description\nDetails of the dataset are given here\n\nSurvived: Survival 0 = No, 1 = Yes\nPclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\nSex: Sex male/female\nAge: Age in years\nSibsp: # of siblings/spouses aboard the Titanic\nParch: # of parents/children aboard the Titanic\nTicket: Ticket number\nFare: Passenger fare\nCabin: Cabin number\nEmbarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\nName: Name of the passenger\nPassengerId: Number to identify passenger\n\n\n\n\n\n\n\nNote\n\n\n\nFor those interested, an extended version of the dataset is available here https://biostat.app.vumc.org/wiki/Main/DataSets.\n\n\nSimple descriptive statistics can be obtained using the describe method:\n\ndf_titanic.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n183.000000\n183.000000\n183.000000\n183.000000\n183.000000\n183.000000\n183.000000\n\n\nmean\n455.366120\n0.672131\n1.191257\n35.674426\n0.464481\n0.475410\n78.682469\n\n\nstd\n247.052476\n0.470725\n0.515187\n15.643866\n0.644159\n0.754617\n76.347843\n\n\nmin\n2.000000\n0.000000\n1.000000\n0.920000\n0.000000\n0.000000\n0.000000\n\n\n25%\n263.500000\n0.000000\n1.000000\n24.000000\n0.000000\n0.000000\n29.700000\n\n\n50%\n457.000000\n1.000000\n1.000000\n36.000000\n0.000000\n0.000000\n57.000000\n\n\n75%\n676.000000\n1.000000\n1.000000\n47.500000\n1.000000\n1.000000\n90.000000\n\n\nmax\n890.000000\n1.000000\n3.000000\n80.000000\n3.000000\n4.000000\n512.329200\n\n\n\n\n\n\n\n\nVisualization\n\nHistograms: usually has one parameter controlling the number of bins (please avoid… often useless, when many samples are available)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 3))\nax.hist(df_titanic['Age'], density=True, bins=25)\nplt.xlabel('Age')\nplt.ylabel('Proportion')\nplt.title(\"Passager age histogram\")\nplt.show()\n\n\n\n\n\n\n\n\n\nKernel Density Estimate (KDE): usually has one parameter (the bandwidth) controlling the smoothing level.\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 3))\nsns.kdeplot(\n    df_titanic[\"Age\"], ax=ax, fill=True, cut=0, bw_adjust=0.1\n)\nplt.xlabel(\"Proportion\")\nplt.ylabel(\"Age\")\nplt.title(\"Passager age kernel density estimate\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bandwidth parameter (here encoded as bw_adjust) controls the smoothing level. It is a common parameter in kernel smoothing, investigated thoroughly in the non-parametric statistics literature.\n\n\n\nSwarmplot:\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 5))\nsns.swarmplot(\n    data=df_titanic_raw,\n    ax=ax,\n    x=\"Sex\",\n    y=\"Age\",\n    hue=\"Survived\",\n    palette={0: \"r\", 1: \"k\"},\n    order=[\"female\", \"male\"],\n)\nplt.title(\"Passager age by gender/survival\")\nplt.legend(labels=[\"Survived\", \"Died\"], loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE density over histogram\n\n\n\nPlot the density estimate over the histogram\n\n\n\n\nInteractivity\nInteractive interaction with codes and output is nowadays getting easier and easier (see also Shiny app in R-software). In Python, one can use plotly, or widgets and the interact package for this purpose. We are going to visualize that on the simple KDE and histogram examples.\n\nimport plotly.graph_objects as go\nimport numpy as np\nfrom scipy import stats\n\n# Create figure\nfig = go.FigureWidget()\n\n# Add traces, one for each slider step\nbws = np.arange(0.01, 0.51, 0.01)\nxx = np.arange(0, 100, 0.5)\nfor step in bws:\n    kernel = stats.gaussian_kde(df_titanic[\"Age\"], bw_method=step)\n    yy = kernel(xx)\n    fig.add_trace(\n        go.Scatter(\n            visible=False,\n            fill=\"tozeroy\",\n            fillcolor=\"rgba(67, 67, 67, 0.5)\",\n            line=dict(color=\"black\", width=2),\n            name=f\"Bw = {step:.2f}\",\n            x=xx,\n            y=yy,\n        )\n    )\n\n# Make 10th trace visible\nfig.data[10].visible = True\n\n# Create and add slider\nsteps = []\nfor i in range(len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[{\"visible\": [False] * len(fig.data)}],\n        label=f\"{bws[i]:.2f}\",\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\nsliders = [\n    dict(\n        active=10,\n        currentvalue={\"prefix\": \"Bandwidth = \"},\n        pad={\"t\": 50},\n        steps=steps,\n    )\n]\n\nfig.update_layout(\n    sliders=sliders,\n    template=\"simple_white\",\n    title=f\"Impact of the bandwidth on the KDE\",\n    showlegend=False,\n    xaxis_title=\"Age\"\n)\n\nfig.show()\n\n\n\n\n\nimport plotly.graph_objects as go\nimport numpy as np\nfrom scipy import stats\n\n# Create figure\nfig = go.FigureWidget()\n\n# Add traces, one for each slider step\nbws = np.arange(0.01, 5.51, 0.3)\nxx = np.arange(0, 100, 0.5)\n\nfor step in bws:\n    fig.add_trace(\n        go.Histogram(\n            x=df_titanic[\"Age\"],\n            xbins=dict(  # bins used for histogram\n                start=xx.min(),\n                end=xx.max(),\n                size=step,\n            ),\n            histnorm=\"probability density\",\n            autobinx=False,\n            visible=False,\n            marker_color=\"rgba(67, 67, 67, 0.5)\",\n            name=f\"Bandwidth = {step:.2f}\",\n        )\n    )\n\n# Make 10th trace visible\nfig.data[10].visible = True\n# Create and add slider\nsteps = []\nfor i in range(len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[{\"visible\": [False] * len(fig.data)}],\n        label=f\"{bws[i]:.2f}\",\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\nsliders = [\n    dict(\n        active=10,\n        currentvalue={\"prefix\": \"Bandwidth = \"},\n        pad={\"t\": 50},\n        steps=steps,\n    )\n]\n\nfig.update_layout(\n    sliders=sliders,\n    template=\"simple_white\",\n    title=f\"Impact of the bandwidth on histograms\",\n    showlegend=False,\n    xaxis_title=\"Age\"\n)\n\nfig.show()\n\n\n\n\n\nIPython widgets (in Jupyter notebooks)\nimport ipywidgets as widgets\nfrom ipywidgets import interact, fixed\n\ndef hist_explore(\n    dataset=df_titanic,\n    variable=df_titanic.columns,\n    n_bins=24,\n    alpha=0.25,\n    density=False,\n):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    ax.hist(dataset[variable], density=density, bins=n_bins, alpha=alpha)\n    plt.ylabel(\"Density level\")\n    plt.title(f\"Dataset {dataset.attrs['name']}:\\n Histogram for passengers' age\")\n    plt.tight_layout()\n    plt.show()\n\n\ninteract(\n    hist_explore,\n    dataset=fixed(df_titanic),\n    n_bins=(1, 50, 1),\n    alpha=(0, 1, 0.1),\n    density=False,\n)\ndef kde_explore(dataset=df_titanic, variable=df_titanic.columns, bw=5):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    sns.kdeplot(dataset[variable], bw_adjust=bw, fill=True, cut=0, ax=ax)\n    plt.ylabel(\"Density level\")\n    plt.title(f\"Dataset:\\n KDE for passengers'  {variable}\")\n    plt.tight_layout()\n    plt.show()\n\ninteract(kde_explore, dataset=fixed(df_titanic), bw=(0.001, 2, 0.01))",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-wrangling",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-wrangling",
    "title": "Titanic dataset",
    "section": "Data wrangling",
    "text": "Data wrangling\n\ngroupby function\nHere is an example of the using the groupby function:\n\ndf_titanic.groupby(['Sex'])[['Survived', 'Age', 'Fare', 'Pclass']].mean()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nPclass\n\n\nSex\n\n\n\n\n\n\n\n\nfemale\n0.931818\n32.676136\n89.000900\n1.215909\n\n\nmale\n0.431579\n38.451789\n69.124343\n1.168421\n\n\n\n\n\n\n\nYou can answer other similar questions, such as “how does the survival rate change w.r.t. to sex”?\n\ndf_titanic_raw.groupby('Sex')[['Survived']].aggregate(lambda x: x.mean())\n\n\n\n\n\n\n\n\nSurvived\n\n\nSex\n\n\n\n\n\nfemale\n0.742038\n\n\nmale\n0.188908\n\n\n\n\n\n\n\nHow does the survival rate change w.r.t. the class?\n\ndf_titanic.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\ndf_titanic.groupby('Pclass')['Survived'].aggregate(lambda x:\n                                                   x.mean()).plot(ax=ax,kind='bar')\nplt.xlabel('Classes')\nplt.ylabel('Survival rate')\nplt.title('Survival rate per classs')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE median by class\n\n\n\nPerform a similar analysis with the median for the price per class in pounds.\n\n\n\n\ncatplot: a visual groupby\n\nax=sns.catplot(\n    x=\"Pclass\",\n    y=\"Age\",\n    hue=\"Sex\",\n    palette={'female': 'red', 'male': 'b'},\n    data=df_titanic_raw,\n    jitter = '0.2',\n    s=8,\n)\nplt.title(\"Sex repartition per class on the Titanic\")\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.8, 0.8))\nplt.show()\n\n\n\n\n\n\n\n\n\nax=sns.catplot(\n    x=\"Pclass\",\n    y=\"Age\",\n    hue=\"Sex\",\n    palette={'female': 'red', 'male': 'b'},\n    alpha=0.8,\n    data=df_titanic_raw,\n    kind='swarm',\n    s=11,\n)\nplt.title(\"Sex repartition per class on the Titanic\")\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.8, 0.8))\nplt.show()\n\n\n\n\n\n\n\n\n\nax=sns.catplot(\n    x=\"Sex\",\n    y=\"Age\",\n    hue=\"Sex\",\n    palette={'female': 'red', 'male': 'b'},\n    col='Pclass',\n    alpha=0.8,\n    data=df_titanic_raw,\n    kind='swarm',\n    s=6,\n    height=5,\n    aspect=0.49,\n)\nplt.suptitle(\"Sex repartition per class on Titanic\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nax=sns.catplot(x='Pclass',\n            y='Age',\n            hue=\"Sex\",\n            palette={'female': 'red', 'male': 'b'},\n            data=df_titanic_raw,\n            kind=\"violin\",\n            alpha=0.8,\n)\nplt.title(\"Sex repartition per class on the Titanic\")\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.8, 0.8))\nplt.show()\n\n\n\n\n\n\n\n\nBeware: there is a large difference in sex ratio by class. You can use groups to get the counts illustrating this:\n\nRaw sex repartition per class:\n\ndf_titanic_raw.groupby(['Sex', 'Pclass'])[['Sex']].count()\n\n\n\n\n\n\n\n\n\nSex\n\n\nSex\nPclass\n\n\n\n\n\nfemale\n1\n94\n\n\n2\n76\n\n\n3\n144\n\n\nmale\n1\n122\n\n\n2\n108\n\n\n3\n347\n\n\n\n\n\n\n\n\n\nRaw sex ratio:\n\ndf_titanic.groupby(['Sex', 'Pclass'])[['Sex']].count()\n\n\n\n\n\n\n\n\n\nSex\n\n\nSex\nPclass\n\n\n\n\n\nfemale\n1\n74\n\n\n2\n9\n\n\n3\n5\n\n\nmale\n1\n84\n\n\n2\n6\n\n\n3\n5\n\n\n\n\n\n\n\n\n\nSex repartition per class (without missing values):\n\ndf_titanic_raw.groupby(['Sex'])[['Sex']].count()\n\n\n\n\n\n\n\n\nSex\n\n\nSex\n\n\n\n\n\nfemale\n314\n\n\nmale\n577\n\n\n\n\n\n\n\n\n\nRaw sex ratio (without missing values):\n\ndf_titanic.groupby(['Sex'])[['Sex']].count()\n\n\n\n\n\n\n\n\nSex\n\n\nSex\n\n\n\n\n\nfemale\n88\n\n\nmale\n95\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nConsider checking the raw data, as often boxplots or simple statistics are not enough to understand the diversity inside the data; see for instance the discussion by Carl Bergstrom on Mastodon.\n\n\nReferences:\n\nPractical Business Python, Comprehensive Guide to Grouping and Aggregating with Pandas, by Chris Moffitt\n\n\n\n\npd.crosstab\n\npd.crosstab(\n    df_titanic_raw[\"Sex\"],\n    df_titanic_raw[\"Pclass\"],\n    values=df_titanic_raw[\"Sex\"],\n    aggfunc=\"count\",\n    normalize=True,\n)\n\n\n\n\n\n\n\nPclass\n1\n2\n3\n\n\nSex\n\n\n\n\n\n\n\nfemale\n0.105499\n0.085297\n0.161616\n\n\nmale\n0.136925\n0.121212\n0.389450\n\n\n\n\n\n\n\n\n\nListing rows and columns\n\ndf_titanic.index\n\nIndex([  1,   3,   6,  10,  11,  21,  23,  27,  52,  54,\n       ...\n       835, 853, 857, 862, 867, 871, 872, 879, 887, 889],\n      dtype='int64', length=183)\n\n\n\ndf_titanic.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\n\nFrom numpy to pandas\nuseful for using packages on top of pandas (e.g., sklearn, though nowadays it works out of the box with pandas)\n\narray_titanic = df_titanic.values  # associated numpy array\narray_titanic\n\narray([[2, 1, 1, ..., 71.2833, 'C85', 'C'],\n       [4, 1, 1, ..., 53.1, 'C123', 'S'],\n       [7, 0, 1, ..., 51.8625, 'E46', 'S'],\n       ...,\n       [880, 1, 1, ..., 83.1583, 'C50', 'C'],\n       [888, 1, 1, ..., 30.0, 'B42', 'S'],\n       [890, 1, 1, ..., 30.0, 'C148', 'C']], dtype=object)\n\n\n\n\n\n\n\n\nEXERCISE: dropna\n\n\n\nPerform the following operation: remove the columns Cabin from the raw dataset, and then remove the rows with the variable Age missing.\nHint: check the ‘dropna’ documentation.",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#pandas-dataframe",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#pandas-dataframe",
    "title": "Titanic dataset",
    "section": "pandas DataFrame",
    "text": "pandas DataFrame\n\n1D dataset: Series (a column of a DataFrame)\nA Series is a labeled 1D column of a kind.\n\nfare = df_titanic['Fare']\nfare\n\n1      71.2833\n3      53.1000\n6      51.8625\n10     16.7000\n        ...   \n872     5.0000\n879    83.1583\n887    30.0000\n889    30.0000\nName: Fare, Length: 183, dtype: float64\n\n\n\n\nAttributes Series: indices and values\n\nfare.values[:10]\n\narray([ 71.2833,  53.1   ,  51.8625,  16.7   ,  26.55  ,  13.    ,\n        35.5   , 263.    ,  76.7292,  61.9792])\n\n\nContrarily to numpy arrays, you can index with other formats than integers (the underlying structure being a dictionary):\n\nhead command\n\ndf_titanic_raw.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n\n\n\n\n\n\n# Be careful, what follows changes the indexing\ndf_titanic_raw = df_titanic_raw.set_index('Name')\ndf_titanic_raw.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBraund, Mr. Owen Harris\n1\n0\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\n2\n1\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\nHeikkinen, Miss. Laina\n3\n1\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n\n\n\n\n\n\n\ntail command\n\ndf_titanic_raw.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBraund, Mr. Owen Harris\n1\n0\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\n2\n1\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\nHeikkinen, Miss. Laina\n3\n1\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n\n\n\n\n\n\nage = df_titanic_raw['Age']\nage['Behr, Mr. Karl Howell']\n\n26.0\n\n\n\nage.mean()\n\n29.69911764705882\n\n\n\ndf_titanic_raw[age &lt; 2]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaldwell, Master. Alden Gates\n79\n1\n2\nmale\n0.83\n0\n2\n248738\n29.0000\nNaN\nS\n\n\nPanula, Master. Eino Viljami\n165\n0\n3\nmale\n1.00\n4\n1\n3101295\n39.6875\nNaN\nS\n\n\nJohnson, Miss. Eleanor Ileen\n173\n1\n3\nfemale\n1.00\n1\n1\n347742\n11.1333\nNaN\nS\n\n\nBecker, Master. Richard F\n184\n1\n2\nmale\n1.00\n2\n1\n230136\n39.0000\nF4\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nDean, Master. Bertram Vere\n789\n1\n3\nmale\n1.00\n1\n2\nC.A. 2315\n20.5750\nNaN\nS\n\n\nThomas, Master. Assad Alexander\n804\n1\n3\nmale\n0.42\n0\n1\n2625\n8.5167\nNaN\nC\n\n\nMallet, Master. Andre\n828\n1\n2\nmale\n1.00\n0\n2\nS.C./PARIS 2079\n37.0042\nNaN\nC\n\n\nRichards, Master. George Sibley\n832\n1\n2\nmale\n0.83\n1\n1\n29106\n18.7500\nNaN\nS\n\n\n\n\n14 rows × 11 columns\n\n\n\n\n# You can come back to the original indexing\ndf_titanic_raw = df_titanic_raw.reset_index()\n\n\n\n\nCounting values for categorical variables\n\ndf_titanic_raw['Embarked'].value_counts(normalize=False, sort=True,\n                                        ascending=False)\n\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\npd.options.display.max_rows = 70\ndf_titanic[df_titanic['Embarked'] == 'C']\ndir(numpy)\n\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n52\n53\n1\n1\nHarper, Mrs. Henry Sleeper (Myna Haxtun)\nfemale\n49.0\n1\n0\nPC 17572\n76.7292\nD33\nC\n\n\n54\n55\n0\n1\nOstby, Mr. Engelhart Cornelius\nmale\n65.0\n0\n1\n113509\n61.9792\nB30\nC\n\n\n96\n97\n0\n1\nGoldschmidt, Mr. George B\nmale\n71.0\n0\n0\nPC 17754\n34.6542\nA5\nC\n\n\n97\n98\n1\n1\nGreenfield, Mr. William Bertram\nmale\n23.0\n0\n1\nPC 17759\n63.3583\nD10 D12\nC\n\n\n118\n119\n0\n1\nBaxter, Mr. Quigg Edmond\nmale\n24.0\n0\n1\nPC 17558\n247.5208\nB58 B60\nC\n\n\n139\n140\n0\n1\nGiglio, Mr. Victor\nmale\n24.0\n0\n0\nPC 17593\n79.2000\nB86\nC\n\n\n174\n175\n0\n1\nSmith, Mr. James Clinch\nmale\n56.0\n0\n0\n17764\n30.6958\nA7\nC\n\n\n177\n178\n0\n1\nIsham, Miss. Ann Elizabeth\nfemale\n50.0\n0\n0\nPC 17595\n28.7125\nC49\nC\n\n\n194\n195\n1\n1\nBrown, Mrs. James Joseph (Margaret Tobin)\nfemale\n44.0\n0\n0\nPC 17610\n27.7208\nB4\nC\n\n\n195\n196\n1\n1\nLurette, Miss. Elise\nfemale\n58.0\n0\n0\nPC 17569\n146.5208\nB80\nC\n\n\n209\n210\n1\n1\nBlank, Mr. Henry\nmale\n40.0\n0\n0\n112277\n31.0000\nA31\nC\n\n\n215\n216\n1\n1\nNewell, Miss. Madeleine\nfemale\n31.0\n1\n0\n35273\n113.2750\nD36\nC\n\n\n218\n219\n1\n1\nBazzani, Miss. Albina\nfemale\n32.0\n0\n0\n11813\n76.2917\nD15\nC\n\n\n273\n274\n0\n1\nNatsch, Mr. Charles H\nmale\n37.0\n0\n1\nPC 17596\n29.7000\nC118\nC\n\n\n291\n292\n1\n1\nBishop, Mrs. Dickinson H (Helen Walton)\nfemale\n19.0\n1\n0\n11967\n91.0792\nB49\nC\n\n\n292\n293\n0\n2\nLevy, Mr. Rene Jacques\nmale\n36.0\n0\n0\nSC/Paris 2163\n12.8750\nD\nC\n\n\n299\n300\n1\n1\nBaxter, Mrs. James (Helene DeLaudeniere Chaput)\nfemale\n50.0\n0\n1\nPC 17558\n247.5208\nB58 B60\nC\n\n\n307\n308\n1\n1\nPenasco y Castellana, Mrs. Victor de Satode (M...\nfemale\n17.0\n1\n0\nPC 17758\n108.9000\nC65\nC\n\n\n309\n310\n1\n1\nFrancatelli, Miss. Laura Mabel\nfemale\n30.0\n0\n0\nPC 17485\n56.9292\nE36\nC\n\n\n310\n311\n1\n1\nHays, Miss. Margaret Bechstein\nfemale\n24.0\n0\n0\n11767\n83.1583\nC54\nC\n\n\n311\n312\n1\n1\nRyerson, Miss. Emily Borie\nfemale\n18.0\n2\n2\nPC 17608\n262.3750\nB57 B59 B63 B66\nC\n\n\n319\n320\n1\n1\nSpedden, Mrs. Frederic Oakley (Margaretta Corn...\nfemale\n40.0\n1\n1\n16966\n134.5000\nE34\nC\n\n\n325\n326\n1\n1\nYoung, Miss. Marie Grice\nfemale\n36.0\n0\n0\nPC 17760\n135.6333\nC32\nC\n\n\n329\n330\n1\n1\nHippach, Miss. Jean Gertrude\nfemale\n16.0\n0\n1\n111361\n57.9792\nB18\nC\n\n\n337\n338\n1\n1\nBurns, Miss. Elizabeth Margaret\nfemale\n41.0\n0\n0\n16966\n134.5000\nE40\nC\n\n\n366\n367\n1\n1\nWarren, Mrs. Frank Manley (Anna Sophia Atkinson)\nfemale\n60.0\n1\n0\n110813\n75.2500\nD37\nC\n\n\n369\n370\n1\n1\nAubart, Mme. Leontine Pauline\nfemale\n24.0\n0\n0\nPC 17477\n69.3000\nB35\nC\n\n\n370\n371\n1\n1\nHarder, Mr. George Achilles\nmale\n25.0\n1\n0\n11765\n55.4417\nE50\nC\n\n\n377\n378\n0\n1\nWidener, Mr. Harry Elkins\nmale\n27.0\n0\n2\n113503\n211.5000\nC82\nC\n\n\n393\n394\n1\n1\nNewell, Miss. Marjorie\nfemale\n23.0\n1\n0\n35273\n113.2750\nD36\nC\n\n\n452\n453\n0\n1\nForeman, Mr. Benjamin Laventall\nmale\n30.0\n0\n0\n113051\n27.7500\nC111\nC\n\n\n453\n454\n1\n1\nGoldenberg, Mr. Samuel L\nmale\n49.0\n1\n0\n17453\n89.1042\nC92\nC\n\n\n473\n474\n1\n2\nJerwan, Mrs. Amin S (Marie Marthe Thuillard)\nfemale\n23.0\n0\n0\nSC/AH Basle 541\n13.7917\nD\nC\n\n\n484\n485\n1\n1\nBishop, Mr. Dickinson H\nmale\n25.0\n1\n0\n11967\n91.0792\nB49\nC\n\n\n487\n488\n0\n1\nKent, Mr. Edward Austin\nmale\n58.0\n0\n0\n11771\n29.7000\nB37\nC\n\n\n496\n497\n1\n1\nEustis, Miss. Elizabeth Mussey\nfemale\n54.0\n1\n0\n36947\n78.2667\nD20\nC\n\n\n505\n506\n0\n1\nPenasco y Castellana, Mr. Victor de Satode\nmale\n18.0\n1\n0\nPC 17758\n108.9000\nC65\nC\n\n\n523\n524\n1\n1\nHippach, Mrs. Louis Albert (Ida Sophia Fischer)\nfemale\n44.0\n0\n1\n111361\n57.9792\nB18\nC\n\n\n539\n540\n1\n1\nFrolicher, Miss. Hedwig Margaritha\nfemale\n22.0\n0\n2\n13568\n49.5000\nB39\nC\n\n\n544\n545\n0\n1\nDouglas, Mr. Walter Donald\nmale\n50.0\n1\n0\nPC 17761\n106.4250\nC86\nC\n\n\n550\n551\n1\n1\nThayer, Mr. John Borland Jr\nmale\n17.0\n0\n2\n17421\n110.8833\nC70\nC\n\n\n556\n557\n1\n1\nDuff Gordon, Lady. (Lucille Christiana Sutherl...\nfemale\n48.0\n1\n0\n11755\n39.6000\nA16\nC\n\n\n581\n582\n1\n1\nThayer, Mrs. John Borland (Marian Longstreth M...\nfemale\n39.0\n1\n1\n17421\n110.8833\nC68\nC\n\n\n583\n584\n0\n1\nRoss, Mr. John Hugo\nmale\n36.0\n0\n0\n13049\n40.1250\nA10\nC\n\n\n587\n588\n1\n1\nFrolicher-Stehli, Mr. Maxmillian\nmale\n60.0\n1\n1\n13567\n79.2000\nB41\nC\n\n\n591\n592\n1\n1\nStephenson, Mrs. Walter Bertram (Martha Eustis)\nfemale\n52.0\n1\n0\n36947\n78.2667\nD20\nC\n\n\n599\n600\n1\n1\nDuff Gordon, Sir. Cosmo Edmund (\"Mr Morgan\")\nmale\n49.0\n1\n0\nPC 17485\n56.9292\nA20\nC\n\n\n632\n633\n1\n1\nStahelin-Maeglin, Dr. Max\nmale\n32.0\n0\n0\n13214\n30.5000\nB50\nC\n\n\n641\n642\n1\n1\nSagesser, Mlle. Emma\nfemale\n24.0\n0\n0\nPC 17477\n69.3000\nB35\nC\n\n\n645\n646\n1\n1\nHarper, Mr. Henry Sleeper\nmale\n48.0\n1\n0\nPC 17572\n76.7292\nD33\nC\n\n\n647\n648\n1\n1\nSimonius-Blumer, Col. Oberst Alfons\nmale\n56.0\n0\n0\n13213\n35.5000\nA26\nC\n\n\n659\n660\n0\n1\nNewell, Mr. Arthur Webster\nmale\n58.0\n0\n2\n35273\n113.2750\nD48\nC\n\n\n679\n680\n1\n1\nCardeza, Mr. Thomas Drake Martinez\nmale\n36.0\n0\n1\nPC 17755\n512.3292\nB51 B53 B55\nC\n\n\n681\n682\n1\n1\nHassab, Mr. Hammad\nmale\n27.0\n0\n0\nPC 17572\n76.7292\nD49\nC\n\n\n698\n699\n0\n1\nThayer, Mr. John Borland\nmale\n49.0\n1\n1\n17421\n110.8833\nC68\nC\n\n\n700\n701\n1\n1\nAstor, Mrs. John Jacob (Madeleine Talmadge Force)\nfemale\n18.0\n1\n0\nPC 17757\n227.5250\nC62 C64\nC\n\n\n710\n711\n1\n1\nMayne, Mlle. Berthe Antonine (\"Mrs de Villiers\")\nfemale\n24.0\n0\n0\nPC 17482\n49.5042\nC90\nC\n\n\n716\n717\n1\n1\nEndres, Miss. Caroline Louise\nfemale\n38.0\n0\n0\nPC 17757\n227.5250\nC45\nC\n\n\n737\n738\n1\n1\nLesurer, Mr. Gustave J\nmale\n35.0\n0\n0\nPC 17755\n512.3292\nB101\nC\n\n\n742\n743\n1\n1\nRyerson, Miss. Susan Parker \"Suzette\"\nfemale\n21.0\n2\n2\nPC 17608\n262.3750\nB57 B59 B63 B66\nC\n\n\n789\n790\n0\n1\nGuggenheim, Mr. Benjamin\nmale\n46.0\n0\n0\nPC 17593\n79.2000\nB82 B84\nC\n\n\n835\n836\n1\n1\nCompton, Miss. Sara Rebecca\nfemale\n39.0\n1\n1\nPC 17756\n83.1583\nE49\nC\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n\n\n\n\n Comments: not all passengers from Cherbourg are Gallic (🇫🇷: gaulois) …\nWhat is the survival rate for raw data?\n\ndf_titanic_raw['Survived'].mean()\n\n0.3838383838383838\n\n\nWhat is the survival rate for data after removing missing values?\n\ndf_titanic['Survived'].mean()\n\n0.6721311475409836\n\n\nSee also the command:\n\ndf_titanic.groupby(['Sex'])[['Survived', 'Age', 'Fare']].mean()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\n\n\nSex\n\n\n\n\n\n\n\nfemale\n0.931818\n32.676136\n89.000900\n\n\nmale\n0.431579\n38.451789\n69.124343\n\n\n\n\n\n\n\nConclusion: Be careful when you remove some missing values, the missingness might be informative!\n\n\n\n\n\n\nEXERCISE: More data analysis\n\n\n\nWhat was the proportion of women on the boat?\n\n\n\n\nData import/export\nThe Pandas library supports many formats:\n\nCSV, text\nSQL database\nExcel\nHDF5\nJSON\nHTML\npickle\nsas, stata\n…",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-exploration",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#data-exploration",
    "title": "Titanic dataset",
    "section": "Data Exploration",
    "text": "Data Exploration\nYou can explore the data starting from the top or the bottom of the file:\n\nAccess values by line (iloc) or columns (loc)\nTheloc and iloc functions select indices: - the loc function selects rows using row labels whereas - the iloc function selects rows using their integer positions.\n\niloc\n\n\ndf_titanic_raw.iloc[0:2]\n\n\n\n\n\n\n\n\nName\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\nBraund, Mr. Owen Harris\n1\n0\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n2\n1\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\nloc\n\n\n# with naming indexing\ndf_titanic_raw = df_titanic_raw.set_index('Name')  # You can only do it once !!\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Fare']\n\n26.55\n\n\n\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth']\n\nPassengerId        12\nSurvived            1\nPclass              1\nSex            female\nAge              58.0\nSibSp               0\nParch               0\nTicket         113783\nFare            26.55\nCabin            C103\nEmbarked            S\nName: Bonnell, Miss. Elizabeth, dtype: object\n\n\n\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived']\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 0\n\n\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth']\n\nPassengerId        12\nSurvived            0\nPclass              1\nSex            female\nAge              58.0\nSibSp               0\nParch               0\nTicket         113783\nFare            26.55\nCabin            C103\nEmbarked            S\nName: Bonnell, Miss. Elizabeth, dtype: object\n\n\n\n# set back the original value\ndf_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 1\ndf_titanic_raw = df_titanic_raw.reset_index()  # Back to original indexing\n\n\n\nCreate binned values\n\nbins=np.arange(0, 100, 10)\ncurrent_palette = sns.color_palette()\n\ndf_test = pd.DataFrame({ 'Age': pd.cut(df_titanic['Age'], bins=bins, right=False)})\nax = sns.countplot(data=df_test, x='Age', color=current_palette[0])\nax.tick_params(axis='x', labelrotation=90)\nplt.title(\"Binning age per decades\")\nplt.show()",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_titanic.html#references",
    "href": "Courses/Scientific_Data_Libraries/pandas_titanic.html#references",
    "title": "Titanic dataset",
    "section": "References",
    "text": "References\n\nThe Python Graph Gallery\nOther interactive tools for data visualization include Altair, Bokeh, etc. See comparisons by Aaron Geller: link\nHow to choose your chart by A ndrew V. Abela.",
    "crumbs": [
      "Pandas",
      "Titanic dataset"
    ]
  },
  {
    "objectID": "Courses/Scientific_Data_Libraries/pandas_airparif.html",
    "href": "Courses/Scientific_Data_Libraries/pandas_airparif.html",
    "title": "Airparif dataset",
    "section": "",
    "text": "Disclaimer: this course is adapted from the work Pandas tutorial by Joris Van den Bossche. R users might also want to read Pandas: Comparison with R / R libraries for a smooth start in Pandas.\nWe start by importing the necessary libraries:\n\n%matplotlib inline\nimport os\nimport numpy as np\nimport calendar\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nimport pooch  # download data / avoid re-downloading\nfrom IPython import get_ipython\n\n\nsns.set_palette(\"colorblind\")\npalette = sns.color_palette(\"twilight\", n_colors=12)\npd.options.display.max_rows = 8\n\nThis part studies air quality in Paris (Source: Airparif) with pandas.\n\n'''\n# works locally but not on \nurl = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/20080421_20160927-PA13_auto.csv\"\npath_target = \"./20080421_20160927-PA13_auto.csv\"\npath, fname = os.path.split(path_target)\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\n'''\n\n'\\n# works locally but not on \\nurl = \"https://github.com/bbensaid30/Course-Software-Development-HAX712X/tree/main/Courses/Scientific_Data_Libraries/20080421_20160927-PA13_auto.csv\"\\npath_target = \"./20080421_20160927-PA13_auto.csv\"\\npath, fname = os.path.split(path_target)\\npooch.retrieve(url, path=path, fname=fname, known_hash=None)\\n'\n\n\nFor instance, you can run in a terminal:\n\n'''\n# same thing\n#| eval: false\n!head -26 ./20080421_20160927-PA13_auto.csv\n'''\n\n'\\n# same thing\\n#| eval: false\\n!head -26 ./20080421_20160927-PA13_auto.csv\\n'\n\n\nAlternatively:\n\n'''\n# same thing as before\n#| eval: false\nfrom IPython import get_ipython\nget_ipython().system('head -26 ./20080421_20160927-PA13_auto.csv')\n'''\n\n\"\\n# same thing as before\\n#| eval: false\\nfrom IPython import get_ipython\\nget_ipython().system('head -26 ./20080421_20160927-PA13_auto.csv')\\n\"\n\n\nReferences:\n\nWorking with time series, Python Data Science Handbook by Jake VanderPlas\n\n\npolution_df = pd.read_csv('20080421_20160927-PA13_auto.csv', sep=';',\n                          comment='#',\n                          na_values=\"n/d\",\n                          converters={'heure': str})\n\n\npd.options.display.max_rows = 30\npolution_df.head(25)\n\n\n\n\n\n\n\n\ndate\nheure\nNO2\nO3\n\n\n\n\n0\n21/04/2008\n1\n13.0\n74.0\n\n\n1\n21/04/2008\n2\n11.0\n73.0\n\n\n2\n21/04/2008\n3\n13.0\n64.0\n\n\n3\n21/04/2008\n4\n23.0\n46.0\n\n\n4\n21/04/2008\n5\n47.0\n24.0\n\n\n5\n21/04/2008\n6\n70.0\n11.0\n\n\n6\n21/04/2008\n7\n70.0\n17.0\n\n\n7\n21/04/2008\n8\n76.0\n16.0\n\n\n8\n21/04/2008\n9\nNaN\nNaN\n\n\n9\n21/04/2008\n10\nNaN\nNaN\n\n\n10\n21/04/2008\n11\nNaN\nNaN\n\n\n11\n21/04/2008\n12\n33.0\n60.0\n\n\n12\n21/04/2008\n13\n31.0\n61.0\n\n\n13\n21/04/2008\n14\n37.0\n61.0\n\n\n14\n21/04/2008\n15\n20.0\n78.0\n\n\n15\n21/04/2008\n16\n29.0\n71.0\n\n\n16\n21/04/2008\n17\n30.0\n70.0\n\n\n17\n21/04/2008\n18\n38.0\n58.0\n\n\n18\n21/04/2008\n19\n52.0\n40.0\n\n\n19\n21/04/2008\n20\n56.0\n29.0\n\n\n20\n21/04/2008\n21\n39.0\n40.0\n\n\n21\n21/04/2008\n22\n31.0\n42.0\n\n\n22\n21/04/2008\n23\n29.0\n42.0\n\n\n23\n21/04/2008\n24\n28.0\n36.0\n\n\n24\n22/04/2008\n1\n46.0\n16.0\n\n\n\n\n\n\n\n\nData preprocessing\n\n# check types\npolution_df.dtypes\n\n# check all\npolution_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 73920 entries, 0 to 73919\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   date    73920 non-null  object \n 1   heure   73920 non-null  object \n 2   NO2     71008 non-null  float64\n 3   O3      71452 non-null  float64\ndtypes: float64(2), object(2)\nmemory usage: 2.3+ MB\n\n\nFor more info on the nature of Pandas objects, see this discussion on Stackoverflow. Moreover, things are slowly moving from numpy to pyarrow, cf. Pandas user guide\n\n\n\n\n\n\nEXERCISE: handling missing values\n\n\n\nWhat is the meaning of “na_values=”n/d” above? Note that an alternative can be obtained with the command polution_df.replace('n/d', np.nan, inplace=True)\n\n\n\n\nIssues with non-conventional hours/day format\nStart by changing to integer type (e.g., int8):\n\npolution_df['heure'] = polution_df['heure'].astype(np.int8)\npolution_df['heure']\n\n0         1\n1         2\n2         3\n3         4\n4         5\n         ..\n73915    20\n73916    21\n73917    22\n73918    23\n73919    24\nName: heure, Length: 73920, dtype: int8\n\n\nNo data is from 1 to 24… not conventional so let’s make it from 0 to 23\n\npolution_df['heure'] = polution_df['heure'] - 1\npolution_df['heure']\n\n0         0\n1         1\n2         2\n3         3\n4         4\n         ..\n73915    19\n73916    20\n73917    21\n73918    22\n73919    23\nName: heure, Length: 73920, dtype: int8\n\n\nand back to strings:\n\npolution_df['heure'] = polution_df['heure'].astype('str')\npolution_df['heure']\n\n0         0\n1         1\n2         2\n3         3\n4         4\n         ..\n73915    19\n73916    20\n73917    21\n73918    22\n73919    23\nName: heure, Length: 73920, dtype: object\n\n\n\n\nTime processing\nNote that we have used the following conventions:\n\nd = day\nm=month\nY=year\nH=hour\nM=minutes\n\n\ntime_improved = pd.to_datetime(polution_df['date'] +\n                               ' ' + polution_df['heure'] + ':00',\n                               format='%d/%m/%Y %H:%M')\n\ntime_improved\n\n0       2008-04-21 00:00:00\n1       2008-04-21 01:00:00\n2       2008-04-21 02:00:00\n3       2008-04-21 03:00:00\n4       2008-04-21 04:00:00\n                ...        \n73915   2016-09-27 19:00:00\n73916   2016-09-27 20:00:00\n73917   2016-09-27 21:00:00\n73918   2016-09-27 22:00:00\n73919   2016-09-27 23:00:00\nLength: 73920, dtype: datetime64[ns]\n\n\n\npolution_df['date'] + ' ' + polution_df['heure'] + ':00'\n\n0         21/04/2008 0:00\n1         21/04/2008 1:00\n2         21/04/2008 2:00\n3         21/04/2008 3:00\n4         21/04/2008 4:00\n               ...       \n73915    27/09/2016 19:00\n73916    27/09/2016 20:00\n73917    27/09/2016 21:00\n73918    27/09/2016 22:00\n73919    27/09/2016 23:00\nLength: 73920, dtype: object\n\n\nCreate correct timing format in the dataframe\n\npolution_df['DateTime'] = time_improved\n # remove useless columns:\ndel polution_df['heure']\ndel polution_df['date']\npolution_df\n\n\n\n\n\n\n\n\nNO2\nO3\nDateTime\n\n\n\n\n0\n13.0\n74.0\n2008-04-21 00:00:00\n\n\n1\n11.0\n73.0\n2008-04-21 01:00:00\n\n\n2\n13.0\n64.0\n2008-04-21 02:00:00\n\n\n3\n23.0\n46.0\n2008-04-21 03:00:00\n\n\n4\n47.0\n24.0\n2008-04-21 04:00:00\n\n\n...\n...\n...\n...\n\n\n73915\n55.0\n31.0\n2016-09-27 19:00:00\n\n\n73916\n85.0\n5.0\n2016-09-27 20:00:00\n\n\n73917\n75.0\n9.0\n2016-09-27 21:00:00\n\n\n73918\n64.0\n15.0\n2016-09-27 22:00:00\n\n\n73919\n57.0\n14.0\n2016-09-27 23:00:00\n\n\n\n\n73920 rows × 3 columns\n\n\n\nVisualize the data set now that the time is well formatted:\n\npolution_ts = polution_df.set_index(['DateTime'])\npolution_ts = polution_ts.sort_index(ascending=True)\npolution_ts.head(12)\n\n\n\n\n\n\n\n\nNO2\nO3\n\n\nDateTime\n\n\n\n\n\n\n2008-04-21 00:00:00\n13.0\n74.0\n\n\n2008-04-21 01:00:00\n11.0\n73.0\n\n\n2008-04-21 02:00:00\n13.0\n64.0\n\n\n2008-04-21 03:00:00\n23.0\n46.0\n\n\n2008-04-21 04:00:00\n47.0\n24.0\n\n\n2008-04-21 05:00:00\n70.0\n11.0\n\n\n2008-04-21 06:00:00\n70.0\n17.0\n\n\n2008-04-21 07:00:00\n76.0\n16.0\n\n\n2008-04-21 08:00:00\nNaN\nNaN\n\n\n2008-04-21 09:00:00\nNaN\nNaN\n\n\n2008-04-21 10:00:00\nNaN\nNaN\n\n\n2008-04-21 11:00:00\n33.0\n60.0\n\n\n\n\n\n\n\n\npolution_ts.describe()\n\n\n\n\n\n\n\n\nNO2\nO3\n\n\n\n\ncount\n71008.000000\n71452.000000\n\n\nmean\n34.453414\n39.610046\n\n\nstd\n20.380702\n28.837333\n\n\nmin\n1.000000\n0.000000\n\n\n25%\n19.000000\n16.000000\n\n\n50%\n30.000000\n38.000000\n\n\n75%\n46.000000\n58.000000\n\n\nmax\n167.000000\n211.000000\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\naxes[0].plot(polution_ts['O3'])\naxes[0].set_title(\"Ozone polution: daily average in Paris\")\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\n\naxes[1].plot(polution_ts['NO2'])\naxes[1].set_title(\"Nitrogen polution: daily average in Paris\")\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\nplt.show()\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n\naxes[0].plot(polution_ts['O3'].resample('d').max(), '--')\naxes[0].plot(polution_ts['O3'].resample('d').min(),'-.')\n\naxes[0].set_title(\"Ozone polution: daily average in Paris\")\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\n\naxes[1].plot(polution_ts['NO2'].resample('d').max(),  '--')\naxes[1].plot(polution_ts['NO2'].resample('d').min(),  '-.')\n\naxes[1].set_title(\"Nitrogen polution: daily average in Paris\")\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\n\nplt.show()\n\n\n\n\n\n\n\nSource : https://www.tutorialspoint.com/python/time_strptime.htm\n\n\n\n\n\n\nEXERCISE: extreme values per day \n\n\n\nProvide the same plots as before, but with daily best and worst on the same figures (and use different colors and/or styles)\nQ: Is the pollution getting better over the years or not?\n\n\nfig, ax = plt.subplots(1, 1)\npolution_ts['2008':].resample('Y').mean().plot(ax=ax)\n# Sample by year (A pour Annual) or Y for Year\nplt.ylim(0, 50)\nplt.title(\"Pollution evolution: \\n yearly average in Paris\")\nplt.ylabel(\"Concentration (µg/m³)\")\nplt.xlabel(\"Year\")\nplt.show()\n\n/tmp/ipykernel_18355/1798743111.py:2: FutureWarning:\n\n'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n\n\n\n\n\n\n\n\n\nLoading colors:\nsns.set_palette(\"GnBu_d\", n_colors=7)\npolution_ts['weekday'] = polution_ts.index.weekday  # Monday=0, Sunday=6\npolution_ts['weekend'] = polution_ts['weekday'].isin([5, 6])\n\npolution_week_no2 = polution_ts.groupby(['weekday', polution_ts.index.hour])[\n    'NO2'].mean().unstack(level=0)\npolution_week_03 = polution_ts.groupby(['weekday', polution_ts.index.hour])[\n    'O3'].mean().unstack(level=0)\nplt.show()\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 7), sharex=True)\n\npolution_week_no2.plot(ax=axes[0])\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\naxes[0].set_xlabel(\"Intraday evolution\")\naxes[0].set_title(\n    \"Daily NO2 concentration: weekend effect?\")\naxes[0].set_xticks(np.arange(0, 24))\naxes[0].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[0].set_ylim(0, 60)\n\npolution_week_03.plot(ax=axes[1])\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\naxes[1].set_xlabel(\"Intraday evolution\")\naxes[1].set_title(\"Daily O3 concentration: weekend effect?\")\naxes[1].set_xticks(np.arange(0, 24))\naxes[1].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[1].set_ylim(0, 70)\naxes[0].legend().set_visible(False)\n# ax.legend()\naxes[1].legend(labels=[day for day in calendar.day_name], loc='lower left', bbox_to_anchor=(1, 0.1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\npolution_ts['month'] = polution_ts.index.month  # Janvier=0, .... Decembre=11\npolution_ts['month'] = polution_ts['month'].apply(lambda x:\n                                                  calendar.month_abbr[x])\npolution_ts.head()\n\n\n\n\n\n\n\n\nNO2\nO3\nweekday\nweekend\nmonth\n\n\nDateTime\n\n\n\n\n\n\n\n\n\n2008-04-21 00:00:00\n13.0\n74.0\n0\nFalse\nApr\n\n\n2008-04-21 01:00:00\n11.0\n73.0\n0\nFalse\nApr\n\n\n2008-04-21 02:00:00\n13.0\n64.0\n0\nFalse\nApr\n\n\n2008-04-21 03:00:00\n23.0\n46.0\n0\nFalse\nApr\n\n\n2008-04-21 04:00:00\n47.0\n24.0\n0\nFalse\nApr\n\n\n\n\n\n\n\n\npolution_month_no2 = polution_ts.groupby(['month', polution_ts.index.hour])[\n    'NO2'].mean().unstack(level=0)\npolution_month_03 = polution_ts.groupby(['month', polution_ts.index.hour])[\n    'O3'].mean().unstack(level=0)\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 7), sharex=True)\n\naxes[0].set_prop_cycle(\n    (\n        cycler(color=palette) + cycler(ms=[4] * 12)\n        + cycler(marker=[\"o\", \"^\", \"s\", \"p\"] * 3)\n        + cycler(linestyle=[\"-\", \"--\", \":\", \"-.\"] * 3)\n    )\n)\npolution_month_no2.plot(ax=axes[0])\naxes[0].set_ylabel(\"Concentration (µg/m³)\")\naxes[0].set_xlabel(\"Hour of the day\")\naxes[0].set_title(\n    \"Daily profile per month (NO2)?\")\naxes[0].set_xticks(np.arange(0, 24))\naxes[0].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[0].set_ylim(0, 90)\n\naxes[1].set_prop_cycle(\n    (\n        cycler(color=palette) + cycler(ms=[4] * 12)\n        + cycler(marker=[\"o\", \"^\", \"s\", \"p\"] * 3)\n        + cycler(linestyle=[\"-\", \"--\", \":\", \"-.\"] * 3)\n    )\n)\npolution_month_03.plot(ax=axes[1])\naxes[1].set_ylabel(\"Concentration (µg/m³)\")\naxes[1].set_xlabel(\"Hour of the day\")\naxes[1].set_title(\"Daily profile per month (O3): weekend effect?\")\naxes[1].set_xticks(np.arange(0, 24))\naxes[1].set_xticklabels(np.arange(0, 24), rotation=45)\naxes[1].set_ylim(0, 90)\n\naxes[0].legend().set_visible(False)\n# ax.legend()\naxes[1].legend(labels=calendar.month_name[1:], loc='lower left',\n               bbox_to_anchor=(1, 0.1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nReferences:\n\nOther interactive tools for data visualization: Altair, Bokeh. See comparisons by Aarron Geller: link\nAn interesting tutorial: Altair introduction\nIntroduction on geopandas\nNotebook on French departememen/womennts issues\nChoropleth Maps in practice with Plotly and Python by Thibaud Lamothe\nEuropean data on France geography\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Pandas",
      "Airparif dataset"
    ]
  },
  {
    "objectID": "Projects/2024-2025/README.html",
    "href": "Projects/2024-2025/README.html",
    "title": "2024-2025",
    "section": "",
    "text": "For this course, the grading consists of long-term project.\nYou will need to work all along the semester to master it. Please start to work on it as soon as possible. You need to use your GitHub account (see Git lecture) as the expected outputs for this course are GitHub repositories.",
    "crumbs": [
      "2024-2025"
    ]
  },
  {
    "objectID": "Projects/2024-2025/README.html#examples",
    "href": "Projects/2024-2025/README.html#examples",
    "title": "2024-2025",
    "section": "2.1 Examples",
    "text": "2.1 Examples\nHere are some links to the Github pages showing what have been done the last year: Group 1 Group 2 Group 3",
    "crumbs": [
      "2024-2025"
    ]
  }
]